{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7446d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6cfb4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.embeddings.late_chunking:Initialized LateChunkingHelper: device=cuda, max_chunk_chars=2048\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: jinaai/jina-embeddings-v3\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "INFO:sentence_transformers.SentenceTransformer:5 prompts are loaded, with the keys: ['retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching']\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()                      # 1️⃣  make env-vars visible\n",
    "\n",
    "from src.containers import Container   # 2️⃣  class body reads env once\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "container = Container()            # 3️⃣  build the container\n",
    "\n",
    "weaviate_helper      = container.weaviate_helper()\n",
    "late_chunking_helper = container.late_chunking_helper()\n",
    "ingest_helper = container.ingest_helper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0dc4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-dev.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-prod.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-available-cgroupv2.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-available.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\Dockerfile' skipped. Reason: File extension 'Dockerfile' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\worker.py' skipped. Reason: File extension 'py' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\Dockerfile' skipped. Reason: File extension 'Dockerfile' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\rediswq.py' skipped. Reason: File extension 'py' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\worker.py' skipped. Reason: File extension 'py' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\game-env-file.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\game.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\ui-env-file.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\ui.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\examples.go' skipped. Reason: File extension 'go' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\examples_test.go' skipped. Reason: File extension 'go' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\config\\redis-config' skipped. Reason: File extension 'redis-config' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\audit.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\fine-grained.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\violation.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\README.md' skipped. Reason: File extension 'md' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\kind-with-cluster-level-baseline-pod-security.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\kind-with-namespace-level-baseline-pod-security.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\Dockerfile' skipped. Reason: File extension 'Dockerfile' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\frontend-nginx.conf' skipped. Reason: File extension 'conf' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\tls\\server-signing-config.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\certificate-signing-request\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\certificate-signing-request\\clusterrole-approve.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\certificate-signing-request\\clusterrole-create.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\certificate-signing-request\\clusterrole-sign.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\deployment-replicas-policy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\endpoints-aggregated.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\image-matches-namespace-environment.policy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\simple-clusterrole.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\simple-clusterrolebinding.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\simple-role.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\simple-rolebinding-with-clusterrole.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\simple-rolebinding-with-role.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\validating-admission-policy-audit-annotation.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\access\\validating-admission-policy-match-conditions.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\cloud\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\cloud\\ccm-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\dns\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\dns\\busybox.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\dns\\dns-horizontal-autoscaler.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\dns\\dnsutils.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\konnectivity\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\konnectivity\\egress-selector-configuration.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\konnectivity\\konnectivity-agent.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\konnectivity\\konnectivity-rbac.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\konnectivity\\konnectivity-server.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\logging\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\logging\\fluentd-sidecar-config.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\logging\\two-files-counter-pod-agent-sidecar.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\logging\\two-files-counter-pod-streaming-sidecar.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\logging\\two-files-counter-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-dev.json\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-dev.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-prod.json\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-prod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-constraints-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-constraints-pod-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-constraints-pod-4.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-constraints-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-constraints.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-defaults-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-defaults-pod-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-defaults-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\cpu-defaults.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\limit-mem-cpu-container.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\limit-mem-cpu-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\limit-memory-ratio-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\limit-range-pod-1.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\limit-range-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\limit-range-pod-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-available-cgroupv2.sh\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-available.sh\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-constraints-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-constraints-pod-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-constraints-pod-4.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-constraints-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-constraints.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-defaults-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-defaults-pod-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-defaults-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-defaults.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\pvc-limit-greater.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\pvc-limit-lower.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-mem-cpu-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-mem-cpu-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-mem-cpu.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-objects-pvc-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-objects-pvc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-objects.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-pod-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\quota-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\storagelimits.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\sched\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\sched\\clusterrole.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\sched\\my-scheduler.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\sched\\pod1.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\sched\\pod2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\sched\\pod3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\snowflake-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\basic-daemonset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\cassandra\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\cassandra\\cassandra-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\cassandra\\cassandra-statefulset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\deployment-patch.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\deployment-retainkeys.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\deployment-scale.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\deployment-sidecar.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\deployment-update.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\\frontend-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\\frontend-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\\redis-follower-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\\redis-follower-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\\redis-leader-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\guestbook\\redis-leader-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\hpa\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\hpa\\php-apache.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\cronjob.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\indexed-job-vol.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\indexed-job.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\job-sidecar.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\job-tmpl.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\Dockerfile\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\job.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\rabbitmq-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\rabbitmq-statefulset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\worker.py\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\Dockerfile\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\job.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\redis-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\redis-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\rediswq.py\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\worker.py\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mongodb\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mongodb\\mongo-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mongodb\\mongo-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mysql\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mysql\\mysql-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mysql\\mysql-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mysql\\mysql-pv.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mysql\\mysql-services.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\mysql\\mysql-statefulset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\nginx\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\nginx\\nginx-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\nginx\\nginx-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\nginx-app.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\nginx-with-request.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\php-apache.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\shell-demo.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\simple_deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\ssa\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\ssa\\nginx-deployment-no-replicas.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\ssa\\nginx-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\update_deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\web\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\web\\web-parallel.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\web\\web.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\wordpress\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\wordpress\\mysql-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\wordpress\\wordpress-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\zookeeper\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\zookeeper\\zookeeper.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\audit\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\audit\\audit-policy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\concepts\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\concepts\\policy\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\concepts\\policy\\limit-range\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\concepts\\policy\\limit-range\\example-conflict-with-limitrange-cpu.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\concepts\\policy\\limit-range\\example-no-conflict-with-limitrange-cpu.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\concepts\\policy\\limit-range\\problematic-limit-range.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\configmap-multikeys.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\configmaps.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\configure-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\env-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\game-env-file.properties\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\game.properties\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\immutable-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\new-immutable-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\ui-env-file.properties\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\ui.properties\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\daemonset-label-selector.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\daemonset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\fluentd-daemonset-update.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\fluentd-daemonset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\frontend.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\hpa-rs.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job-backoff-limit-per-index-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job-pod-failure-policy-config-issue.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job-pod-failure-policy-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job-pod-failure-policy-failjob.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job-pod-failure-policy-ignore.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job-success-policy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\job.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\nginx-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\replicaset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\replication-nginx-1.14.2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\replication-nginx-1.16.1.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\controllers\\replication.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\customresourcedefinition\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\customresourcedefinition\\shirt-resource-definition.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\customresourcedefinition\\shirt-resources.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\counter-pod-err.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\counter-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\event-exporter.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\fluentd-gcp-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\fluentd-gcp-ds.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\node-problem-detector-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\node-problem-detector.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\debug\\termination.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\\deployment-with-capacity-reservation.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\\deployment-with-configmap-and-sidecar-container.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\\deployment-with-configmap-as-envvar.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\\deployment-with-configmap-as-volume.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\\deployment-with-configmap-two-containers.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\deployments\\deployment-with-immutable-configmap-as-volume.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\examples.go\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\examples_test.go\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\mutatingadmissionpolicy\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\mutatingadmissionpolicy\\applyconfiguration-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\mutatingadmissionpolicy\\json-patch-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\commands.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\config\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\config\\example-redis-config.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\config\\redis-config\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\config\\redis-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\image-volumes.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\init-containers.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\dapi-envars-container.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\dapi-envars-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\dapi-volume-resources.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\dapi-volume.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\dependent-envars.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\envars.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\pod-multiple-secret-env-variable.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\pod-secret-envFrom.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\pod-single-secret-env-variable.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\secret-envars-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\secret-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\inject\\secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\lifecycle-events.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-configmap-env-var-valueFrom.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-configmap-envFrom.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-configmap-volume-specific-key.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-configmap-volume.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-multiple-configmap-env-variable.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-nginx-preferred-affinity.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-nginx-required-affinity.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-nginx-specific-node.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-nginx.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-projected-svc-token.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-rs.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-single-configmap-env-variable.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-with-affinity-preferred-weight.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-with-node-affinity.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-with-pod-affinity.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-with-scheduling-gates.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-with-toleration.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\pod-without-scheduling-gates.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\private-reg-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\\exec-liveness.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\\grpc-liveness.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\\http-liveness.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\\pod-with-http-healthcheck.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\\pod-with-tcp-socket-healthcheck.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\probe\\tcp-liveness-readiness.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\qos\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\qos\\qos-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\qos\\qos-pod-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\qos\\qos-pod-4.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\qos\\qos-pod-5.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\qos\\qos-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\cpu-request-limit-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\cpu-request-limit.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\extended-resource-pod-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\extended-resource-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\memory-request-limit-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\memory-request-limit-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\memory-request-limit.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\pod-level-cpu-request-limit.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\pod-level-memory-request-limit.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\resource\\pod-level-resources.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\hello-apparmor.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\alpha\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\alpha\\audit-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\alpha\\default-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\alpha\\fine-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\alpha\\violation-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\fields.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\ga\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\ga\\audit-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\ga\\default-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\ga\\fine-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\ga\\violation-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\kind.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\audit.json\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\fine-grained.json\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\violation.json\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\security-context-2.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\security-context-3.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\security-context-4.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\security-context-5.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\security-context-6.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\security-context.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\share-process-namespace.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\simple-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\projected-clustertrustbundle.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\projected-secret-downwardapi-configmap.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\projected-secrets-nondefault-permission-mode.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\projected-service-account-token.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\projected.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\pv-claim.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\pv-duplicate.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\pv-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\pv-volume.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\storage\\redis.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\topology-spread-constraints\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\topology-spread-constraints\\one-constraint-with-nodeaffinity.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\topology-spread-constraints\\one-constraint.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\topology-spread-constraints\\two-constraints.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\two-container-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\user-namespaces-stateless.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\baseline-psp.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\example-psp.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\high-priority-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\priority-class-resourcequota.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\privileged-psp.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\quota.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\restricted-psp.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\zookeeper-pod-disruption-budget-maxunavailable.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\policy\\zookeeper-pod-disruption-budget-minavailable.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\priority-and-fairness\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\priority-and-fairness\\health-for-strangers.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\priority-and-fairness\\list-events-default-service-account.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\priorityclass\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\priorityclass\\low-priority-class.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\README.md\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\basicauth-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\bootstrap-token-secret-base64.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\bootstrap-token-secret-literal.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\dockercfg-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\dotfile-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\optional-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\serviceaccount\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\serviceaccount\\mysecretname.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\serviceaccount-token-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\ssh-auth-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\secret\\tls-auth-secret.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\example-baseline-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\kind-with-cluster-level-baseline-pod-security.sh\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\kind-with-namespace-level-baseline-pod-security.sh\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\podsecurity-baseline.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\podsecurity-privileged.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\podsecurity-restricted.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\backend-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\backend-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\Dockerfile\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\frontend-deployment.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\frontend-nginx.conf\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\frontend-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\hello-application.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\explore-graceful-termination-nginx.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\load-balancer-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\curlpod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\custom-dns.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\default-ingressclass.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\dual-stack-default-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\dual-stack-ipfamilies-ipv6.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\dual-stack-ipv6-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\dual-stack-prefer-ipv6-lb-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\dual-stack-preferred-ipfamilies-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\dual-stack-preferred-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\example-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\external-lb.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\hostaliases-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\ingress-resource-backend.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\ingress-wildcard-host.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\minimal-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\name-virtual-host-ingress-no-third-host.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\name-virtual-host-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\namespaced-params.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\network-policy-allow-all-egress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\network-policy-allow-all-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\network-policy-default-deny-all.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\network-policy-default-deny-egress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\network-policy-default-deny-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\networkpolicy-multiport-egress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\networkpolicy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\nginx-policy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\nginx-secure-app.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\nginx-svc.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\run-my-nginx.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\simple-fanout-example.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\test-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\networking\\tls-example-ingress.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\nginx-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\pod-with-graceful-termination.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\simple-service.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\rro.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\pod-volume-binding.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-aws-ebs.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-aws-efs.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-azure-file.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-ceph-rbd.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-local.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-nfs.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-portworx-volume.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass\\storageclass-topology.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\storage\\storageclass-low-latency.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\tls\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\tls\\server-signing-config.json\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\basic-example-binding.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\basic-example-policy.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\binding-with-param-prod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\binding-with-param.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\failure-policy-ignore.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\policy-with-param.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\replicalimit-param-prod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\replicalimit-param.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\typechecking-multiple-match.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\validatingadmissionpolicy\\typechecking.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\configmap-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\daemonset.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\deploy-hyperv.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\deploy-resource.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\emptydir-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\hostpath-volume-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\run-as-username-container.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\run-as-username-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\secret-pod.yaml\n",
      "***************************************\n",
      "***************************************\n",
      "knowledge\\kubernetes\\website-main\\content\\en\\examples\\windows\\simple-pod.yaml\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-approve.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-create.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-sign.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\deployment-replicas-policy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\endpoints-aggregated.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\image-matches-namespace-environment.policy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-clusterrole.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-clusterrolebinding.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-role.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-rolebinding-with-clusterrole.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-rolebinding-with-role.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-audit-annotation.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-match-conditions.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\cloud\\\\ccm-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\busybox.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dns-horizontal-autoscaler.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dnsutils.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\egress-selector-configuration.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-agent.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-rbac.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-server.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\fluentd-sidecar-config.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-agent-sidecar.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-streaming-sidecar.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-dev.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-prod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-4.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-mem-cpu-container.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-mem-cpu-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-memory-ratio-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-1.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-4.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\pvc-limit-greater.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\pvc-limit-lower.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects-pvc-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects-pvc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\storagelimits.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\clusterrole.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\my-scheduler.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod1.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\snowflake-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\basic-daemonset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-statefulset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-patch.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-retainkeys.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-scale.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-sidecar.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-update.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\hpa\\\\php-apache.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\cronjob.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job-vol.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-sidecar.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-tmpl.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\job.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\rabbitmq-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\rabbitmq-statefulset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\job.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\redis-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\redis-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-pv.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-services.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-statefulset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-app.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-with-request.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\php-apache.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\shell-demo.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\simple_deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment-no-replicas.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\update_deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web-parallel.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\mysql-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\wordpress-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\zookeeper\\\\zookeeper.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\audit\\\\audit-policy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-conflict-with-limitrange-cpu.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-no-conflict-with-limitrange-cpu.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\problematic-limit-range.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configmap-multikeys.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configmaps.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configure-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\env-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\immutable-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\new-immutable-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\daemonset-label-selector.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\daemonset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset-update.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\frontend.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\hpa-rs.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-backoff-limit-per-index-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-config-issue.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-failjob.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-ignore.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-success-policy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\nginx-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replicaset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.14.2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.16.1.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resource-definition.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resources.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\counter-pod-err.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\counter-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\event-exporter.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\fluentd-gcp-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\fluentd-gcp-ds.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\node-problem-detector-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\node-problem-detector.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\termination.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-capacity-reservation.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-and-sidecar-container.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-envvar.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-volume.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-two-containers.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-immutable-configmap-as-volume.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\mutatingadmissionpolicy\\\\applyconfiguration-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\mutatingadmissionpolicy\\\\json-patch-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\commands.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\example-redis-config.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\redis-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\image-volumes.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\init-containers.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-envars-container.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-envars-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume-resources.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dependent-envars.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\envars.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-multiple-secret-env-variable.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-secret-envFrom.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-single-secret-env-variable.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret-envars-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\lifecycle-events.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-env-var-valueFrom.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-envFrom.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-volume-specific-key.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-volume.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-multiple-configmap-env-variable.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-preferred-affinity.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-required-affinity.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-specific-node.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-projected-svc-token.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-rs.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-single-configmap-env-variable.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-affinity-preferred-weight.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-node-affinity.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-pod-affinity.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-scheduling-gates.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-toleration.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-without-scheduling-gates.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\private-reg-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\exec-liveness.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\grpc-liveness.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\http-liveness.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\pod-with-http-healthcheck.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\pod-with-tcp-socket-healthcheck.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\tcp-liveness-readiness.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-4.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-5.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-cpu-request-limit.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-memory-request-limit.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-resources.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\hello-apparmor.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\audit-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\default-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\fine-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\violation-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\fields.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\audit-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\default-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\fine-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\violation-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\kind.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-2.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-3.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-4.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-5.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-6.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\share-process-namespace.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\simple-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-clustertrustbundle.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-secret-downwardapi-configmap.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-secrets-nondefault-permission-mode.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-service-account-token.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-claim.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-duplicate.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-volume.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\redis.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint-with-nodeaffinity.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\two-constraints.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\two-container-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\user-namespaces-stateless.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\baseline-psp.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\example-psp.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\high-priority-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\priority-class-resourcequota.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\privileged-psp.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\quota.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\restricted-psp.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\zookeeper-pod-disruption-budget-maxunavailable.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\zookeeper-pod-disruption-budget-minavailable.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priority-and-fairness\\\\health-for-strangers.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priority-and-fairness\\\\list-events-default-service-account.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priorityclass\\\\low-priority-class.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\basicauth-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\bootstrap-token-secret-base64.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\bootstrap-token-secret-literal.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\dockercfg-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\dotfile-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\optional-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\serviceaccount\\\\mysecretname.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\serviceaccount-token-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\ssh-auth-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\tls-auth-secret.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\example-baseline-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-baseline.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-privileged.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-restricted.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-deployment.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\hello-application.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\explore-graceful-termination-nginx.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\load-balancer-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\curlpod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\custom-dns.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\default-ingressclass.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-default-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-ipfamilies-ipv6.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-ipv6-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-prefer-ipv6-lb-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-preferred-ipfamilies-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-preferred-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\example-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\external-lb.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\hostaliases-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\ingress-resource-backend.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\ingress-wildcard-host.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\minimal-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\name-virtual-host-ingress-no-third-host.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\name-virtual-host-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\namespaced-params.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-allow-all-egress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-allow-all-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-all.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-egress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\networkpolicy-multiport-egress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\networkpolicy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-policy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-secure-app.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-svc.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\run-my-nginx.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\simple-fanout-example.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\test-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\tls-example-ingress.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\nginx-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\pod-with-graceful-termination.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\simple-service.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\rro.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\pod-volume-binding.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-aws-ebs.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-aws-efs.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-azure-file.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-ceph-rbd.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-local.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-nfs.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-portworx-volume.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-topology.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass-low-latency.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\basic-example-binding.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\basic-example-policy.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\binding-with-param-prod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\binding-with-param.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\failure-policy-ignore.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\policy-with-param.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param-prod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\typechecking-multiple-match.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\typechecking.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\configmap-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\daemonset.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-hyperv.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-resource.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\emptydir-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\hostpath-volume-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\run-as-username-container.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\run-as-username-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\secret-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'},\n",
       "  {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\simple-pod.yaml',\n",
       "   'content': '',\n",
       "   'file_extension': 'yaml'}],\n",
       " [{'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-dev.json',\n",
       "   'reason': \"File extension 'json' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'json' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-prod.json',\n",
       "   'reason': \"File extension 'json' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'json' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-available-cgroupv2.sh',\n",
       "   'reason': \"File extension 'sh' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'sh' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-available.sh',\n",
       "   'reason': \"File extension 'sh' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'sh' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\Dockerfile',\n",
       "   'reason': \"File extension 'Dockerfile' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'Dockerfile' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\worker.py',\n",
       "   'reason': \"File extension 'py' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'py' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\Dockerfile',\n",
       "   'reason': \"File extension 'Dockerfile' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'Dockerfile' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\rediswq.py',\n",
       "   'reason': \"File extension 'py' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'py' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\worker.py',\n",
       "   'reason': \"File extension 'py' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'py' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\game-env-file.properties',\n",
       "   'reason': \"File extension 'properties' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'properties' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\game.properties',\n",
       "   'reason': \"File extension 'properties' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'properties' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\ui-env-file.properties',\n",
       "   'reason': \"File extension 'properties' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'properties' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\ui.properties',\n",
       "   'reason': \"File extension 'properties' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'properties' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\examples.go',\n",
       "   'reason': \"File extension 'go' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'go' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\examples_test.go',\n",
       "   'reason': \"File extension 'go' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'go' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\redis-config',\n",
       "   'reason': \"File extension 'redis-config' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'redis-config' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\profiles\\\\audit.json',\n",
       "   'reason': \"File extension 'json' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'json' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\profiles\\\\fine-grained.json',\n",
       "   'reason': \"File extension 'json' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'json' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\profiles\\\\violation.json',\n",
       "   'reason': \"File extension 'json' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'json' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\README.md',\n",
       "   'reason': \"File extension 'md' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'md' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\kind-with-cluster-level-baseline-pod-security.sh',\n",
       "   'reason': \"File extension 'sh' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'sh' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\kind-with-namespace-level-baseline-pod-security.sh',\n",
       "   'reason': \"File extension 'sh' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'sh' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\Dockerfile',\n",
       "   'reason': \"File extension 'Dockerfile' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'Dockerfile' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-nginx.conf',\n",
       "   'reason': \"File extension 'conf' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'conf' not in include list ['yaml', 'yml']\"},\n",
       "  {'file': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\tls\\\\server-signing-config.json',\n",
       "   'reason': \"File extension 'json' not in include list ['yaml', 'yml']\",\n",
       "   'traceback': None,\n",
       "   'summary': \"File extension 'json' not in include list ['yaml', 'yml']\"}])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_helper._scan_and_get_directories(\"knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\", {\"include\": [\"yaml\", \"yml\"], \"exclude\": [], \"min_length\": 0, \"max_length\": -1, \"generate_summary\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c156a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object(uuid=_WeaviateUUIDInt('0012f0f6-fb11-5f88-8507-3374b64724b5'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Pod configuration in YAML, focusing on setting up a liveness probe for container health monitoring. The Pod is named \"liveness-exec\" and contains a single container running the BusyBox image. The container executes a shell command that creates a file (`/tmp/healthy`), waits for 30 seconds, deletes the file, and then sleeps for an additional 600 seconds.\\n\\nThe key feature is the liveness probe, which periodically checks if the container is healthy by executing the `cat /tmp/healthy` command. If the file exists, the command succeeds, indicating the container is alive; if the file has been removed, the probe fails, prompting Kubernetes to restart the container. The probe starts after an initial delay of 5 seconds and repeats every 5 seconds. This setup ensures the container\\'s health is actively monitored based on the presence of the `/tmp/healthy` file, demonstrating how to implement custom, command-based liveness probes in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  labels:\\n    test: liveness\\n  name: liveness-exec\\nspec:\\n  containers:\\n  - name: liveness\\n    image: registry.k8s.io/busybox:1.27.2\\n    args:\\n    - /bin/sh\\n    - -c\\n    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\\n    livenessProbe:\\n      exec:\\n        command:\\n        - cat\\n        - /tmp/healthy\\n      initialDelaySeconds: 5\\n      periodSeconds: 5\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\exec-liveness.yaml', 'summary': 'This content describes a Kubernetes Pod configuration in YAML, focusing on setting up a liveness probe for container health monitoring. The Pod is named \"liveness-exec\" and contains a single container running the BusyBox image. The container executes a shell command that creates a file (`/tmp/healthy`), waits for 30 seconds, deletes the file, and then sleeps for an additional 600 seconds.\\n\\nThe key feature is the liveness probe, which periodically checks if the container is healthy by executing the `cat /tmp/healthy` command. If the file exists, the command succeeds, indicating the container is alive; if the file has been removed, the probe fails, prompting Kubernetes to restart the container. The probe starts after an initial delay of 5 seconds and repeats every 5 seconds. This setup ensures the container\\'s health is actively monitored based on the presence of the `/tmp/healthy` file, demonstrating how to implement custom, command-based liveness probes in Kubernetes.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('00c74257-bba5-51be-966e-95b064ec68aa'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The content provides a Kubernetes RoleBinding configuration that grants a user named \"jane\" permission to read pods within the \"default\" namespace. It assumes the existence of a Role called \"pod-reader\" that specifies the actual permissions. The RoleBinding links the user to this role, effectively allowing \"jane\" to perform read-only operations on pods, which is useful for access control and security management in Kubernetes clusters. The configuration includes details such as metadata, subject information (the user), and references to the role it binds to, ensuring precise permission assignment.\\napiVersion: rbac.authorization.k8s.io/v1\\n# This role binding allows \"jane\" to read pods in the \"default\" namespace.\\n# You need to already have a Role named \"pod-reader\" in that namespace.\\nkind: RoleBinding\\nmetadata:\\n  name: read-pods\\n  namespace: default\\nsubjects:\\n# You can specify more than one \"subject\"\\n- kind: User\\n  name: jane # \"name\" is case sensitive\\n  apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  # \"roleRef\" specifies the binding to a Role / ClusterRole\\n  kind: Role #this must be Role or ClusterRole\\n  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to\\n  apiGroup: rbac.authorization.k8s.io\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-rolebinding-with-role.yaml', 'summary': 'The content provides a Kubernetes RoleBinding configuration that grants a user named \"jane\" permission to read pods within the \"default\" namespace. It assumes the existence of a Role called \"pod-reader\" that specifies the actual permissions. The RoleBinding links the user to this role, effectively allowing \"jane\" to perform read-only operations on pods, which is useful for access control and security management in Kubernetes clusters. The configuration includes details such as metadata, subject information (the user), and references to the role it binds to, ensuring precise permission assignment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('01012e73-69fe-5c1a-a3fd-e3829b5376f9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with specific security settings and multiple container types. The Pod includes a `securityContext` that sets an unconfined seccomp profile at the Pod level, which allows broad system call permissions. It also contains an ephemeral container, an init container, and a main container, each with distinct seccomp profiles: the ephemeral and init containers use the `RuntimeDefault` profile, providing standard restricted profiles, while the main container uses a `Localhost` profile, referencing a custom security profile (`my-profile.json`) stored locally on the host.\\n\\nThe configuration demonstrates how to apply different seccomp profiles at multiple levels within a Pod, allowing fine-grained security control for containers during various phases of their lifecycle. This setup enhances security by tailoring system call restrictions to each container’s role, balancing isolation and necessary permissions.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod\\nspec:\\n  securityContext:\\n    seccompProfile:\\n      type: Unconfined\\n  ephemeralContainers:\\n  - name: ephemeral-container\\n    image: debian\\n    securityContext:\\n      seccompProfile:\\n        type: RuntimeDefault\\n  initContainers:\\n  - name: init-container\\n    image: debian\\n    securityContext:\\n      seccompProfile:\\n        type: RuntimeDefault\\n  containers:\\n  - name: container\\n    image: docker.io/library/debian:stable\\n    securityContext:\\n      seccompProfile:\\n        type: Localhost\\n        localhostProfile: my-profile.json\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod with specific security settings and multiple container types. The Pod includes a `securityContext` that sets an unconfined seccomp profile at the Pod level, which allows broad system call permissions. It also contains an ephemeral container, an init container, and a main container, each with distinct seccomp profiles: the ephemeral and init containers use the `RuntimeDefault` profile, providing standard restricted profiles, while the main container uses a `Localhost` profile, referencing a custom security profile (`my-profile.json`) stored locally on the host.\\n\\nThe configuration demonstrates how to apply different seccomp profiles at multiple levels within a Pod, allowing fine-grained security control for containers during various phases of their lifecycle. This setup enhances security by tailoring system call restrictions to each container’s role, balancing isolation and necessary permissions.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\fields.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('020d19ff-fa40-56e0-a135-486b89f143df'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes YAML manifest for creating a Secret resource. This Secret is named \"test-secret\" and stores sensitive data such as a username and password in encoded form. The \"data\" field contains base64-encoded strings: \"bXktYXBw\" for the username and \"Mzk1MjgkdmRnN0pi\" for the password. This type of resource is used to securely manage sensitive information in Kubernetes, allowing applications to access credentials without hardcoding them in plain text.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: test-secret\\ndata:\\n  username: bXktYXBw\\n  password: Mzk1MjgkdmRnN0pi\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes YAML manifest for creating a Secret resource. This Secret is named \"test-secret\" and stores sensitive data such as a username and password in encoded form. The \"data\" field contains base64-encoded strings: \"bXktYXBw\" for the username and \"Mzk1MjgkdmRnN0pi\" for the password. This type of resource is used to securely manage sensitive information in Kubernetes, allowing applications to access credentials without hardcoding them in plain text.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('03f111af-2a89-5828-b9e1-eac6156ef791'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Job configuration written in YAML, which specifies how to run a batch task with defined behaviors on failures. The job is configured to run eight completions with a maximum of two parallel pods at a time, and it uses a container based on the Bash 5 image. The container executes a script that prints a message, simulates a bug by exiting with code 42 after sleeping for 30 seconds. The `restartPolicy` is set to \"Never,\" so failed pods are not automatically restarted. \\n\\nA key feature in this configuration is the `podFailurePolicy`, which handles pod failures based on exit codes. Specifically, if the container named \"main\" exits with code 42, the `action: FailJob` triggers, causing the entire job to fail immediately. The `backoffLimit` is set to 6, allowing a maximum of six retries before considering the job as failed. This setup demonstrates how Kubernetes can be configured to handle specific failure scenarios, such as software bugs that produce certain exit codes, enabling more controlled and predictable job execution.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-pod-failure-policy-failjob\\nspec:\\n  completions: 8\\n  parallelism: 2\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      containers:\\n      - name: main\\n        image: docker.io/library/bash:5\\n        command: [\"bash\"]\\n        args:\\n        - -c\\n        - echo \"Hello world! I\\'m going to exit with 42 to simulate a software bug.\" && sleep 30 && exit 42\\n  backoffLimit: 6\\n  podFailurePolicy:\\n    rules:\\n    - action: FailJob\\n      onExitCodes:\\n        containerName: main\\n        operator: In\\n        values: [42]\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Job configuration written in YAML, which specifies how to run a batch task with defined behaviors on failures. The job is configured to run eight completions with a maximum of two parallel pods at a time, and it uses a container based on the Bash 5 image. The container executes a script that prints a message, simulates a bug by exiting with code 42 after sleeping for 30 seconds. The `restartPolicy` is set to \"Never,\" so failed pods are not automatically restarted. \\n\\nA key feature in this configuration is the `podFailurePolicy`, which handles pod failures based on exit codes. Specifically, if the container named \"main\" exits with code 42, the `action: FailJob` triggers, causing the entire job to fail immediately. The `backoffLimit` is set to 6, allowing a maximum of six retries before considering the job as failed. This setup demonstrates how Kubernetes can be configured to handle specific failure scenarios, such as software bugs that produce certain exit codes, enabling more controlled and predictable job execution.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-failjob.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0410aa7e-1885-5eac-8473-7515493c478c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration defined in YAML format. It specifies a pod named \"my-empty-dir-pod\" that runs a container using the Windows Server Core image. The pod includes two `emptyDir` volumes, \"cache-volume\" and \"scratch-volume,\" which are temporary storage solutions that are created when the pod starts and deleted when it terminates. These volumes are mounted inside the container at the specified paths: `/cache` and `C:/scratch`. The configuration also includes a node selector to ensure the pod runs on a Windows node, aligning the container\\'s OS with the host environment. This setup is useful for applications requiring ephemeral storage during runtime.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: my-empty-dir-pod\\nspec:\\n  containers:\\n  - image: microsoft/windowsservercore:1709\\n    name: my-empty-dir-pod\\n    volumeMounts:\\n    - mountPath: /cache\\n      name: cache-volume\\n    - mountPath: C:/scratch\\n      name: scratch-volume\\n  volumes:\\n  - name: cache-volume\\n    emptyDir: {}\\n  - name: scratch-volume\\n    emptyDir: {}\\n  nodeSelector:\\n    kubernetes.io/os: windows\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration defined in YAML format. It specifies a pod named \"my-empty-dir-pod\" that runs a container using the Windows Server Core image. The pod includes two `emptyDir` volumes, \"cache-volume\" and \"scratch-volume,\" which are temporary storage solutions that are created when the pod starts and deleted when it terminates. These volumes are mounted inside the container at the specified paths: `/cache` and `C:/scratch`. The configuration also includes a node selector to ensure the pod runs on a Windows node, aligning the container\\'s OS with the host environment. This setup is useful for applications requiring ephemeral storage during runtime.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\emptydir-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('04d52462-e9a5-5282-a6e8-3046a8fed438'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Service configuration for a MongoDB database. It defines a service with the name \"mongo\" that exposes port 27017, the default port for MongoDB. The service uses labels for identification and a selector to associate the service with the appropriate pods that have matching labels, ensuring that traffic directed to the service is routed to the correct MongoDB pod(s). This setup allows applications within the cluster to connect to the MongoDB database via the service\\'s stable network endpoint, abstracting the underlying pod IP addresses.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: mongo\\n  labels:\\n    app.kubernetes.io/name: mongo\\n    app.kubernetes.io/component: backend\\nspec:\\n  ports:\\n  - port: 27017\\n    targetPort: 27017\\n  selector:\\n    app.kubernetes.io/name: mongo\\n    app.kubernetes.io/component: backend\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Service configuration for a MongoDB database. It defines a service with the name \"mongo\" that exposes port 27017, the default port for MongoDB. The service uses labels for identification and a selector to associate the service with the appropriate pods that have matching labels, ensuring that traffic directed to the service is routed to the correct MongoDB pod(s). This setup allows applications within the cluster to connect to the MongoDB database via the service\\'s stable network endpoint, abstracting the underlying pod IP addresses.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('04e96862-938b-5b76-be75-0b558d3f1c75'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes YAML configuration defines a Pod named \"cpu-demo\" in the \"pod-resources-example\" namespace. The pod requests a minimum of 0.5 CPU and limits it to a maximum of 1 CPU, ensuring resource control. Inside the pod, there is a single container named \"cpu-demo-ctr\" that uses the \"vish/stress\" image, designed for stress testing CPU resources. The container runs with arguments to spawn two CPU stress workers (\"-cpus\" \"2\"), simulating CPU load to test resource management and performance under load.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: cpu-demo\\n  namespace: pod-resources-example\\nspec:\\n  resources:\\n    limits:\\n      cpu: \"1\"\\n    requests:\\n      cpu: \"0.5\"\\n  containers:\\n  - name: cpu-demo-ctr\\n    image: vish/stress\\n    args:\\n    - -cpus\\n    - \"2\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-cpu-request-limit.yaml', 'summary': 'This Kubernetes YAML configuration defines a Pod named \"cpu-demo\" in the \"pod-resources-example\" namespace. The pod requests a minimum of 0.5 CPU and limits it to a maximum of 1 CPU, ensuring resource control. Inside the pod, there is a single container named \"cpu-demo-ctr\" that uses the \"vish/stress\" image, designed for stress testing CPU resources. The container runs with arguments to spawn two CPU stress workers (\"-cpus\" \"2\"), simulating CPU load to test resource management and performance under load.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('050632a5-813d-557a-a006-0733c347635b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"shell-demo\" with a single container running the nginx image. The Pod uses an emptyDir volume called \"shared-data,\" which provides a temporary shared storage accessible by the containers within the Pod. The nginx container mounts this volume at the path \"/usr/share/nginx/html,\" allowing it to serve static content stored in the shared volume. The configuration also specifies that the Pod utilizes the host\\'s network namespace with \"hostNetwork: true,\" and sets the DNS policy to default. Overall, this setup enables nginx to serve shared data directly from the shared volume, facilitating simple file sharing within the container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: shell-demo\\nspec:\\n  volumes:\\n  - name: shared-data\\n    emptyDir: {}\\n  containers:\\n  - name: nginx\\n    image: nginx\\n    volumeMounts:\\n    - name: shared-data\\n      mountPath: /usr/share/nginx/html\\n  hostNetwork: true\\n  dnsPolicy: Default\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"shell-demo\" with a single container running the nginx image. The Pod uses an emptyDir volume called \"shared-data,\" which provides a temporary shared storage accessible by the containers within the Pod. The nginx container mounts this volume at the path \"/usr/share/nginx/html,\" allowing it to serve static content stored in the shared volume. The configuration also specifies that the Pod utilizes the host\\'s network namespace with \"hostNetwork: true,\" and sets the DNS policy to default. Overall, this setup enables nginx to serve shared data directly from the shared volume, facilitating simple file sharing within the container.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\shell-demo.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('05641183-5f3e-5294-b7ac-2eaaac34fbad'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"violation-pod\" with specific metadata, including labels and annotations. The annotation links to a Seccomp profile (\"localhost/profiles/violation.json\"), which is used to restrict system calls for enhanced security. The Pod contains a single container using the \"hashicorp/http-echo:0.2.3\" image, configured to display the message \"just made some syscalls!\" when executed. The security context of the container explicitly disallows privilege escalation, adding an extra layer of security. Overall, this configuration exemplifies how to deploy a container with security policies like Seccomp profiles and privilege restrictions within Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: violation-pod\\n  labels:\\n    app: violation-pod\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/violation.json\\nspec:\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:0.2.3\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"violation-pod\" with specific metadata, including labels and annotations. The annotation links to a Seccomp profile (\"localhost/profiles/violation.json\"), which is used to restrict system calls for enhanced security. The Pod contains a single container using the \"hashicorp/http-echo:0.2.3\" image, configured to display the message \"just made some syscalls!\" when executed. The security context of the container explicitly disallows privilege escalation, adding an extra layer of security. Overall, this configuration exemplifies how to deploy a container with security policies like Seccomp profiles and privilege restrictions within Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\violation-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('05ec98ce-6129-5a5e-acbc-1bc85e56f264'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The content is a Kubernetes YAML configuration defining a Deployment that manages three replica pods running an Alpine Linux container. The container continuously executes a shell command that prints the current date and the contents of a configuration file, updating every 10 seconds. The configuration uses a ConfigMap volume named `company-name-20150801`, which is mounted at `/etc/config` inside each container, making the configuration data accessible to the running application. This setup enables dynamic configuration management and demonstrates how ConfigMaps can be mounted as volumes to provide configuration data to containers.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: immutable-configmap-volume\\n  labels:\\n    app.kubernetes.io/name: immutable-configmap-volume\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: immutable-configmap-volume\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: immutable-configmap-volume\\n    spec:\\n      containers:\\n        - name: alpine\\n          image: alpine:3\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) The name of the company is $(cat /etc/config/company_name)\";\\n              sleep 10; done;\\n          ports:\\n            - containerPort: 80\\n          volumeMounts:\\n            - name: config-volume\\n              mountPath: /etc/config\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: company-name-20150801', 'subchunk': '1/1', 'summary': 'The content is a Kubernetes YAML configuration defining a Deployment that manages three replica pods running an Alpine Linux container. The container continuously executes a shell command that prints the current date and the contents of a configuration file, updating every 10 seconds. The configuration uses a ConfigMap volume named `company-name-20150801`, which is mounted at `/etc/config` inside each container, making the configuration data accessible to the running application. This setup enables dynamic configuration management and demonstrates how ConfigMaps can be mounted as volumes to provide configuration data to containers.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-immutable-configmap-as-volume.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('06735c5a-6b38-5e90-bfbf-cf1dab7d3149'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Job that executes a simple task using an Alpine Linux container. The main container, named \"myjob,\" runs a command to write the text \"logging\" into a file located at /opt/logs.txt. It mounts a shared volume at /opt to enable data persistence and sharing with an init container. The init container, \"logshipper,\" also based on Alpine, continuously tails the logs file using the command `tail -F /opt/logs.txt`, effectively monitoring new entries in real-time. Both containers share an ephemeral empty directory volume, which provides temporary storage accessible during the job\\'s execution. The overall design allows for initial log processing or monitoring during the job execution, with the main container performing a logging task and the init container handling real-time log tailing before the main container completes.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: myjob\\nspec:\\n  template:\\n    spec:\\n      containers:\\n        - name: myjob\\n          image: alpine:latest\\n          command: [\\'sh\\', \\'-c\\', \\'echo \"logging\" > /opt/logs.txt\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      initContainers:\\n        - name: logshipper\\n          image: alpine:latest\\n          restartPolicy: Always\\n          command: [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      restartPolicy: Never\\n      volumes:\\n        - name: data\\n          emptyDir: {}', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Job that executes a simple task using an Alpine Linux container. The main container, named \"myjob,\" runs a command to write the text \"logging\" into a file located at /opt/logs.txt. It mounts a shared volume at /opt to enable data persistence and sharing with an init container. The init container, \"logshipper,\" also based on Alpine, continuously tails the logs file using the command `tail -F /opt/logs.txt`, effectively monitoring new entries in real-time. Both containers share an ephemeral empty directory volume, which provides temporary storage accessible during the job\\'s execution. The overall design allows for initial log processing or monitoring during the job execution, with the main container performing a logging task and the init container handling real-time log tailing before the main container completes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-sidecar.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('08055b25-c9b4-5aac-8ded-718a0f4c8bba'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes cluster using Kind (Kubernetes IN Docker). It specifies a cluster with a single control-plane node and includes an extra mount point. The extraMounts section lines up a directory from the host machine (`./profiles`) to a specific path inside the container (`/var/lib/kubelet/seccomp/profiles`). This setup allows the control-plane node to access custom seccomp profiles stored on the host, facilitating security and system call filtering configurations within the cluster.\\napiVersion: kind.x-k8s.io/v1alpha4\\nkind: Cluster\\nnodes:\\n- role: control-plane\\n  extraMounts:\\n  - hostPath: \"./profiles\"\\n    containerPath: \"/var/lib/kubelet/seccomp/profiles\"', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes cluster using Kind (Kubernetes IN Docker). It specifies a cluster with a single control-plane node and includes an extra mount point. The extraMounts section lines up a directory from the host machine (`./profiles`) to a specific path inside the container (`/var/lib/kubelet/seccomp/profiles`). This setup allows the control-plane node to access custom seccomp profiles stored on the host, facilitating security and system call filtering configurations within the cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\kind.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('081b60e4-37ca-5b3f-90de-6f44ee3d2b30'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes resource of kind `ValidatingAdmissionPolicyBinding`, which is used to associate a specific admission policy with resources in the cluster. The binding, named \"replicalimit-binding-test.example.com\", references a validation policy called \"replicalimit-policy.example.com\" and applies it with a \"Deny\" action, meaning requests that violate the policy will be rejected. The `paramRef` specifies an external configuration or parameters named \"replica-limit-test.example.com\" within the \"default\" namespace, providing context or constraints for the validation. The `matchResources` section ensures that this policy binding only applies to resources within namespaces labeled with `environment: test`. Overall, this setup enforces custom validation rules specifically for test environment namespaces, likely related to controls on resource replication or scaling limits.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicyBinding\\nmetadata:\\n  name: \"replicalimit-binding-test.example.com\"\\nspec:\\n  policyName: \"replicalimit-policy.example.com\"\\n  validationActions: [Deny]\\n  paramRef:\\n    name: \"replica-limit-test.example.com\"\\n    namespace: \"default\"\\n  matchResources:\\n    namespaceSelector:\\n      matchLabels:\\n        environment: test', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\binding-with-param.yaml', 'summary': 'This content defines a Kubernetes resource of kind `ValidatingAdmissionPolicyBinding`, which is used to associate a specific admission policy with resources in the cluster. The binding, named \"replicalimit-binding-test.example.com\", references a validation policy called \"replicalimit-policy.example.com\" and applies it with a \"Deny\" action, meaning requests that violate the policy will be rejected. The `paramRef` specifies an external configuration or parameters named \"replica-limit-test.example.com\" within the \"default\" namespace, providing context or constraints for the validation. The `matchResources` section ensures that this policy binding only applies to resources within namespaces labeled with `environment: test`. Overall, this setup enforces custom validation rules specifically for test environment namespaces, likely related to controls on resource replication or scaling limits.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('089765f5-534b-523d-ac29-b3adf5a687ab'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The given content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"extended-resource-demo-2\" that contains a single container running the nginx image. The configuration specifies resource requests and limits for a custom extended resource called \"example.com/dongle,\" setting both to 2 units. This ensures that the container requests a minimum of 2 units of the extended resource and is limited to using up to 2 units, enabling efficient resource allocation and management within a Kubernetes cluster that supports custom resources.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: extended-resource-demo-2\\nspec:\\n  containers:\\n  - name: extended-resource-demo-2-ctr\\n    image: nginx\\n    resources:\\n      requests:\\n        example.com/dongle: 2\\n      limits:\\n        example.com/dongle: 2\\n', 'subchunk': '1/1', 'summary': 'The given content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"extended-resource-demo-2\" that contains a single container running the nginx image. The configuration specifies resource requests and limits for a custom extended resource called \"example.com/dongle,\" setting both to 2 units. This ensures that the container requests a minimum of 2 units of the extended resource and is limited to using up to 2 units, enabling efficient resource allocation and management within a Kubernetes cluster that supports custom resources.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('09c94826-73b2-5ba2-99d8-464ea8315368'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"nginx,\" which runs a single container based on the official Nginx image. The container exposes port 80, allowing it to serve HTTP traffic. This configuration is simple and illustrates how to deploy a basic Nginx server within a Kubernetes environment. It demonstrates the essential elements needed to specify a pod\\'s metadata, container image, container name, and the ports that the container will listen on.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  containers:\\n    - image: nginx\\n      name: nginx\\n      ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"nginx,\" which runs a single container based on the official Nginx image. The container exposes port 80, allowing it to serve HTTP traffic. This configuration is simple and illustrates how to deploy a basic Nginx server within a Kubernetes environment. It demonstrates the essential elements needed to specify a pod\\'s metadata, container image, container name, and the ports that the container will listen on.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\example-baseline-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0a1d9abc-3529-50ea-82f2-64c78b929f36'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content includes Kubernetes configuration files for deploying a MySQL database in a containerized environment. It consists of two main resources: a Service and a Deployment. The Service is set up with `clusterIP: None`, which creates a Headless Service allowing direct access to individual MySQL pods for stateful applications or discovery purposes. It exposes port 3306 and uses a label selector to target pods with the label `app: mysql`.\\n\\nThe Deployment defines the desired state for running MySQL version 5.6, including replica management and update strategies. It specifies a containerized MySQL instance with environment variables for configuration, such as setting the root password (note that in practical scenarios, secrets should be securely managed rather than hardcoded). The deployment also uses a volume mount to attach persistent storage, ensuring data durability, by linking to a PersistentVolumeClaim named `mysql-pv-claim`. These configurations automate the deployment, scaling, and persistence of MySQL within a Kubernetes cluster, enabling a reliable and manageable database service.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: mysql\\nspec:\\n  ports:\\n  - port: 3306\\n  selector:\\n    app: mysql\\n  clusterIP: None\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: mysql\\nspec:\\n  selector:\\n    matchLabels:\\n      app: mysql\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      labels:\\n        app: mysql\\n    spec:\\n      containers:\\n      - image: mysql:5.6\\n        name: mysql\\n        env:\\n          # Use secret in real usage\\n        - name: MYSQL_ROOT_PASSWORD\\n          value: password\\n        ports:\\n        - containerPort: 3306\\n          name: mysql\\n        volumeMounts:\\n        - name: mysql-persistent-storage\\n          mountPath: /var/lib/mysql\\n      volumes:\\n      - name: mysql-persistent-storage\\n        persistentVolumeClaim:\\n          claimName: mysql-pv-claim\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-deployment.yaml', 'summary': 'The provided content includes Kubernetes configuration files for deploying a MySQL database in a containerized environment. It consists of two main resources: a Service and a Deployment. The Service is set up with `clusterIP: None`, which creates a Headless Service allowing direct access to individual MySQL pods for stateful applications or discovery purposes. It exposes port 3306 and uses a label selector to target pods with the label `app: mysql`.\\n\\nThe Deployment defines the desired state for running MySQL version 5.6, including replica management and update strategies. It specifies a containerized MySQL instance with environment variables for configuration, such as setting the root password (note that in practical scenarios, secrets should be securely managed rather than hardcoded). The deployment also uses a volume mount to attach persistent storage, ensuring data durability, by linking to a PersistentVolumeClaim named `mysql-pv-claim`. These configurations automate the deployment, scaling, and persistence of MySQL within a Kubernetes cluster, enabling a reliable and manageable database service.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0a901872-754e-5a66-a3a0-ca097cd03d83'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"envar-demo\" with a container called \"envar-demo-container\" that uses the image \"gcr.io/google-samples/hello-app:2.0\". The configuration demonstrates the use of environment variables by setting two variables: DEMO_GREETING and DEMO_FAREWELL, with specified string values. This setup is useful for injecting environment-specific data into containers, enabling dynamic configuration and behavior based on environment variables during runtime.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: envar-demo\\n  labels:\\n    purpose: demonstrate-envars\\nspec:\\n  containers:\\n  - name: envar-demo-container\\n    image: gcr.io/google-samples/hello-app:2.0\\n    env:\\n    - name: DEMO_GREETING\\n      value: \"Hello from the environment\"\\n    - name: DEMO_FAREWELL\\n      value: \"Such a sweet sorrow\"\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"envar-demo\" with a container called \"envar-demo-container\" that uses the image \"gcr.io/google-samples/hello-app:2.0\". The configuration demonstrates the use of environment variables by setting two variables: DEMO_GREETING and DEMO_FAREWELL, with specified string values. This setup is useful for injecting environment-specific data into containers, enabling dynamic configuration and behavior based on environment variables during runtime.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\envars.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0aa7b824-de37-50cd-aaa1-a288ae17addd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Deployment configuration written in YAML. It defines a deployment named \"iis\" that manages three replicas of a container running the Microsoft IIS web server. The deployment uses a label selector to identify the pods it manages and specifies container details including the image (\"microsoft/iis\") and a container port (80). Additionally, it includes an annotation indicating the use of Hyper-V isolation type for Windows containers, ensuring better isolation and compatibility on Windows nodes. This configuration automates the deployment and scaling of IIS web server instances within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iis\\nspec:\\n  selector:\\n    matchLabels:\\n      app: iis\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: iis\\n      annotations:\\n        experimental.windows.kubernetes.io/isolation-type: hyperv\\n    spec:\\n      containers:\\n      - name: iis\\n        image: microsoft/iis\\n        ports:\\n        - containerPort: 80\\n\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Deployment configuration written in YAML. It defines a deployment named \"iis\" that manages three replicas of a container running the Microsoft IIS web server. The deployment uses a label selector to identify the pods it manages and specifies container details including the image (\"microsoft/iis\") and a container port (80). Additionally, it includes an annotation indicating the use of Hyper-V isolation type for Windows containers, ensuring better isolation and compatibility on Windows nodes. This configuration automates the deployment and scaling of IIS web server instances within a Kubernetes cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-hyperv.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0b060f5c-1b15-50a2-93ca-07b42a1da79b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code is a Kubernetes pod configuration in YAML format. It defines a simple pod named \"test-pod\" which runs a single container with the name \"pause.\" The container uses the \"registry.k8s.io/pause:3.6\" image, which is typically used as a placeholder or to keep the pod alive without performing any specific tasks. This configuration is straightforward and mainly serves as a minimal example of creating a pod in Kubernetes, focusing on defining the pod\\'s metadata and the container\\'s details.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: test-pod\\nspec:\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.6\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-without-scheduling-gates.yaml', 'summary': 'The provided code is a Kubernetes pod configuration in YAML format. It defines a simple pod named \"test-pod\" which runs a single container with the name \"pause.\" The container uses the \"registry.k8s.io/pause:3.6\" image, which is typically used as a placeholder or to keep the pod alive without performing any specific tasks. This configuration is straightforward and mainly serves as a minimal example of creating a pod in Kubernetes, focusing on defining the pod\\'s metadata and the container\\'s details.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0be102c0-07be-5a7e-b234-be450b457338'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"qos-demo-2\" in the \"qos-example\" namespace. The Pod includes a single container that uses the \"nginx\" image. The resource specifications for the container set a memory request of 100Mi and a limit of 200Mi. This configuration is designed to demonstrate Kubernetes Quality of Service (QoS) classes based on how resource requests and limits are set: a pod with a request and limit on memory will fall into the \"Burstable\" QoS class, allowing some flexibility in resource management and prioritization during contention.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: qos-demo-2\\n  namespace: qos-example\\nspec:\\n  containers:\\n  - name: qos-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n      requests:\\n        memory: \"100Mi\"\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"qos-demo-2\" in the \"qos-example\" namespace. The Pod includes a single container that uses the \"nginx\" image. The resource specifications for the container set a memory request of 100Mi and a limit of 200Mi. This configuration is designed to demonstrate Kubernetes Quality of Service (QoS) classes based on how resource requests and limits are set: a pod with a request and limit on memory will fall into the \"Burstable\" QoS class, allowing some flexibility in resource management and prioritization during contention.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0d205fd2-8790-550b-9561-e9b72dd505f1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a custom resource of kind `ReplicaLimit` in the `rules.example.com/v1` API group. It specifies a limit on the number of replicas, setting the maximum to 100 for the resource named \"replica-limit-prod.example.com\". This type of configuration is typically used in Kubernetes to enforce scaling policies, ensuring that the number of replica pods does not exceed the defined limit, which helps in resource management and maintaining cluster stability.\\napiVersion: rules.example.com/v1\\nkind: ReplicaLimit\\nmetadata:\\n  name: \"replica-limit-prod.example.com\"\\nmaxReplicas: 100', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param-prod.yaml', 'summary': 'This YAML configuration defines a custom resource of kind `ReplicaLimit` in the `rules.example.com/v1` API group. It specifies a limit on the number of replicas, setting the maximum to 100 for the resource named \"replica-limit-prod.example.com\". This type of configuration is typically used in Kubernetes to enforce scaling policies, ensuring that the number of replica pods does not exceed the defined limit, which helps in resource management and maintaining cluster stability.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0d6b2594-ac27-5441-b9d9-04a7f4bce6c2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code is a Kubernetes pod configuration in YAML format. It specifies the creation of a pod named \"qos-demo-3\" within the \"qos-example\" namespace. The pod contains a single container named \"qos-demo-3-ctr\" which uses the \"nginx\" image, a popular web server. This configuration is straightforward and primarily used for deploying a lightweight nginx server within a Kubernetes environment. It serves as a basic example of defining a pod with minimal specifications.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: qos-demo-3\\n  namespace: qos-example\\nspec:\\n  containers:\\n  - name: qos-demo-3-ctr\\n    image: nginx\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-3.yaml', 'summary': 'The provided code is a Kubernetes pod configuration in YAML format. It specifies the creation of a pod named \"qos-demo-3\" within the \"qos-example\" namespace. The pod contains a single container named \"qos-demo-3-ctr\" which uses the \"nginx\" image, a popular web server. This configuration is straightforward and primarily used for deploying a lightweight nginx server within a Kubernetes environment. It serves as a basic example of defining a pod with minimal specifications.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0df328cb-ad1c-51e0-bf9f-ae4340d20f8d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ValidatingAdmissionPolicy, which is used to enforce specific validation rules on resources during their creation or update. The policy is named \"replica-policy.example.com\" and applies to \"deployments\" and \"replicasets\" resources within the \"apps\" API group and \"v1\" version. It specifically targets \"CREATE\" and \"UPDATE\" operations. The core validation logic involves an expression that checks whether the number of replicas specified in the resource is greater than 1, with a custom message \"must be replicated\" and an invalid reason if the validation fails. \\n\\nHowever, the expression contains a mistake: it references \"object.replicas\" instead of the correct \"object.spec.replicas\". When functioning correctly, the policy ensures that any deployment or replicaset resource created or updated must have more than one replica, enforcing a replication requirement.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"replica-policy.example.com\"\\nspec:\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\",\"replicasets\"]\\n  validations:\\n  - expression: \"object.replicas > 1\" # should be \"object.spec.replicas > 1\"\\n    message: \"must be replicated\"\\n    reason: Invalid', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ValidatingAdmissionPolicy, which is used to enforce specific validation rules on resources during their creation or update. The policy is named \"replica-policy.example.com\" and applies to \"deployments\" and \"replicasets\" resources within the \"apps\" API group and \"v1\" version. It specifically targets \"CREATE\" and \"UPDATE\" operations. The core validation logic involves an expression that checks whether the number of replicas specified in the resource is greater than 1, with a custom message \"must be replicated\" and an invalid reason if the validation fails. \\n\\nHowever, the expression contains a mistake: it references \"object.replicas\" instead of the correct \"object.spec.replicas\". When functioning correctly, the policy ensures that any deployment or replicaset resource created or updated must have more than one replica, enforcing a replication requirement.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\typechecking-multiple-match.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('0e6d45c1-4973-5e79-92c1-176732788cc1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a YAML configuration for a Kubernetes Pod, specifying how a containerized application should run on the cluster. It includes metadata such as the pod\\'s name and specifies that the pod should be scheduled on a node labeled \"kube-01\". The pod uses a PersistentVolumeClaim named \"task-pv-claim\" for storage, which is mounted inside the container at \"/usr/share/nginx/html\". The container runs the Nginx image and exposes port 80 for HTTP traffic, serving as a web server that serves the content stored in the persistent volume. This configuration demonstrates how to deploy a stateless web server with persistent storage in Kubernetes, ensuring data continuity even if the container restarts.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: task-pv-pod\\nspec:\\n  nodeSelector:\\n    kubernetes.io/hostname: kube-01\\n  volumes:\\n    - name: task-pv-storage\\n      persistentVolumeClaim:\\n        claimName: task-pv-claim\\n  containers:\\n    - name: task-pv-container\\n      image: nginx\\n      ports:\\n        - containerPort: 80\\n          name: \"http-server\"\\n      volumeMounts:\\n        - mountPath: \"/usr/share/nginx/html\"\\n          name: task-pv-storage\\n', 'subchunk': '1/1', 'summary': 'The provided content is a YAML configuration for a Kubernetes Pod, specifying how a containerized application should run on the cluster. It includes metadata such as the pod\\'s name and specifies that the pod should be scheduled on a node labeled \"kube-01\". The pod uses a PersistentVolumeClaim named \"task-pv-claim\" for storage, which is mounted inside the container at \"/usr/share/nginx/html\". The container runs the Nginx image and exposes port 80 for HTTP traffic, serving as a web server that serves the content stored in the persistent volume. This configuration demonstrates how to deploy a stateless web server with persistent storage in Kubernetes, ensuring data continuity even if the container restarts.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\pod-volume-binding.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('108d03f9-acea-5160-8fe5-89086d210df9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration YAML file. It defines a Pod named \"secret-test-pod\" that runs an Nginx container. The key feature of this configuration is the use of a secret volume, which mounts secret data into the container\\'s filesystem at the path \"/etc/secret-volume\" in a read-only mode. The volume is linked to a Kubernetes secret named \"test-secret,\" allowing sensitive data stored in the secret to be securely accessed by the container without exposing it in the container image or environment variables. This approach is fundamental for managing secrets safely in Kubernetes, enabling secure access to sensitive information such as passwords, API keys, or tokens within containers.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: secret-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: nginx\\n      volumeMounts:\\n        # name must match the volume name below\\n        - name: secret-volume\\n          mountPath: /etc/secret-volume\\n          readOnly: true\\n  # The secret data is exposed to Containers in the Pod through a Volume.\\n  volumes:\\n    - name: secret-volume\\n      secret:\\n        secretName: test-secret\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret-pod.yaml', 'summary': 'The provided content is a Kubernetes Pod configuration YAML file. It defines a Pod named \"secret-test-pod\" that runs an Nginx container. The key feature of this configuration is the use of a secret volume, which mounts secret data into the container\\'s filesystem at the path \"/etc/secret-volume\" in a read-only mode. The volume is linked to a Kubernetes secret named \"test-secret,\" allowing sensitive data stored in the secret to be securely accessed by the container without exposing it in the container image or environment variables. This approach is fundamental for managing secrets safely in Kubernetes, enabling secure access to sensitive information such as passwords, API keys, or tokens within containers.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('11aea462-890e-5798-8b45-5becddbbab31'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes manifest defines a Pod named `constraints-cpu-demo-2` with a single container running the `nginx` image. The key focus is on resource management, specifically CPU constraints. The container has a CPU request of 500 millicores (0.5 CPU) and a CPU limit of 1.5 cores, indicating it requests half a CPU for initial scheduling and can use up to one and a half cores when needed. This setup ensures that the container has a guaranteed minimum CPU and a maximum utilization limit, helping manage resource allocation effectively in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo-2\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"1.5\"\\n      requests:\\n        cpu: \"500m\"\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes manifest defines a Pod named `constraints-cpu-demo-2` with a single container running the `nginx` image. The key focus is on resource management, specifically CPU constraints. The container has a CPU request of 500 millicores (0.5 CPU) and a CPU limit of 1.5 cores, indicating it requests half a CPU for initial scheduling and can use up to one and a half cores when needed. This setup ensures that the container has a guaranteed minimum CPU and a maximum utilization limit, helping manage resource allocation effectively in the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('12023c28-c839-5a67-87f0-af6a094fc761'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named `nginx-deployment` that manages a pod running an Nginx container. The deployment specifies a label selector to match the pods it manages, with the label `app: nginx`. The pod template within the deployment describes a container named `nginx` using the `nginx:1.16.1` Docker image, which listens on port 80. \\n\\nThe purpose of this configuration is to automate the deployment and management of an Nginx server within a Kubernetes cluster, ensuring it can be scaled, updated, and maintained easily. The YAML specifies the necessary parameters for Kubernetes to create and manage the containerized Nginx application correctly.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1 # update the image\\n        ports:\\n        - containerPort: 80\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named `nginx-deployment` that manages a pod running an Nginx container. The deployment specifies a label selector to match the pods it manages, with the label `app: nginx`. The pod template within the deployment describes a container named `nginx` using the `nginx:1.16.1` Docker image, which listens on port 80. \\n\\nThe purpose of this configuration is to automate the deployment and management of an Nginx server within a Kubernetes cluster, ensuring it can be scaled, updated, and maintained easily. The YAML specifies the necessary parameters for Kubernetes to create and manage the containerized Nginx application correctly.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\update_deployment.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('12c24b3e-31cf-583a-ac99-559e5da2019f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Ingress resource, which manages external access to services within a cluster based on hostname and path routing rules. The Ingress is named \"name-virtual-host-ingress\" and contains two rules corresponding to two different hostnames: \"foo.bar.com\" and \"bar.foo.com\". For each hostname, the ingress directs incoming HTTP traffic with a URL prefix of \"/\" to specific backend services. Traffic destined for \"foo.bar.com\" is routed to \"service1\" on port 80, while traffic for \"bar.foo.com\" is routed to \"service2\" on port 80. This setup enables multiple virtual hosts to be served through a single ingress point, facilitating domain-based routing within a Kubernetes cluster.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: name-virtual-host-ingress\\nspec:\\n  rules:\\n  - host: foo.bar.com\\n    http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/\"\\n        backend:\\n          service:\\n            name: service1\\n            port:\\n              number: 80\\n  - host: bar.foo.com\\n    http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/\"\\n        backend:\\n          service:\\n            name: service2\\n            port:\\n              number: 80\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\name-virtual-host-ingress.yaml', 'summary': 'The provided YAML configuration defines a Kubernetes Ingress resource, which manages external access to services within a cluster based on hostname and path routing rules. The Ingress is named \"name-virtual-host-ingress\" and contains two rules corresponding to two different hostnames: \"foo.bar.com\" and \"bar.foo.com\". For each hostname, the ingress directs incoming HTTP traffic with a URL prefix of \"/\" to specific backend services. Traffic destined for \"foo.bar.com\" is routed to \"service1\" on port 80, while traffic for \"bar.foo.com\" is routed to \"service2\" on port 80. This setup enables multiple virtual hosts to be served through a single ingress point, facilitating domain-based routing within a Kubernetes cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('144d167e-d6d6-53d0-8f32-051415038600'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes NetworkPolicy named \"default-deny-all.\" It applies to all pods in the cluster since the podSelector is empty, meaning no specific labels are targeted. The policy specifies both ingress and egress rules, effectively blocking all inbound and outbound network traffic for the pods unless additional rules are added later. This policy is typically used as a default deny rule to enhance security by restricting network access until specific allowances are configured.\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: default-deny-all\\nspec:\\n  podSelector: {}\\n  policyTypes:\\n  - Ingress\\n  - Egress\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes NetworkPolicy named \"default-deny-all.\" It applies to all pods in the cluster since the podSelector is empty, meaning no specific labels are targeted. The policy specifies both ingress and egress rules, effectively blocking all inbound and outbound network traffic for the pods unless additional rules are added later. This policy is typically used as a default deny rule to enhance security by restricting network access until specific allowances are configured.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-all.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('159ffc32-e545-5c85-ad62-6cfe0b75ca67'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Pod named \"counter\" with three containers that work together to generate and monitor log entries. The first container, \"count,\" runs a Bash script within a Busybox image that continuously increments a counter every second, logs the current count with a timestamp to two log files, and thus demonstrates a simple log-generating process. The other two containers, \"count-log-1\" and \"count-log-2,\" continuously follow (tail -f) these log files to monitor new entries in real-time.\\n\\nThe code\\'s main function is to simulate a logging scenario where one container produces log entries, while the others act as log consumers or viewers. This setup showcases container coordination via shared volumes; all three containers mount the same shared directory, /var/log, provided by an emptyDir volume, ensuring that log files are accessible across containers. Overall, the configuration highlights inter-container communication through shared storage, continuous log generation, and real-time log monitoring, which are fundamental concepts in DevOps and container orchestration.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: counter\\nspec:\\n  containers:\\n  - name: count\\n    image: busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  - name: count-log-1\\n    image: busybox:1.28\\n    args: [/bin/sh, -c, \\'tail -n+1 -F /var/log/1.log\\']\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  - name: count-log-2\\n    image: busybox:1.28\\n    args: [/bin/sh, -c, \\'tail -n+1 -F /var/log/2.log\\']\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  volumes:\\n  - name: varlog\\n    emptyDir: {}\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-streaming-sidecar.yaml', 'summary': 'This Kubernetes configuration defines a Pod named \"counter\" with three containers that work together to generate and monitor log entries. The first container, \"count,\" runs a Bash script within a Busybox image that continuously increments a counter every second, logs the current count with a timestamp to two log files, and thus demonstrates a simple log-generating process. The other two containers, \"count-log-1\" and \"count-log-2,\" continuously follow (tail -f) these log files to monitor new entries in real-time.\\n\\nThe code\\'s main function is to simulate a logging scenario where one container produces log entries, while the others act as log consumers or viewers. This setup showcases container coordination via shared volumes; all three containers mount the same shared directory, /var/log, provided by an emptyDir volume, ensuring that log files are accessible across containers. Overall, the configuration highlights inter-container communication through shared storage, continuous log generation, and real-time log monitoring, which are fundamental concepts in DevOps and container orchestration.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('180014cb-8679-5609-94e1-6bbb7d29e246'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML, focusing on the use of the `hostAliases` feature. It defines a Pod named `hostaliases-pod` with a restart policy set to \"Never,\" meaning it will not restart automatically after termination. The `hostAliases` section allows mapping custom hostnames to specific IP addresses directly within the Pod\\'s `/etc/hosts` file. In this case, two sets of host-to-IP mappings are specified: one linking `foo.local` and `bar.local` to localhost (`127.0.0.1`), and another associating `foo.remote` and `bar.remote` with the IP `10.1.2.3`.\\n\\nThe Pod contains a single container named `cat-hosts`, which uses the `busybox:1.28` image. This container runs the command `cat /etc/hosts`, effectively displaying the contents of the Pod’s hosts file, which will include the custom hostname-to-IP mappings defined in `hostAliases`. This setup is useful for testing, debugging, or customizing hostname resolution within a Pod environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: hostaliases-pod\\nspec:\\n  restartPolicy: Never\\n  hostAliases:\\n  - ip: \"127.0.0.1\"\\n    hostnames:\\n    - \"foo.local\"\\n    - \"bar.local\"\\n  - ip: \"10.1.2.3\"\\n    hostnames:\\n    - \"foo.remote\"\\n    - \"bar.remote\"\\n  containers:\\n  - name: cat-hosts\\n    image: busybox:1.28\\n    command:\\n    - cat\\n    args:\\n    - \"/etc/hosts\"\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML, focusing on the use of the `hostAliases` feature. It defines a Pod named `hostaliases-pod` with a restart policy set to \"Never,\" meaning it will not restart automatically after termination. The `hostAliases` section allows mapping custom hostnames to specific IP addresses directly within the Pod\\'s `/etc/hosts` file. In this case, two sets of host-to-IP mappings are specified: one linking `foo.local` and `bar.local` to localhost (`127.0.0.1`), and another associating `foo.remote` and `bar.remote` with the IP `10.1.2.3`.\\n\\nThe Pod contains a single container named `cat-hosts`, which uses the `busybox:1.28` image. This container runs the command `cat /etc/hosts`, effectively displaying the contents of the Pod’s hosts file, which will include the custom hostname-to-IP mappings defined in `hostAliases`. This setup is useful for testing, debugging, or customizing hostname resolution within a Pod environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\hostaliases-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1b3914fd-23fb-5227-ab3e-26a5fd45c35d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes CustomResourceDefinition (CRD), which is a way to extend the Kubernetes API with custom resource types. The CRD specifies a new resource called \"Shirt\" under the group \"stable.example.com,\" with the plural name \"shirts\" and singular \"shirt.\" It is namespaced, meaning each Shirt resource exists within a specific namespace.\\n\\nThe CRD includes schema validation details, indicating that each Shirt resource has a \"spec\" object with properties \"color\" and \"size,\" both of which are strings. It also defines which fields can be used for selection (--selectableFields--) and specifies additional columns (\"Color\" and \"Size\") that will appear when listing these resources in the kubectl CLI.\\n\\nOverall, this CRD allows users to create, manage, and view custom \"Shirt\" resources with defined properties, integrating seamlessly into Kubernetes cluster management.\\napiVersion: apiextensions.k8s.io/v1\\nkind: CustomResourceDefinition\\nmetadata:\\n  name: shirts.stable.example.com\\nspec:\\n  group: stable.example.com\\n  scope: Namespaced\\n  names:\\n    plural: shirts\\n    singular: shirt\\n    kind: Shirt\\n  versions:\\n  - name: v1\\n    served: true\\n    storage: true\\n    schema:\\n      openAPIV3Schema:\\n        type: object\\n        properties:\\n          spec:\\n            type: object\\n            properties:\\n              color:\\n                type: string\\n              size:\\n                type: string\\n    selectableFields:\\n    - jsonPath: .spec.color\\n    - jsonPath: .spec.size\\n    additionalPrinterColumns:\\n    - jsonPath: .spec.color\\n      name: Color\\n      type: string\\n    - jsonPath: .spec.size\\n      name: Size\\n      type: string\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes CustomResourceDefinition (CRD), which is a way to extend the Kubernetes API with custom resource types. The CRD specifies a new resource called \"Shirt\" under the group \"stable.example.com,\" with the plural name \"shirts\" and singular \"shirt.\" It is namespaced, meaning each Shirt resource exists within a specific namespace.\\n\\nThe CRD includes schema validation details, indicating that each Shirt resource has a \"spec\" object with properties \"color\" and \"size,\" both of which are strings. It also defines which fields can be used for selection (--selectableFields--) and specifies additional columns (\"Color\" and \"Size\") that will appear when listing these resources in the kubectl CLI.\\n\\nOverall, this CRD allows users to create, manage, and view custom \"Shirt\" resources with defined properties, integrating seamlessly into Kubernetes cluster management.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resource-definition.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1bf3b71a-7946-5e19-8bae-be9aa25483b8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"nginx\" with specific scheduling preferences. It includes a node affinity rule that prefers nodes with a label key \"disktype\" set to \"ssd,\" indicating a preference for deploying the pod on faster storage devices. The pod contains a single container running the Nginx image, with an image pull policy set to fetch the image only if it is not already present locally. This setup helps optimize pod placement for better performance by leveraging node labels and ensures efficient image management.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  affinity:\\n    nodeAffinity:\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 1\\n        preference:\\n          matchExpressions:\\n          - key: disktype\\n            operator: In\\n            values:\\n            - ssd          \\n  containers:\\n  - name: nginx\\n    image: nginx\\n    imagePullPolicy: IfNotPresent\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-preferred-affinity.yaml', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"nginx\" with specific scheduling preferences. It includes a node affinity rule that prefers nodes with a label key \"disktype\" set to \"ssd,\" indicating a preference for deploying the pod on faster storage devices. The pod contains a single container running the Nginx image, with an image pull policy set to fetch the image only if it is not already present locally. This setup helps optimize pod placement for better performance by leveraging node labels and ensures efficient image management.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1ca3c17e-f359-57f9-8d8b-a7f89c0b2862'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a PersistentVolumeClaim (PVC) in Kubernetes, which is a request for storage resources. The PVC is named \"pvc-limit-greater\" and specifies that it requires a storage capacity of 5 GiB. It has an access mode of \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node at a time. This configuration helps pods in Kubernetes manage and utilize persistent storage efficiently by requesting a specific amount of storage with defined access permissions.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: pvc-limit-greater\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 5Gi\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a PersistentVolumeClaim (PVC) in Kubernetes, which is a request for storage resources. The PVC is named \"pvc-limit-greater\" and specifies that it requires a storage capacity of 5 GiB. It has an access mode of \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node at a time. This configuration helps pods in Kubernetes manage and utilize persistent storage efficiently by requesting a specific amount of storage with defined access permissions.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\pvc-limit-greater.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1cc4f501-d58e-5f1f-b42f-348fbdaeda28'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Service configuration in YAML format, used to define how applications are exposed within a cluster or externally. The service named \"my-service\" is configured to use a LoadBalancer type, which allows external access, and it prefers dual-stack networking with support for IPv6. The selector ensures that the service targets pods labeled with \"app.kubernetes.io/name: MyApp\". The service listens on TCP port 80, which is typically used for HTTP traffic. Overall, this setup facilitates external connectivity to the application (MyApp) via a load balancer, with support for both IPv4 and IPv6 addressing.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\n  labels:\\n    app.kubernetes.io/name: MyApp\\nspec:\\n  ipFamilyPolicy: PreferDualStack\\n  ipFamilies:\\n  - IPv6\\n  type: LoadBalancer\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Service configuration in YAML format, used to define how applications are exposed within a cluster or externally. The service named \"my-service\" is configured to use a LoadBalancer type, which allows external access, and it prefers dual-stack networking with support for IPv6. The selector ensures that the service targets pods labeled with \"app.kubernetes.io/name: MyApp\". The service listens on TCP port 80, which is typically used for HTTP traffic. Overall, this setup facilitates external connectivity to the application (MyApp) via a load balancer, with support for both IPv4 and IPv6 addressing.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-prefer-ipv6-lb-svc.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1d89f19e-f4d3-566b-bbbd-931696e57549'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a NetworkPolicy named \"default-deny-ingress.\" The key purpose of this policy is to restrict incoming network traffic (Ingress) to pods within the cluster. The `podSelector: {}` indicates that the policy applies to all pods in the namespace, effectively enforcing a default-deny rule for any ingress traffic unless explicitly allowed by other policies. The `policyTypes` specifies that this policy targets Ingress traffic, helping to secure the cluster by blocking unauthorized access at the network level.\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: default-deny-ingress\\nspec:\\n  podSelector: {}\\n  policyTypes:\\n  - Ingress\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes configuration defines a NetworkPolicy named \"default-deny-ingress.\" The key purpose of this policy is to restrict incoming network traffic (Ingress) to pods within the cluster. The `podSelector: {}` indicates that the policy applies to all pods in the namespace, effectively enforcing a default-deny rule for any ingress traffic unless explicitly allowed by other policies. The `policyTypes` specifies that this policy targets Ingress traffic, helping to secure the cluster by blocking unauthorized access at the network level.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-ingress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1e3b9097-5188-5b2e-a905-6737081ec268'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This configuration defines a Horizontal Pod Autoscaler (HPA) in Kubernetes, which automatically adjusts the number of pod replicas based on resource utilization. Specifically, it targets a Deployment named \"php-apache\" and maintains the replica count between 1 and 10. The autoscaler monitors CPU utilization and aims to keep the average CPU usage at around 50%, dynamically scaling the number of pods up or down to meet this target. This setup helps ensure efficient resource use and maintain application performance under variable load.\\napiVersion: autoscaling/v2\\nkind: HorizontalPodAutoscaler\\nmetadata:\\n  name: php-apache\\nspec:\\n  scaleTargetRef:\\n    apiVersion: apps/v1\\n    kind: Deployment\\n    name: php-apache\\n  minReplicas: 1\\n  maxReplicas: 10\\n  metrics:\\n  - type: Resource\\n    resource:\\n      name: cpu\\n      target:\\n        type: Utilization\\n        averageUtilization: 50\\n', 'chunk': '1/1', 'summary': 'This configuration defines a Horizontal Pod Autoscaler (HPA) in Kubernetes, which automatically adjusts the number of pod replicas based on resource utilization. Specifically, it targets a Deployment named \"php-apache\" and maintains the replica count between 1 and 10. The autoscaler monitors CPU utilization and aims to keep the average CPU usage at around 50%, dynamically scaling the number of pods up or down to meet this target. This setup helps ensure efficient resource use and maintain application performance under variable load.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\hpa\\\\php-apache.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1f02b039-068c-5747-aefb-0f3233962911'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Deployment configuration defining a simple application stack. It creates a deployment named \"myapp\" with a single replica that runs an Alpine Linux container. The main container executes a shell command that continuously appends the word \"logging\" to a log file every second, simulating ongoing log generation. It mounts a shared volume at \"/opt\" to store logs persistently. \\n\\nAn init container called \"logshipper\" is configured to run before the main container; it also utilizes Alpine Linux and executes a command to continuously \"tail\" the log file, which allows it to process or monitor log entries as they are created. Both containers share an \"emptyDir\" volume named \"data,\" which provides temporary storage during the pod\\'s lifecycle and facilitates log sharing between the init and main containers.\\n\\nOverall, this setup demonstrates how to coordinate multiple containers within a pod using shared volumes, with the init container preparing or monitoring logs before the main application runs, highlighting concepts like volume sharing, init containers, and container orchestration in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: myapp\\n  labels:\\n    app: myapp\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: myapp\\n  template:\\n    metadata:\\n      labels:\\n        app: myapp\\n    spec:\\n      containers:\\n        - name: myapp\\n          image: alpine:latest\\n          command: [\\'sh\\', \\'-c\\', \\'while true; do echo \"logging\" >> /opt/logs.txt; sleep 1; done\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      initContainers:\\n        - name: logshipper\\n          image: alpine:latest\\n          restartPolicy: Always\\n          command: [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      volumes:\\n        - name: data\\n          emptyDir: {}', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Deployment configuration defining a simple application stack. It creates a deployment named \"myapp\" with a single replica that runs an Alpine Linux container. The main container executes a shell command that continuously appends the word \"logging\" to a log file every second, simulating ongoing log generation. It mounts a shared volume at \"/opt\" to store logs persistently. \\n\\nAn init container called \"logshipper\" is configured to run before the main container; it also utilizes Alpine Linux and executes a command to continuously \"tail\" the log file, which allows it to process or monitor log entries as they are created. Both containers share an \"emptyDir\" volume named \"data,\" which provides temporary storage during the pod\\'s lifecycle and facilitates log sharing between the init and main containers.\\n\\nOverall, this setup demonstrates how to coordinate multiple containers within a pod using shared volumes, with the init container preparing or monitoring logs before the main application runs, highlighting concepts like volume sharing, init containers, and container orchestration in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-sidecar.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('1f3367f6-8b9c-572d-8d97-c7f5da38f399'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes manifest defining a Service resource for a Redis leader node. The Service is configured to expose Redis on port 6379 and uses label selectors to identify the corresponding Redis leader pods. Specifically, it targets pods with labels `app=redis`, `role=leader`, and `tier=backend`, ensuring that network traffic directed to the Service is routed to the appropriate Redis leader instances. This setup allows for organized and scalable access to Redis leader nodes within a Kubernetes cluster, facilitating communication and load distribution for applications relying on Redis as a backend cache or database.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: redis-leader\\n  labels:\\n    app: redis\\n    role: leader\\n    tier: backend\\nspec:\\n  ports:\\n  - port: 6379\\n    targetPort: 6379\\n  selector:\\n    app: redis\\n    role: leader\\n    tier: backend', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-service.yaml', 'summary': 'This content is a Kubernetes manifest defining a Service resource for a Redis leader node. The Service is configured to expose Redis on port 6379 and uses label selectors to identify the corresponding Redis leader pods. Specifically, it targets pods with labels `app=redis`, `role=leader`, and `tier=backend`, ensuring that network traffic directed to the Service is routed to the appropriate Redis leader instances. This setup allows for organized and scalable access to Redis leader nodes within a Kubernetes cluster, facilitating communication and load distribution for applications relying on Redis as a backend cache or database.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('213128fd-ab9a-5aa6-b82e-a5c6f3457258'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"volume-test\" with a single container running the BusyBox image. The container executes a sleep command for one hour, effectively keeping it alive for demonstration or testing purposes. A key feature of this setup is the use of a projected volume called \"all-in-one,\" which consolidates multiple data sources into a single read-only filesystem mounted at \"/projected-volume\".\\n\\nThis projected volume combines three sources: a secret, a downward API, and a ConfigMap. The secret supplies a key-value pair (username), mapped to a specific path within the volume. The downward API provides dynamic information such as the pod\\'s labels and CPU limits, which are accessible via files in the volume. The ConfigMap supplies a configuration file, allowing external configuration data to be injected into the container. Overall, this setup showcases how to dynamically inject various types of data—secrets, metadata, resource limits, and configuration files—into a Kubernetes container through a versatile projected volume.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: volume-test\\nspec:\\n  containers:\\n  - name: container-test\\n    image: busybox:1.28\\n    command: [\"sleep\", \"3600\"]\\n    volumeMounts:\\n    - name: all-in-one\\n      mountPath: \"/projected-volume\"\\n      readOnly: true\\n  volumes:\\n  - name: all-in-one\\n    projected:\\n      sources:\\n      - secret:\\n          name: mysecret\\n          items:\\n            - key: username\\n              path: my-group/my-username\\n      - downwardAPI:\\n          items:\\n            - path: \"labels\"\\n              fieldRef:\\n                fieldPath: metadata.labels\\n            - path: \"cpu_limit\"\\n              resourceFieldRef:\\n                containerName: container-test\\n                resource: limits.cpu\\n      - configMap:\\n          name: myconfigmap\\n          items:\\n            - key: config\\n              path: my-group/my-config\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-secret-downwardapi-configmap.yaml', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"volume-test\" with a single container running the BusyBox image. The container executes a sleep command for one hour, effectively keeping it alive for demonstration or testing purposes. A key feature of this setup is the use of a projected volume called \"all-in-one,\" which consolidates multiple data sources into a single read-only filesystem mounted at \"/projected-volume\".\\n\\nThis projected volume combines three sources: a secret, a downward API, and a ConfigMap. The secret supplies a key-value pair (username), mapped to a specific path within the volume. The downward API provides dynamic information such as the pod\\'s labels and CPU limits, which are accessible via files in the volume. The ConfigMap supplies a configuration file, allowing external configuration data to be injected into the container. Overall, this setup showcases how to dynamically inject various types of data—secrets, metadata, resource limits, and configuration files—into a Kubernetes container through a versatile projected volume.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2277a0e8-fcf7-5fa2-b7e5-de24451ef2c3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML, defining how a pod should run in a cluster. The pod is named \"task-pv-pod\" and includes a single container running the Nginx web server. It specifies the use of a Persistent Volume Claim named \"task-pv-claim\", which is mounted into the container at the directory \"/usr/share/nginx/html\". This setup allows the container to serve dynamic or persistent content stored on the volume. The configuration effectively links a persistent storage resource to the Nginx container, enabling it to serve web pages or static content stored on the persistent volume.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: task-pv-pod\\nspec:\\n  volumes:\\n    - name: task-pv-storage\\n      persistentVolumeClaim:\\n        claimName: task-pv-claim\\n  containers:\\n    - name: task-pv-container\\n      image: nginx\\n      ports:\\n        - containerPort: 80\\n          name: \"http-server\"\\n      volumeMounts:\\n        - mountPath: \"/usr/share/nginx/html\"\\n          name: task-pv-storage\\n\\n\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML, defining how a pod should run in a cluster. The pod is named \"task-pv-pod\" and includes a single container running the Nginx web server. It specifies the use of a Persistent Volume Claim named \"task-pv-claim\", which is mounted into the container at the directory \"/usr/share/nginx/html\". This setup allows the container to serve dynamic or persistent content stored on the volume. The configuration effectively links a persistent storage resource to the Nginx container, enabling it to serve web pages or static content stored on the persistent volume.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('22a80f0d-e923-5a74-bffb-29b0f8dac4c6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes Pod configuration defines a pod named \"mypod\" with a label \"foo: bar\". It specifies a topology spread constraint to ensure the pod is evenly distributed across different zones based on the \"zone\" topology key, with a maximum skew of 1. If the spread cannot be satisfied, scheduling is prevented (\"DoNotSchedule\"). The pod contains a single container named \"pause\" that uses the official \"pause\" image from the Kubernetes registry, which is typically used as a placeholder or for network topology purposes within a cluster. Overall, this configuration emphasizes controlled pod placement across zones to maintain high availability or fault tolerance.\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: mypod\\n  labels:\\n    foo: bar\\nspec:\\n  topologySpreadConstraints:\\n  - maxSkew: 1\\n    topologyKey: zone\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.1', 'subchunk': '1/1', 'summary': 'This Kubernetes Pod configuration defines a pod named \"mypod\" with a label \"foo: bar\". It specifies a topology spread constraint to ensure the pod is evenly distributed across different zones based on the \"zone\" topology key, with a maximum skew of 1. If the spread cannot be satisfied, scheduling is prevented (\"DoNotSchedule\"). The pod contains a single container named \"pause\" that uses the official \"pause\" image from the Kubernetes registry, which is typically used as a placeholder or for network topology purposes within a cluster. Overall, this configuration emphasizes controlled pod placement across zones to maintain high availability or fault tolerance.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('22c38548-d39d-5015-8bae-9caf9b708b24'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"redis-follower\" that manages two replicas of a containerized Redis follower. The deployment specifies labels for organization and selector matching, ensuring that the correct pods are targeted. Each pod runs a container based on a specific Redis follower image, with resource requests set to limit CPU and memory usage, and exposes port 6379 for Redis communication. This configuration facilitates scalable, containerized deployment of Redis followers within a Kubernetes cluster, supporting application resilience and load balancing.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: redis-follower\\n  labels:\\n    app: redis\\n    role: follower\\n    tier: backend\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: redis\\n  template:\\n    metadata:\\n      labels:\\n        app: redis\\n        role: follower\\n        tier: backend\\n    spec:\\n      containers:\\n      - name: follower\\n        image: us-docker.pkg.dev/google-samples/containers/gke/gb-redis-follower:v2\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 6379', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"redis-follower\" that manages two replicas of a containerized Redis follower. The deployment specifies labels for organization and selector matching, ensuring that the correct pods are targeted. Each pod runs a container based on a specific Redis follower image, with resource requests set to limit CPU and memory usage, and exposes port 6379 for Redis communication. This configuration facilitates scalable, containerized deployment of Redis followers within a Kubernetes cluster, supporting application resilience and load balancing.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('25df2a38-febc-5d2f-8558-441f810a2618'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content consists of two Kubernetes Pod configurations written in YAML. Each Pod is labeled as \"frontend\" and contains a single container running the \"hello-app\" image from Google\\'s container registry. The first Pod, named \"pod1,\" uses version 2.0 of the image, while the second Pod, \"pod2,\" runs version 1.0. \\n\\nThese configurations demonstrate how to define simple Pods with specific images, illustrating how different versions of an application can be deployed side by side within a Kubernetes cluster. The YAML files specify the API version, resource kind, metadata, labels for organization and selection, and container specifications including names and container images.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod1\\n  labels:\\n    tier: frontend\\nspec:\\n  containers:\\n  - name: hello1\\n    image: gcr.io/google-samples/hello-app:2.0\\n\\n---\\n\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod2\\n  labels:\\n    tier: frontend\\nspec:\\n  containers:\\n  - name: hello2\\n    image: gcr.io/google-samples/hello-app:1.0\\n', 'subchunk': '1/1', 'summary': 'The provided content consists of two Kubernetes Pod configurations written in YAML. Each Pod is labeled as \"frontend\" and contains a single container running the \"hello-app\" image from Google\\'s container registry. The first Pod, named \"pod1,\" uses version 2.0 of the image, while the second Pod, \"pod2,\" runs version 1.0. \\n\\nThese configurations demonstrate how to define simple Pods with specific images, illustrating how different versions of an application can be deployed side by side within a Kubernetes cluster. The YAML files specify the API version, resource kind, metadata, labels for organization and selection, and container specifications including names and container images.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-rs.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('25eb6588-2e4a-5608-835b-820fac69ba29'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ClusterRole resource named \"csr-signer\" using YAML. It specifies the permissions associated with this role related to the certificates.k8s.io API group, which manages certificate signing requests (CSRs). The role allows users to get, list, and watch CSR resources, update the status of CSRs, and sign certificates using a specific signer name \"example.com/my-signer-name.\" The use of resourceNames restricts the signing permission to a particular signer, facilitating controlled access to certificate signing operations within a Kubernetes cluster.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: csr-signer\\nrules:\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - signers\\n  resourceNames:\\n  - example.com/my-signer-name # example.com/* can be used to authorize for all signers in the \\'example.com\\' domain\\n  verbs:\\n  - sign\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ClusterRole resource named \"csr-signer\" using YAML. It specifies the permissions associated with this role related to the certificates.k8s.io API group, which manages certificate signing requests (CSRs). The role allows users to get, list, and watch CSR resources, update the status of CSRs, and sign certificates using a specific signer name \"example.com/my-signer-name.\" The use of resourceNames restricts the signing permission to a particular signer, facilitating controlled access to certificate signing operations within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-sign.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('264f8249-057a-511a-8b0e-1facb908a8f9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a Kubernetes Pod configuration written in YAML, defining a single pod named \"busybox1\" with four containers. Each container uses the BusyBox image (version 1.28) and runs an infinite loop that echoes a message every 10 seconds, demonstrating simple, continuous shell commands within containers.\\n\\nThe configuration specifies resource requests and limits for some containers, for example, \"busybox-cnt01\" requests 100Mi of memory and 100m of CPU, with limits of 200Mi and 500m respectively. \"busybox-cnt02\" only has resource requests defined, while \"busybox-cnt03\" has limits set similarly to \"cnt01\". The other container, \"busybox-cnt04\", does not specify resource constraints. Overall, the configuration demonstrates how to run multiple containers within a single pod, control their resource usage, and execute simple shell commands remotely.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: busybox1\\nspec:\\n  containers:\\n  - name: busybox-cnt01\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt01; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt02\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt02; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n  - name: busybox-cnt03\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt03; sleep 10;done\"]\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt04\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt04; sleep 10;done\"]\\n', 'subchunk': '1/1', 'summary': 'This content provides a Kubernetes Pod configuration written in YAML, defining a single pod named \"busybox1\" with four containers. Each container uses the BusyBox image (version 1.28) and runs an infinite loop that echoes a message every 10 seconds, demonstrating simple, continuous shell commands within containers.\\n\\nThe configuration specifies resource requests and limits for some containers, for example, \"busybox-cnt01\" requests 100Mi of memory and 100m of CPU, with limits of 200Mi and 500m respectively. \"busybox-cnt02\" only has resource requests defined, while \"busybox-cnt03\" has limits set similarly to \"cnt01\". The other container, \"busybox-cnt04\", does not specify resource constraints. Overall, the configuration demonstrates how to run multiple containers within a single pod, control their resource usage, and execute simple shell commands remotely.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-1.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('27b5692c-40f3-57d3-9cad-9886e586da28'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"high-priority\" that runs an Ubuntu container with a simple looping command that outputs \"hello\" every 10 seconds. The container requests and limits are set to 10Gi of memory and 500 millicpus CPUs, ensuring resource allocation and constraints. Additionally, the pod is assigned to a priority class called \"high,\" which influences scheduling and preemption priorities within the Kubernetes cluster. This configuration is used to ensure that the pod has guaranteed resources and high scheduling priority in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: high-priority\\nspec:\\n  containers:\\n  - name: high-priority\\n    image: ubuntu\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"10Gi\"\\n        cpu: \"500m\"\\n      limits:\\n        memory: \"10Gi\"\\n        cpu: \"500m\"\\n  priorityClassName: high\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"high-priority\" that runs an Ubuntu container with a simple looping command that outputs \"hello\" every 10 seconds. The container requests and limits are set to 10Gi of memory and 500 millicpus CPUs, ensuring resource allocation and constraints. Additionally, the pod is assigned to a priority class called \"high,\" which influences scheduling and preemption priorities within the Kubernetes cluster. This configuration is used to ensure that the pod has guaranteed resources and high scheduling priority in the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\high-priority-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('27b6f821-b6b0-5267-b04f-9d7bf2859123'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages two replicas of an Nginx web server container. The deployment uses the selector label \"app: nginx\" to identify the Pods it manages. The Pod template specifies that each container runs the Nginx image version 1.16.1 (updating from a previous version 1.14.2), and exposes port 80 for web traffic. Overall, this configuration automates the deployment, scaling, and management of the Nginx web servers in a Kubernetes cluster, ensuring high availability and easy updates.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1 # Update the version of nginx from 1.14.2 to 1.16.1\\n        ports:\\n        - containerPort: 80\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages two replicas of an Nginx web server container. The deployment uses the selector label \"app: nginx\" to identify the Pods it manages. The Pod template specifies that each container runs the Nginx image version 1.16.1 (updating from a previous version 1.14.2), and exposes port 80 for web traffic. Overall, this configuration automates the deployment, scaling, and management of the Nginx web servers in a Kubernetes cluster, ensuring high availability and easy updates.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-update.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('28314b9e-5219-5f7e-84a6-8c0a3c2f178d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes StorageClass resource in YAML format, which specifies the parameters for dynamic volume provisioning within a Kubernetes cluster. The StorageClass is named \"standard\" and uses a custom provisioner identified as \"example.com/example\". It sets parameters such as \"type\" as \"pd-standard\" (which might refer to a persistent disk type, e.g., in cloud environments like Google Cloud). The \"volumeBindingMode\" is set to \"WaitForFirstConsumer\", meaning that PersistentVolume claims will wait until a pod that consumes the volume is scheduled, allowing the scheduler to consider topology constraints.\\n\\nAdditionally, the StorageClass includes \"allowedTopologies\", which restrict volume provisioning to specific zone labels, in this case, \"us-central-1a\" and \"us-central-1b\". This ensures that storage volumes are created in compatible zones, supporting data locality and high availability.\\n\\nThere is no executable code in this snippet; instead, it is a declarative configuration that guides how storage resources are provisioned in a Kubernetes environment.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: standard\\nprovisioner:  example.com/example\\nparameters:\\n  type: pd-standard\\nvolumeBindingMode: WaitForFirstConsumer\\nallowedTopologies:\\n- matchLabelExpressions:\\n  - key: topology.kubernetes.io/zone\\n    values:\\n    - us-central-1a\\n    - us-central-1b\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-topology.yaml', 'summary': 'This content defines a Kubernetes StorageClass resource in YAML format, which specifies the parameters for dynamic volume provisioning within a Kubernetes cluster. The StorageClass is named \"standard\" and uses a custom provisioner identified as \"example.com/example\". It sets parameters such as \"type\" as \"pd-standard\" (which might refer to a persistent disk type, e.g., in cloud environments like Google Cloud). The \"volumeBindingMode\" is set to \"WaitForFirstConsumer\", meaning that PersistentVolume claims will wait until a pod that consumes the volume is scheduled, allowing the scheduler to consider topology constraints.\\n\\nAdditionally, the StorageClass includes \"allowedTopologies\", which restrict volume provisioning to specific zone labels, in this case, \"us-central-1a\" and \"us-central-1b\". This ensures that storage volumes are created in compatible zones, supporting data locality and high availability.\\n\\nThere is no executable code in this snippet; instead, it is a declarative configuration that guides how storage resources are provisioned in a Kubernetes environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('29f7e4f3-4013-5b76-8ad4-a237a70aee93'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Job that runs multiple container instances with specific failure handling policies. The job is set to run 4 completions with 2 pods executing in parallel, each running a Bash container that prints a message, sleeps for 90 seconds, and then exits successfully. The restart policy for pods is set to \"Never,\" meaning that failed jobs won\\'t automatically restart. Additionally, the job includes a pod failure policy with a rule to \"Ignore\" failures caused by certain pod conditions, specifically those labeled as \"DisruptionTarget.\" This configuration is useful in scenarios where certain pod failures should not cause the entire job to be marked as failed, allowing for more flexible failure handling in complex workflows.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-pod-failure-policy-ignore\\nspec:\\n  completions: 4\\n  parallelism: 2\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      containers:\\n      - name: main\\n        image: docker.io/library/bash:5\\n        command: [\"bash\"]\\n        args:\\n        - -c\\n        - echo \"Hello world! I\\'m going to exit with 0 (success).\" && sleep 90 && exit 0\\n  backoffLimit: 0\\n  podFailurePolicy:\\n    rules:\\n    - action: Ignore\\n      onPodConditions:\\n      - type: DisruptionTarget\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Job that runs multiple container instances with specific failure handling policies. The job is set to run 4 completions with 2 pods executing in parallel, each running a Bash container that prints a message, sleeps for 90 seconds, and then exits successfully. The restart policy for pods is set to \"Never,\" meaning that failed jobs won\\'t automatically restart. Additionally, the job includes a pod failure policy with a rule to \"Ignore\" failures caused by certain pod conditions, specifically those labeled as \"DisruptionTarget.\" This configuration is useful in scenarios where certain pod failures should not cause the entire job to be marked as failed, allowing for more flexible failure handling in complex workflows.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-ignore.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2a19592d-eccd-5f9c-a27b-eeec79d07dbd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes YAML configuration file for defining a batch Job resource. It specifies a job named \"job-wq-2\" that runs with a parallelism of 2, meaning two pods will execute concurrently. The job runs containers based on the image stored in Google Container Registry (`gcr.io/myproject/job-wq-2`). The restart policy is set to \"OnFailure,\" which ensures that only failed pods are restarted, providing fault tolerance for the batch job. Overall, this configuration facilitates parallel execution of a containerized task in a Kubernetes environment, optimizing resource usage and job completion time.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-wq-2\\nspec:\\n  parallelism: 2\\n  template:\\n    metadata:\\n      name: job-wq-2\\n    spec:\\n      containers:\\n      - name: c\\n        image: gcr.io/myproject/job-wq-2\\n      restartPolicy: OnFailure\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes YAML configuration file for defining a batch Job resource. It specifies a job named \"job-wq-2\" that runs with a parallelism of 2, meaning two pods will execute concurrently. The job runs containers based on the image stored in Google Container Registry (`gcr.io/myproject/job-wq-2`). The restart policy is set to \"OnFailure,\" which ensures that only failed pods are restarted, providing fault tolerance for the batch job. Overall, this configuration facilitates parallel execution of a containerized task in a Kubernetes environment, optimizing resource usage and job completion time.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\job.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2b171c91-b142-5fd1-aa65-91a02450ff0a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# This content provides a Kubernetes service definition in YAML format for a Redis follower instance. It specifies the service's API version and type, with metadata including its name and labels that categorize its role within the application. The spec section details that the service listens on port 6379 and selects pods with specific labels (`app: redis`, `role: follower`, `tier: backend`) to route traffic to. This setup enables load balancing and discoverability for Redis follower pods within a Kubernetes cluster.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: redis-follower\\n  labels:\\n    app: redis\\n    role: follower\\n    tier: backend\\nspec:\\n  ports:\\n    # the port that this service should serve on\\n  - port: 6379\\n  selector:\\n    app: redis\\n    role: follower\\n    tier: backend\", 'subchunk': '1/1', 'summary': \"This content provides a Kubernetes service definition in YAML format for a Redis follower instance. It specifies the service's API version and type, with metadata including its name and labels that categorize its role within the application. The spec section details that the service listens on port 6379 and selects pods with specific labels (`app: redis`, `role: follower`, `tier: backend`) to route traffic to. This setup enables load balancing and discoverability for Redis follower pods within a Kubernetes cluster.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2b5b9947-94a1-5a63-a6cf-04f7f4989dac'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ValidatingAdmissionPolicy resource, which is used to enforce custom policies during resource creation or modification. The policy is named \"demo-policy.example.com\" and is configured with a failure policy set to \"Fail,\" meaning invalid requests will be rejected.\\n\\nThe policy applies specifically to create and update operations on \"deployments\" within the \"apps\" API group and version \"v1.\" It contains a validation rule that checks whether the number of replicas specified in the deployment exceeds 50. If a deployment\\'s replica count is greater than 50, a message is generated indicating the number of replicas set. Additionally, an audit annotation logs cases where the replica count is high, capturing the same information.\\n\\nIn essence, this policy ensures that no deployment can be created or updated with more than 50 replicas, maintaining control over resource scaling by validating input and providing detailed feedback and audit logs.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"demo-policy.example.com\"\\nspec:\\n  failurePolicy: Fail\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  validations:\\n    - expression: \"object.spec.replicas > 50\"\\n      messageExpression: \"\\'Deployment spec.replicas set to \\' + string(object.spec.replicas)\"\\n  auditAnnotations:\\n    - key: \"high-replica-count\"\\n      valueExpression: \"\\'Deployment spec.replicas set to \\' + string(object.spec.replicas)\"\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ValidatingAdmissionPolicy resource, which is used to enforce custom policies during resource creation or modification. The policy is named \"demo-policy.example.com\" and is configured with a failure policy set to \"Fail,\" meaning invalid requests will be rejected.\\n\\nThe policy applies specifically to create and update operations on \"deployments\" within the \"apps\" API group and version \"v1.\" It contains a validation rule that checks whether the number of replicas specified in the deployment exceeds 50. If a deployment\\'s replica count is greater than 50, a message is generated indicating the number of replicas set. Additionally, an audit annotation logs cases where the replica count is high, capturing the same information.\\n\\nIn essence, this policy ensures that no deployment can be created or updated with more than 50 replicas, maintaining control over resource scaling by validating input and providing detailed feedback and audit logs.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-audit-annotation.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2c4a91dd-5eb0-5a4f-96bc-5ceda61dda87'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Ingress resource configuration for managing external access to services inside a cluster. The Ingress is named \"tls-example-ingress\" and is set up to handle HTTPS traffic for the host \"https-example.foo.com\". It specifies a TLS secret called \"testsecret-tls\" to enable secure communication. The rule routes all requests to the root path (\"/\") of this host to a backend service named \"service1\" listening on port 80. Overall, this configuration facilitates secure, managed access to internal services via an Ingress controller, with TLS termination handled by the specified secret.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: tls-example-ingress\\nspec:\\n  tls:\\n  - hosts:\\n      - https-example.foo.com\\n    secretName: testsecret-tls\\n  rules:\\n  - host: https-example.foo.com\\n    http:\\n      paths:\\n      - path: /\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: service1\\n            port:\\n              number: 80\\n', 'chunk': '1/1', 'summary': 'This content describes a Kubernetes Ingress resource configuration for managing external access to services inside a cluster. The Ingress is named \"tls-example-ingress\" and is set up to handle HTTPS traffic for the host \"https-example.foo.com\". It specifies a TLS secret called \"testsecret-tls\" to enable secure communication. The rule routes all requests to the root path (\"/\") of this host to a backend service named \"service1\" listening on port 80. Overall, this configuration facilitates secure, managed access to internal services via an Ingress controller, with TLS termination handled by the specified secret.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\tls-example-ingress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2cad7049-8bda-51dc-aa95-f8eb74d18982'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration written in YAML, which manages the deployment of a frontend application. The deployment is named \"frontend\" and specifies a selector to match pods with specific labels, ensuring proper management and updates. It is configured to run a single replica pod.\\n\\nThe pod template within the deployment defines a container running the NGINX web server using the Docker image \"gcr.io/google-samples/hello-frontend:1.0\". It also includes a lifecycle hook, specifically the `preStop` hook, which executes the command `\"/usr/sbin/nginx -s quit\"` when the container is about to stop. This command gracefully instructs NGINX to shut down, allowing for proper termination and resource cleanup. Overall, this YAML configuration automates deploying a frontend service with a graceful shutdown process.\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  selector:\\n    matchLabels:\\n      app: hello\\n      tier: frontend\\n      track: stable\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: hello\\n        tier: frontend\\n        track: stable\\n    spec:\\n      containers:\\n        - name: nginx\\n          image: \"gcr.io/google-samples/hello-frontend:1.0\"\\n          lifecycle:\\n            preStop:\\n              exec:\\n                command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\\n...', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-deployment.yaml', 'summary': 'This content is a Kubernetes deployment configuration written in YAML, which manages the deployment of a frontend application. The deployment is named \"frontend\" and specifies a selector to match pods with specific labels, ensuring proper management and updates. It is configured to run a single replica pod.\\n\\nThe pod template within the deployment defines a container running the NGINX web server using the Docker image \"gcr.io/google-samples/hello-frontend:1.0\". It also includes a lifecycle hook, specifically the `preStop` hook, which executes the command `\"/usr/sbin/nginx -s quit\"` when the container is about to stop. This command gracefully instructs NGINX to shut down, allowing for proper termination and resource cleanup. Overall, this YAML configuration automates deploying a frontend service with a graceful shutdown process.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2d373eb7-823e-5ff2-84e3-cb961e7d4d9c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Service definition in YAML format that configures network access to a set of pods running the nginx application. The service is named \"nginx-service\" and is configured to listen on port 8000, which is the external port clients will connect to. The `targetPort` specifies that traffic received on port 8000 will be forwarded to port 80 on the nginx pods. The `selector` matches pods labeled with `app: nginx`, ensuring that the service load balances incoming traffic across all such pods.\\n\\nIn terms of functionality, this configuration creates a TCP load balancer that exposes nginx pods on port 8000, rerouting traffic to their internal port 80. This allows external clients to access the nginx application through a stable, dedicated service endpoint. The YAML is a fundamental part of managing network traffic in Kubernetes, enabling decoupling of the exposed port from the internal container port and facilitating scalable, reliable access to containerized applications.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx-service\\nspec:\\n  ports:\\n  - port: 8000 # the port that this service should serve on\\n    # the container on each pod to connect to, can be a name\\n    # (e.g. \\'www\\') or a number (e.g. 80)\\n    targetPort: 80\\n    protocol: TCP\\n  # just like the selector in the deployment,\\n  # but this time it identifies the set of pods to load balance\\n  # traffic to.\\n  selector:\\n    app: nginx\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\nginx-service.yaml', 'summary': 'This content is a Kubernetes Service definition in YAML format that configures network access to a set of pods running the nginx application. The service is named \"nginx-service\" and is configured to listen on port 8000, which is the external port clients will connect to. The `targetPort` specifies that traffic received on port 8000 will be forwarded to port 80 on the nginx pods. The `selector` matches pods labeled with `app: nginx`, ensuring that the service load balances incoming traffic across all such pods.\\n\\nIn terms of functionality, this configuration creates a TCP load balancer that exposes nginx pods on port 8000, rerouting traffic to their internal port 80. This allows external clients to access the nginx application through a stable, dedicated service endpoint. The YAML is a fundamental part of managing network traffic in Kubernetes, enabling decoupling of the exposed port from the internal container port and facilitating scalable, reliable access to containerized applications.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2e6a6c43-1e5b-554a-a8f1-7e24605ff429'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content offers a practical guide for deploying the cloud-controller-manager as a DaemonSet within a Kubernetes cluster, primarily for environments where master nodes can run pods, and with specific role labels. It emphasizes that this setup is a template and requires customization depending on the cloud provider and cluster configuration.\\n\\nThe code begins by creating a ServiceAccount for the cloud-controller-manager and then binds it to the cluster-admin role using a ClusterRoleBinding, ensuring it has the necessary permissions to manage cloud resources. The main part is the DaemonSet configuration, which ensures that the cloud-controller-manager runs on each node designated as a master, using node selectors and tolerations for control plane nodes. The container within the DaemonSet runs the cloud-controller-manager image and is configured with command-line flags, including placeholders for the cloud provider (to be customized by the user), leader election, and network configuration parameters like the cluster CIDR.\\n\\nThis setup enables the cloud-controller-manager to manage cloud-specific tasks, such as routing and load balancing, directly from the control plane nodes. The tolerations and node selectors embedded in the DaemonSet specification ensure that the controller runs on appropriate nodes while respecting node taints, making it adaptable for different cluster architectures.\\n# This is an example of how to set up cloud-controller-manager as a Daemonset in your cluster.\\n# It assumes that your masters can run pods and has the role node-role.kubernetes.io/master\\n# Note that this Daemonset will not work straight out of the box for your cloud, this is\\n# meant to be a guideline.\\n\\n---\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: cloud-controller-manager\\n  namespace: kube-system\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: system:cloud-controller-manager\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cluster-admin\\nsubjects:\\n- kind: ServiceAccount\\n  name: cloud-controller-manager\\n  namespace: kube-system\\n---\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  labels:\\n    k8s-app: cloud-controller-manager\\n  name: cloud-controller-manager\\n  namespace: kube-system\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cloud-controller-manager\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: cloud-controller-manager\\n    spec:\\n      serviceAccountName: cloud-controller-manager\\n      containers:\\n      - name: cloud-controller-manager\\n        # for in-tree providers we use registry.k8s.io/cloud-controller-manager\\n        # this can be replaced with any other image for out-of-tree providers\\n        image: registry.k8s.io/cloud-controller-manager:v1.8.0\\n        command:\\n        - /usr/local/bin/cloud-controller-manager\\n        - --cloud-provider=[YOUR_CLOUD_PROVIDER]  # Add your own cloud provider here!\\n        - --leader-elect=true\\n        - --use-service-account-credentials\\n        # these flags will vary for every cloud provider\\n        - --allocate-node-cidrs=true\\n        - --configure-cloud-routes=true\\n        - --cluster-cidr=172.17.0.0/16\\n      tolerations:\\n      # this is required so CCM can bootstrap itself\\n      - key: node.cloudprovider.kubernetes.io/uninitialized\\n        value: \"true\"\\n        effect: NoSchedule\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator: Exists\\n        effect: NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      # this is to restrict CCM to only run on master nodes\\n      # the node selector may vary depending on your cluster setup\\n      nodeSelector:\\n        node-role.kubernetes.io/master: \"\"\\n', 'chunk': '1/1', 'summary': 'The provided content offers a practical guide for deploying the cloud-controller-manager as a DaemonSet within a Kubernetes cluster, primarily for environments where master nodes can run pods, and with specific role labels. It emphasizes that this setup is a template and requires customization depending on the cloud provider and cluster configuration.\\n\\nThe code begins by creating a ServiceAccount for the cloud-controller-manager and then binds it to the cluster-admin role using a ClusterRoleBinding, ensuring it has the necessary permissions to manage cloud resources. The main part is the DaemonSet configuration, which ensures that the cloud-controller-manager runs on each node designated as a master, using node selectors and tolerations for control plane nodes. The container within the DaemonSet runs the cloud-controller-manager image and is configured with command-line flags, including placeholders for the cloud provider (to be customized by the user), leader election, and network configuration parameters like the cluster CIDR.\\n\\nThis setup enables the cloud-controller-manager to manage cloud-specific tasks, such as routing and load balancing, directly from the control plane nodes. The tolerations and node selectors embedded in the DaemonSet specification ensure that the controller runs on appropriate nodes while respecting node taints, making it adaptable for different cluster architectures.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\cloud\\\\ccm-example.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('2f0df79c-b97d-5739-887d-01cd2747c2a3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a Kubernetes DaemonSet configuration, which is used for deploying a specific pod on each node in a cluster. In this case, the DaemonSet is for the Node Problem Detector, a tool designed to monitor and report node health issues. The configuration specifies metadata such as the name, namespace, labels, and selector criteria to identify the DaemonSet. \\n\\nThe pod template configured in the DaemonSet runs with host networking, which allows it to access network interfaces directly on the node. It contains a single container based on the `registry.k8s.io/node-problem-detector:v0.1` image, with privileged access, enabling it to perform low-level system diagnostics. Resource limits and requests are defined for CPU and memory to ensure proper resource management. The container mounts two volumes: one for log files stored on the host at `/var/log/`, and another for configuration options via a ConfigMap named `node-problem-detector-config`, mounted at `/config`. This setup ensures the Node Problem Detector can access relevant logs and configuration data on each node, facilitating effective monitoring and health reporting.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: node-problem-detector-v0.1\\n  namespace: kube-system\\n  labels:\\n    k8s-app: node-problem-detector\\n    version: v0.1\\n    kubernetes.io/cluster-service: \"true\"\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: node-problem-detector  \\n      version: v0.1\\n      kubernetes.io/cluster-service: \"true\"\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: node-problem-detector\\n        version: v0.1\\n        kubernetes.io/cluster-service: \"true\"\\n    spec:\\n      hostNetwork: true\\n      containers:\\n      - name: node-problem-detector\\n        image: registry.k8s.io/node-problem-detector:v0.1\\n        securityContext:\\n          privileged: true\\n        resources:\\n          limits:\\n            cpu: \"200m\"\\n            memory: \"100Mi\"\\n          requests:\\n            cpu: \"20m\"\\n            memory: \"20Mi\"\\n        volumeMounts:\\n        - name: log\\n          mountPath: /log\\n          readOnly: true\\n        - name: config # Overwrite the config/ directory with ConfigMap volume\\n          mountPath: /config\\n          readOnly: true\\n      volumes:\\n      - name: log\\n        hostPath:\\n          path: /var/log/\\n      - name: config # Define ConfigMap volume\\n        configMap:\\n          name: node-problem-detector-config', 'subchunk': '1/1', 'summary': 'This content provides a Kubernetes DaemonSet configuration, which is used for deploying a specific pod on each node in a cluster. In this case, the DaemonSet is for the Node Problem Detector, a tool designed to monitor and report node health issues. The configuration specifies metadata such as the name, namespace, labels, and selector criteria to identify the DaemonSet. \\n\\nThe pod template configured in the DaemonSet runs with host networking, which allows it to access network interfaces directly on the node. It contains a single container based on the `registry.k8s.io/node-problem-detector:v0.1` image, with privileged access, enabling it to perform low-level system diagnostics. Resource limits and requests are defined for CPU and memory to ensure proper resource management. The container mounts two volumes: one for log files stored on the host at `/var/log/`, and another for configuration options via a ConfigMap named `node-problem-detector-config`, mounted at `/config`. This setup ensures the Node Problem Detector can access relevant logs and configuration data on each node, facilitating effective monitoring and health reporting.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\node-problem-detector-configmap.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3028a890-eaec-55be-8706-369f79dae077'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Ingress resource configuration that manages HTTP routing based on hostnames. It defines multiple rules to direct traffic to different backend services depending on the request\\'s host header. Specifically, traffic to \"first.bar.com\" is routed to \"service1,\" traffic to \"second.bar.com\" is routed to \"service2,\" and any other requests matching the general pattern are directed to \"service3.\" The configuration uses path-based routing with the root path (\"/\") and specifies port 80 for all services. \\n\\nThis setup allows different domain names to map to distinct services within a Kubernetes cluster, enabling shared ingress but specialized routing, which is essential for hosting multiple applications under the same ingress controller. The structure also reflects typical use of the Kubernetes `networking.k8s.io/v1` API version for defining ingress rules.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: name-virtual-host-ingress-no-third-host\\nspec:\\n  rules:\\n  - host: first.bar.com\\n    http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/\"\\n        backend:\\n          service:\\n            name: service1\\n            port:\\n              number: 80\\n  - host: second.bar.com\\n    http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/\"\\n        backend:\\n          service:\\n            name: service2\\n            port:\\n              number: 80\\n  - http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/\"\\n        backend:\\n          service:\\n            name: service3\\n            port:\\n              number: 80\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Ingress resource configuration that manages HTTP routing based on hostnames. It defines multiple rules to direct traffic to different backend services depending on the request\\'s host header. Specifically, traffic to \"first.bar.com\" is routed to \"service1,\" traffic to \"second.bar.com\" is routed to \"service2,\" and any other requests matching the general pattern are directed to \"service3.\" The configuration uses path-based routing with the root path (\"/\") and specifies port 80 for all services. \\n\\nThis setup allows different domain names to map to distinct services within a Kubernetes cluster, enabling shared ingress but specialized routing, which is essential for hosting multiple applications under the same ingress controller. The structure also reflects typical use of the Kubernetes `networking.k8s.io/v1` API version for defining ingress rules.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\name-virtual-host-ingress-no-third-host.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('30d4213c-f1fc-5b6b-857e-7a938eef987f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML, designed to demonstrate how environment variables can be dynamically populated using field references. The Pod runs a single container based on the BusyBox image, executing a shell command in an infinite loop that outputs several environment variables every 10 seconds. These variables include the node name, pod name, namespace, pod IP, and service account name, which are populated using the `fieldRef` mechanism referencing specific fields within the pod\\'s specification and status. This approach allows the container to access real-time metadata about its runtime environment directly through environment variables, avoiding the need for manual updates or external configuration.\\n\\nThe YAML defines a detailed setup where each environment variable (`MY_NODE_NAME`, `MY_POD_NAME`, etc.) uses `valueFrom.fieldRef.fieldPath` to fetch specific data from the pod\\'s specification or status. This method enables dynamic injection of runtime information into the container\\'s environment, facilitating tasks like logging, monitoring, or configuration. The container\\'s command repeatedly prints these environment variables every 10 seconds, demonstrating how this metadata can be accessed and used within the containerized environment. The Pod\\'s restart policy is set to \"Never,\" indicating it will not automatically restart after completion or failure.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-envars-fieldref\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"sh\", \"-c\"]\\n      args:\\n      - while true; do\\n          echo -en \\'\\\\n\\';\\n          printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;\\n          printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;\\n          sleep 10;\\n        done;\\n      env:\\n        - name: MY_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: spec.nodeName\\n        - name: MY_POD_NAME\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: metadata.name\\n        - name: MY_POD_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: metadata.namespace\\n        - name: MY_POD_IP\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: status.podIP\\n        - name: MY_POD_SERVICE_ACCOUNT\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: spec.serviceAccountName\\n  restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML, designed to demonstrate how environment variables can be dynamically populated using field references. The Pod runs a single container based on the BusyBox image, executing a shell command in an infinite loop that outputs several environment variables every 10 seconds. These variables include the node name, pod name, namespace, pod IP, and service account name, which are populated using the `fieldRef` mechanism referencing specific fields within the pod\\'s specification and status. This approach allows the container to access real-time metadata about its runtime environment directly through environment variables, avoiding the need for manual updates or external configuration.\\n\\nThe YAML defines a detailed setup where each environment variable (`MY_NODE_NAME`, `MY_POD_NAME`, etc.) uses `valueFrom.fieldRef.fieldPath` to fetch specific data from the pod\\'s specification or status. This method enables dynamic injection of runtime information into the container\\'s environment, facilitating tasks like logging, monitoring, or configuration. The container\\'s command repeatedly prints these environment variables every 10 seconds, demonstrating how this metadata can be accessed and used within the containerized environment. The Pod\\'s restart policy is set to \"Never,\" indicating it will not automatically restart after completion or failure.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-envars-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('30e06ef1-2871-5101-8d43-ebbda0f12023'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration YAML that defines a pod named \"dapi-test-pod\" with a single container. The container uses the BusyBox image and executes an echo command that prints two environment variables, \"SPECIAL_LEVEL_KEY\" and \"SPECIAL_TYPE_KEY.\" These environment variables are dynamically set from a ConfigMap named \"special-config,\" specifically retrieving the \"SPECIAL_LEVEL\" and \"SPECIAL_TYPE\" keys from it. The restart policy is set to \"Never,\" indicating the pod will not restart automatically after completion. Overall, this configuration demonstrates how to inject configuration data into a container via environment variables sourced from a ConfigMap and execute a command upon startup.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"/bin/echo\", \"$(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ]\\n      env:\\n        - name: SPECIAL_LEVEL_KEY\\n          valueFrom:\\n            configMapKeyRef:\\n              name: special-config\\n              key: SPECIAL_LEVEL\\n        - name: SPECIAL_TYPE_KEY\\n          valueFrom:\\n            configMapKeyRef:\\n              name: special-config\\n              key: SPECIAL_TYPE\\n  restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration YAML that defines a pod named \"dapi-test-pod\" with a single container. The container uses the BusyBox image and executes an echo command that prints two environment variables, \"SPECIAL_LEVEL_KEY\" and \"SPECIAL_TYPE_KEY.\" These environment variables are dynamically set from a ConfigMap named \"special-config,\" specifically retrieving the \"SPECIAL_LEVEL\" and \"SPECIAL_TYPE\" keys from it. The restart policy is set to \"Never,\" indicating the pod will not restart automatically after completion. Overall, this configuration demonstrates how to inject configuration data into a container via environment variables sourced from a ConfigMap and execute a command upon startup.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-env-var-valueFrom.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('314a91ac-bffc-59af-904c-c5110c01c468'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Job configuration in YAML format designed to execute multiple parallel tasks. The job is named \"indexed-job\" and is set to run a total of 5 completions, with a maximum of 3 pods working simultaneously, utilizing an indexed completion mode. This mode allows each pod to process a unique index, which is used to distinguish individual tasks within the job.\\n\\nThe job\\'s pod template runs a container using the \"busybox\" image, executing the command \"rev /input/data.txt\" to reverse the contents of a specific text file. The file \"data.txt\" is made available inside the container through a mounted volume, which uses the downward API to inject the pod\\'s completion index into the file\\'s path, allowing each pod to process a unique data segment based on its assigned index. This setup is useful for parallel batch processing where each task operates on distinct data segments, with the overall process managed efficiently by Kubernetes.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: \\'indexed-job\\'\\nspec:\\n  completions: 5\\n  parallelism: 3\\n  completionMode: Indexed\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      containers:\\n      - name: \\'worker\\'\\n        image: \\'docker.io/library/busybox\\'\\n        command:\\n        - \"rev\"\\n        - \"/input/data.txt\"\\n        volumeMounts:\\n        - mountPath: /input\\n          name: input\\n      volumes:\\n      - name: input\\n        downwardAPI:\\n          items:\\n          - path: \"data.txt\"\\n            fieldRef:\\n              fieldPath: metadata.annotations[\\'batch.kubernetes.io/job-completion-index\\']', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes Job configuration in YAML format designed to execute multiple parallel tasks. The job is named \"indexed-job\" and is set to run a total of 5 completions, with a maximum of 3 pods working simultaneously, utilizing an indexed completion mode. This mode allows each pod to process a unique index, which is used to distinguish individual tasks within the job.\\n\\nThe job\\'s pod template runs a container using the \"busybox\" image, executing the command \"rev /input/data.txt\" to reverse the contents of a specific text file. The file \"data.txt\" is made available inside the container through a mounted volume, which uses the downward API to inject the pod\\'s completion index into the file\\'s path, allowing each pod to process a unique data segment based on its assigned index. This setup is useful for parallel batch processing where each task operates on distinct data segments, with the overall process managed efficiently by Kubernetes.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job-vol.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('31effe17-31bc-55c7-8e2f-589c87eeead1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration in YAML format, defining a pod named \"secret-envars-test-pod\" with a single container running the Nginx image. The key feature of this configuration is the use of environment variables that are populated from a Kubernetes secret named \"test-secret.\" Specifically, it sets two environment variables, \"SECRET_USERNAME\" and \"SECRET_PASSWORD,\" by referencing the secret’s \"username\" and \"password\" keys, respectively. This approach enhances security by allowing sensitive data like credentials to be stored securely in secrets and injected into containers at runtime, avoiding exposure of plain-text secrets in the pod specification.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: secret-envars-test-pod\\nspec:\\n  containers:\\n  - name: envars-test-container\\n    image: nginx\\n    env:\\n    - name: SECRET_USERNAME\\n      valueFrom:\\n        secretKeyRef:\\n          name: test-secret\\n          key: username\\n    - name: SECRET_PASSWORD\\n      valueFrom:\\n        secretKeyRef:\\n          name: test-secret\\n          key: password\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret-envars-pod.yaml', 'summary': 'This content is a Kubernetes Pod configuration in YAML format, defining a pod named \"secret-envars-test-pod\" with a single container running the Nginx image. The key feature of this configuration is the use of environment variables that are populated from a Kubernetes secret named \"test-secret.\" Specifically, it sets two environment variables, \"SECRET_USERNAME\" and \"SECRET_PASSWORD,\" by referencing the secret’s \"username\" and \"password\" keys, respectively. This approach enhances security by allowing sensitive data like credentials to be stored securely in secrets and injected into containers at runtime, avoiding exposure of plain-text secrets in the pod specification.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('327d8e76-fe28-5f70-9e9e-5c7af523cafb'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes admission policy designed to enforce image repository naming conventions tailored to namespace environment labels. It employs a ValidatingAdmissionPolicy to validate deployment creation and updates, ensuring that container images align with the namespace\\'s environment label (e.g., staging, prod). The policy permits exceptions for deployments labeled with \"exempt=true\" or for containers not belonging to the \"example.com\" domain, such as sidecars or shared components. \\n\\nThe policy defines variables that extract environment labels from namespace metadata, check for exemption labels, and retrieve container specs. It filters containers that include \"example.com\" in their image, then validates that these images correspond to the namespace\\'s environment (for instance, images should start with \"staging.\" if the environment is staging). If the validation fails, it blocks the operation and provides a message indicating the allowed environment images for the namespace. Overall, this policy helps enforce environment-specific image tagging for secure and consistent deployment practices.\\n# This policy enforces that all containers of a deployment has the image repo match the environment label of its namespace.\\n# Except for \"exempt\" deployments, or any containers that do not belong to the \"example.com\" organization (e.g. common sidecars).\\n# For example, if the namespace has a label of {\"environment\": \"staging\"}, all container images must be either staging.example.com/*\\n# or do not contain \"example.com\" at all, unless the deployment has {\"exempt\": \"true\"} label.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"image-matches-namespace-environment.policy.example.com\"\\nspec:\\n  failurePolicy: Fail\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  variables:\\n  - name: environment\\n    expression: \"\\'environment\\' in namespaceObject.metadata.labels ? namespaceObject.metadata.labels[\\'environment\\'] : \\'prod\\'\"\\n  - name: exempt\\n    expression: \"\\'exempt\\' in object.metadata.labels && object.metadata.labels[\\'exempt\\'] == \\'true\\'\"\\n  - name: containers\\n    expression: \"object.spec.template.spec.containers\"\\n  - name: containersToCheck\\n    expression: \"variables.containers.filter(c, c.image.contains(\\'example.com/\\'))\"\\n  validations:\\n  - expression: \"variables.exempt || variables.containersToCheck.all(c, c.image.startsWith(variables.environment + \\'.\\'))\"\\n    messageExpression: \"\\'only \\' + variables.environment + \\' images are allowed in namespace \\' + namespaceObject.metadata.name\"', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\image-matches-namespace-environment.policy.yaml', 'summary': 'This content describes a Kubernetes admission policy designed to enforce image repository naming conventions tailored to namespace environment labels. It employs a ValidatingAdmissionPolicy to validate deployment creation and updates, ensuring that container images align with the namespace\\'s environment label (e.g., staging, prod). The policy permits exceptions for deployments labeled with \"exempt=true\" or for containers not belonging to the \"example.com\" domain, such as sidecars or shared components. \\n\\nThe policy defines variables that extract environment labels from namespace metadata, check for exemption labels, and retrieve container specs. It filters containers that include \"example.com\" in their image, then validates that these images correspond to the namespace\\'s environment (for instance, images should start with \"staging.\" if the environment is staging). If the validation fails, it blocks the operation and provides a message indicating the allowed environment images for the namespace. Overall, this policy helps enforce environment-specific image tagging for secure and consistent deployment practices.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('327e157e-d133-5836-afa1-884dfb3f8471'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"dapi-test-pod\" with a single container that uses the BusyBox image. The container executes a command to output the contents of a file located at \"/etc/config/keys\" upon startup. The pod configuration mounts a ConfigMap named \"special-config\" as a volume, specifically linking the key \"SPECIAL_LEVEL\" from the ConfigMap to a file named \"keys\" in the container\\'s \"/etc/config\" directory. This setup enables the container to access configuration data dynamically provided by the ConfigMap. The restart policy is set to \"Never,\" indicating that the pod will not automatically restart after completion or failure.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"/bin/sh\",\"-c\",\"cat /etc/config/keys\" ]\\n      volumeMounts:\\n      - name: config-volume\\n        mountPath: /etc/config\\n  volumes:\\n    - name: config-volume\\n      configMap:\\n        name: special-config\\n        items:\\n        - key: SPECIAL_LEVEL\\n          path: keys\\n  restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"dapi-test-pod\" with a single container that uses the BusyBox image. The container executes a command to output the contents of a file located at \"/etc/config/keys\" upon startup. The pod configuration mounts a ConfigMap named \"special-config\" as a volume, specifically linking the key \"SPECIAL_LEVEL\" from the ConfigMap to a file named \"keys\" in the container\\'s \"/etc/config\" directory. This setup enables the container to access configuration data dynamically provided by the ConfigMap. The restart policy is set to \"Never,\" indicating that the pod will not automatically restart after completion or failure.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-volume-specific-key.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('335458d3-6400-5353-a746-08c372ed6cdd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes NetworkPolicy configuration defines a policy named \"allow-all-egress\" that applies to all pods in the namespace (indicated by the empty podSelector). The policy specifies that all outbound network traffic (egress) is allowed from these pods, without restrictions, by using an empty egress block. The policyType set to Egress explicitly states that this rule governs outgoing traffic. Overall, this configuration permits all pods to send traffic externally without any network restrictions.\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: allow-all-egress\\nspec:\\n  podSelector: {}\\n  egress:\\n  - {}\\n  policyTypes:\\n  - Egress\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes NetworkPolicy configuration defines a policy named \"allow-all-egress\" that applies to all pods in the namespace (indicated by the empty podSelector). The policy specifies that all outbound network traffic (egress) is allowed from these pods, without restrictions, by using an empty egress block. The policyType set to Egress explicitly states that this rule governs outgoing traffic. Overall, this configuration permits all pods to send traffic externally without any network restrictions.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-allow-all-egress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('335d0a11-b676-5b39-8c7b-3fe5d872ec5c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# The provided content defines a Kubernetes Secret resource of type `bootstrap.kubernetes.io/token`, which is used for bootstrapping nodes in a Kubernetes cluster. This secret includes encoded data fields such as `auth-extra-groups`, `expiration`, `token-id`, `token-secret`, and usage flags for `bootstrap-authentication` and `bootstrap-signing`. These fields are base64-encoded, representing token credentials and associated metadata used during the initial node joining process.\\n\\nIn detail, the secret contains a token ID and secret that nodes can use to authenticate with the cluster during bootstrap procedures. The `auth-extra-groups` field specifies additional group memberships for the token bearer, while the `expiration` marks the token's validity period. The usage flags indicate that this token is intended solely for bootstrap authentication and signing processes, ensuring secure and controlled node addition to the Kubernetes control plane. This mechanism facilitates automated, secure onboarding of new nodes without manual intervention.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: bootstrap-token-5emitj\\n  namespace: kube-system\\ntype: bootstrap.kubernetes.io/token\\ndata:\\n  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=\\n  expiration: MjAyMC0wOS0xM1QwNDozOToxMFo=\\n  token-id: NWVtaXRq\\n  token-secret: a3E0Z2lodnN6emduMXAwcg==\\n  usage-bootstrap-authentication: dHJ1ZQ==\\n  usage-bootstrap-signing: dHJ1ZQ==\", 'subchunk': '1/1', 'summary': \"The provided content defines a Kubernetes Secret resource of type `bootstrap.kubernetes.io/token`, which is used for bootstrapping nodes in a Kubernetes cluster. This secret includes encoded data fields such as `auth-extra-groups`, `expiration`, `token-id`, `token-secret`, and usage flags for `bootstrap-authentication` and `bootstrap-signing`. These fields are base64-encoded, representing token credentials and associated metadata used during the initial node joining process.\\n\\nIn detail, the secret contains a token ID and secret that nodes can use to authenticate with the cluster during bootstrap procedures. The `auth-extra-groups` field specifies additional group memberships for the token bearer, while the `expiration` marks the token's validity period. The usage flags indicate that this token is intended solely for bootstrap authentication and signing processes, ensuring secure and controlled node addition to the Kubernetes control plane. This mechanism facilitates automated, secure onboarding of new nodes without manual intervention.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\bootstrap-token-secret-base64.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3470bbd3-027d-5ee0-b21d-961f48f9cbd0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code is a Kubernetes Pod configuration in YAML format. It defines a single Pod named `example-conflict-with-limitrange-cpu` that includes one container called `demo`. The container uses the `pause:3.8` image from the Kubernetes registry. The configuration specifies a resource request for CPU, asking for 700 millicores (or 0.7 CPU). This setup enables the Kubernetes scheduler to allocate appropriate resources for the container, though it does not specify a CPU limit, which could lead to conflicts if there is a LimitRange set in the namespace with conflicting constraints. The configuration is straightforward, illustrating resource requests in a Pod specification.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: example-conflict-with-limitrange-cpu\\nspec:\\n  containers:\\n  - name: demo\\n    image: registry.k8s.io/pause:3.8\\n    resources:\\n      requests:\\n        cpu: 700m\\n', 'chunk': '1/1', 'summary': 'The provided code is a Kubernetes Pod configuration in YAML format. It defines a single Pod named `example-conflict-with-limitrange-cpu` that includes one container called `demo`. The container uses the `pause:3.8` image from the Kubernetes registry. The configuration specifies a resource request for CPU, asking for 700 millicores (or 0.7 CPU). This setup enables the Kubernetes scheduler to allocate appropriate resources for the container, though it does not specify a CPU limit, which could lead to conflicts if there is a LimitRange set in the namespace with conflicting constraints. The configuration is straightforward, illustrating resource requests in a Pod specification.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-conflict-with-limitrange-cpu.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('34729781-7e29-5954-9b50-21c9b1a3c5c6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration expressed in YAML. It defines a deployment named \"snowflake\" with two replicas, ensuring that two pods running the specified container will be maintained at all times. The deployment uses a container image from the Kubernetes registry, specifically \"registry.k8s.io/serve_hostname,\" and sets the image pull policy to \"Always,\" ensuring the latest image version is retrieved during each deployment or pod update. This configuration provides a scalable and resilient setup for running containerized applications within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app: snowflake\\n  name: snowflake\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: snowflake\\n  template:\\n    metadata:\\n      labels:\\n        app: snowflake\\n    spec:\\n      containers:\\n      - image: registry.k8s.io/serve_hostname\\n        imagePullPolicy: Always\\n        name: snowflake\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration expressed in YAML. It defines a deployment named \"snowflake\" with two replicas, ensuring that two pods running the specified container will be maintained at all times. The deployment uses a container image from the Kubernetes registry, specifically \"registry.k8s.io/serve_hostname,\" and sets the image pull policy to \"Always,\" ensuring the latest image version is retrieved during each deployment or pod update. This configuration provides a scalable and resilient setup for running containerized applications within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\snowflake-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('37246ff8-4986-5886-a1ad-44de9d101bc3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes NetworkPolicy named \"allow-all-ingress\" is a configuration that permits all incoming network traffic to the pods within the cluster. It uses an empty podSelector, which applies the policy to all pods, and an empty ingress rule, meaning there are no restrictions on incoming connections. The policyType is set to \"Ingress,\" indicating that this policy controls inbound traffic. Overall, this configuration effectively allows unrestricted ingress traffic to all pods in the namespace.\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: allow-all-ingress\\nspec:\\n  podSelector: {}\\n  ingress:\\n  - {}\\n  policyTypes:\\n  - Ingress\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes NetworkPolicy named \"allow-all-ingress\" is a configuration that permits all incoming network traffic to the pods within the cluster. It uses an empty podSelector, which applies the policy to all pods, and an empty ingress rule, meaning there are no restrictions on incoming connections. The policyType is set to \"Ingress,\" indicating that this policy controls inbound traffic. Overall, this configuration effectively allows unrestricted ingress traffic to all pods in the namespace.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-allow-all-ingress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('39600e2e-a64a-5709-868a-db5b1c8bb210'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Ingress resource, which manages external access to services within a Kubernetes cluster. The Ingress is named \"simple-fanout-example\" and is configured with a set of rules for routing incoming HTTP requests based on the hostname and URL path. Specifically, requests directed to \"foo.bar.com\" with the path \"/foo\" are routed to \"service1\" on port 4200, while requests with the path \"/bar\" are routed to \"service2\" on port 8080. The configuration uses the \"Prefix\" path type, meaning that any URL starting with the specified paths will match and be forwarded accordingly.\\n\\nThe code sets up a basic traffic distribution pattern (fanout), enabling a single hostname to route different URL prefixes to different backend services. This approach simplifies managing multiple services under a common domain, facilitating scalable and organized access within the cluster.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: simple-fanout-example\\nspec:\\n  rules:\\n  - host: foo.bar.com\\n    http:\\n      paths:\\n      - path: /foo\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: service1\\n            port:\\n              number: 4200\\n      - path: /bar\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: service2\\n            port:\\n              number: 8080\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\simple-fanout-example.yaml', 'summary': 'This content defines a Kubernetes Ingress resource, which manages external access to services within a Kubernetes cluster. The Ingress is named \"simple-fanout-example\" and is configured with a set of rules for routing incoming HTTP requests based on the hostname and URL path. Specifically, requests directed to \"foo.bar.com\" with the path \"/foo\" are routed to \"service1\" on port 4200, while requests with the path \"/bar\" are routed to \"service2\" on port 8080. The configuration uses the \"Prefix\" path type, meaning that any URL starting with the specified paths will match and be forwarded accordingly.\\n\\nThe code sets up a basic traffic distribution pattern (fanout), enabling a single hostname to route different URL prefixes to different backend services. This approach simplifies managing multiple services under a common domain, facilitating scalable and organized access within the cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3b102468-f326-5c67-b27f-3ced10024b5a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes YAML manifest that defines a Pod with specific resource requests and limits. The Pod, named \"pod-resources-demo,\" is located in the \"pod-resources-example\" namespace. It requests and limits a total of 1 CPU and 100Mi of memory at the Pod level. Inside, it contains two containers: one running Nginx with a request and limit of 0.5 CPU and 50Mi of memory, and another running Fedora with a command to sleep indefinitely. This setup demonstrates resource management, illustrating how CPU and memory allocations can be specified at both the Pod and container levels to ensure proper resource allocation and isolation within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-resources-demo\\n  namespace: pod-resources-example\\nspec:\\n  resources:\\n    limits:\\n      cpu: \"1\"\\n      memory: \"200Mi\"\\n    requests:\\n      cpu: \"1\"\\n      memory: \"100Mi\"\\n  containers:\\n  - name: pod-resources-demo-ctr-1\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"0.5\"\\n        memory: \"100Mi\"\\n      requests:\\n        cpu: \"0.5\"\\n        memory: \"50Mi\"\\n  - name: pod-resources-demo-ctr-2\\n    image: fedora\\n    command:\\n    - sleep\\n    - inf \\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes YAML manifest that defines a Pod with specific resource requests and limits. The Pod, named \"pod-resources-demo,\" is located in the \"pod-resources-example\" namespace. It requests and limits a total of 1 CPU and 100Mi of memory at the Pod level. Inside, it contains two containers: one running Nginx with a request and limit of 0.5 CPU and 50Mi of memory, and another running Fedora with a command to sleep indefinitely. This setup demonstrates resource management, illustrating how CPU and memory allocations can be specified at both the Pod and container levels to ensure proper resource allocation and isolation within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-resources.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3b44e0fd-f62c-5d8d-aa82-4e31821a33d3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format. It specifies a pod named \"constraints-cpu-demo\" with a single container running the \"nginx\" image. The resource specifications for the container include CPU limits and requests: a maximum of 800 millicores (800m) and a minimum of 500 millicores (500m). These constraints ensure proper CPU allocation, helping manage resource usage and maintain cluster stability. The configuration illustrates how to set CPU resource requests and limits to control container performance and resource sharing within a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"800m\"\\n      requests:\\n        cpu: \"500m\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod.yaml', 'summary': 'This content defines a Kubernetes Pod configuration in YAML format. It specifies a pod named \"constraints-cpu-demo\" with a single container running the \"nginx\" image. The resource specifications for the container include CPU limits and requests: a maximum of 800 millicores (800m) and a minimum of 500 millicores (500m). These constraints ensure proper CPU allocation, helping manage resource usage and maintain cluster stability. The configuration illustrates how to set CPU resource requests and limits to control container performance and resource sharing within a Kubernetes environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3b55f6c6-05b0-58f7-9b76-903b2ee79687'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML, which defines a pod named \"counter\" containing a single container called \"count.\" The container uses the BusyBox image and runs a shell script that continuously increments a counter variable every second.\\n\\nThe script inside the container performs an infinite loop where it appends the current count and timestamp to a log file at /var/log/1.log, and also logs an informational message with the timestamp and current count to another file at /var/log/2.log. These log files are stored in a shared volume mounted at /var/log, which is configured as an emptyDir volume, meaning it persists only for the lifetime of the pod.\\n\\nOverall, the code demonstrates how to run a basic containerized log generator within Kubernetes, utilizing volume mounts for persistent-like storage during the pod lifecycle.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: counter\\nspec:\\n  containers:\\n  - name: count\\n    image: busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  volumes:\\n  - name: varlog\\n    emptyDir: {}\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML, which defines a pod named \"counter\" containing a single container called \"count.\" The container uses the BusyBox image and runs a shell script that continuously increments a counter variable every second.\\n\\nThe script inside the container performs an infinite loop where it appends the current count and timestamp to a log file at /var/log/1.log, and also logs an informational message with the timestamp and current count to another file at /var/log/2.log. These log files are stored in a shared volume mounted at /var/log, which is configured as an emptyDir volume, meaning it persists only for the lifetime of the pod.\\n\\nOverall, the code demonstrates how to run a basic containerized log generator within Kubernetes, utilizing volume mounts for persistent-like storage during the pod lifecycle.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3c529241-0b90-53b2-98bc-2993b63b0861'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes LimitRange resource named \"cpu-limit-range.\" This resource sets default CPU limits and requests for containers within a namespace. Specifically, it designates that each container will have a default CPU limit of 1 core and a default CPU request of 0.5 cores if not explicitly specified. This ensures efficient resource management and prevents containers from consuming excessive CPU resources, thereby maintaining the overall stability and predictability of the cluster.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: cpu-limit-range\\nspec:\\n  limits:\\n  - default:\\n      cpu: 1\\n    defaultRequest:\\n      cpu: 0.5\\n    type: Container\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes LimitRange resource named \"cpu-limit-range.\" This resource sets default CPU limits and requests for containers within a namespace. Specifically, it designates that each container will have a default CPU limit of 1 core and a default CPU request of 0.5 cores if not explicitly specified. This ensures efficient resource management and prevents containers from consuming excessive CPU resources, thereby maintaining the overall stability and predictability of the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3d3ac63a-8911-5673-8f0a-e315c9b687cd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"cpu-demo\" in the \"cpu-example\" namespace. The Pod contains a single container built from the \"vish/stress\" image, which is typically used to generate stress or load on system resources. The resource allocation specifies a CPU request of 0.5 cores and a limit of 1 core, meaning the container is guaranteed half a CPU but cannot exceed one. The container is configured to run with arguments \"-cpus\" and \"2,\" instructing it to utilize two CPUs for stress testing despite the resource limits, demonstrating how resource requests, limits, and container arguments work together to manage CPU workload in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: cpu-demo\\n  namespace: cpu-example\\nspec:\\n  containers:\\n  - name: cpu-demo-ctr\\n    image: vish/stress\\n    resources:\\n      limits:\\n        cpu: \"1\"\\n      requests:\\n        cpu: \"0.5\"\\n    args:\\n    - -cpus\\n    - \"2\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit.yaml', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"cpu-demo\" in the \"cpu-example\" namespace. The Pod contains a single container built from the \"vish/stress\" image, which is typically used to generate stress or load on system resources. The resource allocation specifies a CPU request of 0.5 cores and a limit of 1 core, meaning the container is guaranteed half a CPU but cannot exceed one. The container is configured to run with arguments \"-cpus\" and \"2,\" instructing it to utilize two CPUs for stress testing despite the resource limits, demonstrating how resource requests, limits, and container arguments work together to manage CPU workload in Kubernetes.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3e4344e0-744a-5355-8d0a-c2b466afaed6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration in YAML format, which defines a deployment named \"redis-leader\" running a Redis server container. This deployment ensures that one replica of the Redis leader pod is always running, with specifications for resource requests (CPU and memory) to guarantee availability. The container uses the Redis 6.0.5 image from Docker Hub and exposes port 6379 for Redis communication. In essence, the code automates the deployment of a Redis server instance within a Kubernetes cluster, providing a reliable backend component for applications that need in-memory data storage or caching.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: redis-leader\\n  labels:\\n    app: redis\\n    role: leader\\n    tier: backend\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: redis\\n  template:\\n    metadata:\\n      labels:\\n        app: redis\\n        role: leader\\n        tier: backend\\n    spec:\\n      containers:\\n      - name: leader\\n        image: \"docker.io/redis:6.0.5\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 6379', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes deployment configuration in YAML format, which defines a deployment named \"redis-leader\" running a Redis server container. This deployment ensures that one replica of the Redis leader pod is always running, with specifications for resource requests (CPU and memory) to guarantee availability. The container uses the Redis 6.0.5 image from Docker Hub and exposes port 6379 for Redis communication. In essence, the code automates the deployment of a Redis server instance within a Kubernetes cluster, providing a reliable backend component for applications that need in-memory data storage or caching.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3e84c54b-277e-5e60-a4ca-b74d308805b8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML. It defines a Pod named \"userns\" with the specification that it does not allow host user namespaces (`hostUsers: false`) for security isolation. The Pod contains a single container named \"shell\" which runs a Debian image. The container\\'s command is set to keep the container running indefinitely with `sleep infinity`, making it useful for debugging or testing purposes.\\n\\nThe overall purpose of this configuration is to create an isolated environment where a Debian-based container runs continuously, which can be used for administrative tasks, debugging, or further configuration within a controlled namespace. The use of `hostUsers: false` ensures the container does not share the host\\'s user namespace, providing additional security and isolation.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: userns\\nspec:\\n  hostUsers: false\\n  containers:\\n  - name: shell\\n    command: [\"sleep\", \"infinity\"]\\n    image: debian\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML. It defines a Pod named \"userns\" with the specification that it does not allow host user namespaces (`hostUsers: false`) for security isolation. The Pod contains a single container named \"shell\" which runs a Debian image. The container\\'s command is set to keep the container running indefinitely with `sleep infinity`, making it useful for debugging or testing purposes.\\n\\nThe overall purpose of this configuration is to create an isolated environment where a Debian-based container runs continuously, which can be used for administrative tasks, debugging, or further configuration within a controlled namespace. The use of `hostUsers: false` ensures the container does not share the host\\'s user namespace, providing additional security and isolation.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\user-namespaces-stateless.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3f610a9f-4dff-5061-8b31-bb4dc23205ca'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration written in YAML, defining a Redis server deployment. It specifies a single container running Redis version 5.0.4, with the startup command to run `redis-server` using a specific configuration file located at `/redis-master/redis.conf`. Environment variables are set, notably `MASTER` with a value of \"true,\" which could be used for configuration or clustering purposes. The container is configured with resource limits, restricting CPU usage to 0.1 cores, and it exposes port 6379 for Redis communication. The Pod also includes volume mounts: one for data persistence (`/redis-master-data`), using an empty directory, and another for configuration (`/redis-master`) which sources its settings from a ConfigMap named `example-redis-config`, specifically pulling the `redis.conf` configuration file from the key `redis-config`. This setup ensures the Redis server runs with the specified configuration and has isolated data storage, suitable for development or testing environments.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: redis\\nspec:\\n  containers:\\n  - name: redis\\n    image: redis:5.0.4\\n    command:\\n      - redis-server\\n      - \"/redis-master/redis.conf\"\\n    env:\\n    - name: MASTER\\n      value: \"true\"\\n    ports:\\n    - containerPort: 6379\\n    resources:\\n      limits:\\n        cpu: \"0.1\"\\n    volumeMounts:\\n    - mountPath: /redis-master-data\\n      name: data\\n    - mountPath: /redis-master\\n      name: config\\n  volumes:\\n    - name: data\\n      emptyDir: {}\\n    - name: config\\n      configMap:\\n        name: example-redis-config\\n        items:\\n        - key: redis-config\\n          path: redis.conf\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration written in YAML, defining a Redis server deployment. It specifies a single container running Redis version 5.0.4, with the startup command to run `redis-server` using a specific configuration file located at `/redis-master/redis.conf`. Environment variables are set, notably `MASTER` with a value of \"true,\" which could be used for configuration or clustering purposes. The container is configured with resource limits, restricting CPU usage to 0.1 cores, and it exposes port 6379 for Redis communication. The Pod also includes volume mounts: one for data persistence (`/redis-master-data`), using an empty directory, and another for configuration (`/redis-master`) which sources its settings from a ConfigMap named `example-redis-config`, specifically pulling the `redis.conf` configuration file from the key `redis-config`. This setup ensures the Redis server runs with the specified configuration and has isolated data storage, suitable for development or testing environments.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\redis-pod.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3f620258-61e7-5ee1-9075-a1f80d0611ab'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML that sets up an etcd container with specific settings. The configuration defines a pod named \"etcd-with-grpc\" which runs a single container using the etcd image version 3.5.1-0 from the Kubernetes registry. The container executes the etcd server with command-line options specifying the data directory, listening address, and advertised client URL, along with a debug log level for detailed logging. It exposes port 2379 for client communication and includes a liveness probe utilizing gRPC on the same port to monitor the container\\'s health, with an initial delay of 10 seconds before the probe starts. This configuration ensures that the etcd instance is correctly set up for client interactions and is actively monitored for availability within a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: etcd-with-grpc\\nspec:\\n  containers:\\n  - name: etcd\\n    image: registry.k8s.io/etcd:3.5.1-0\\n    command: [ \"/usr/local/bin/etcd\", \"--data-dir\",  \"/var/lib/etcd\", \"--listen-client-urls\", \"http://0.0.0.0:2379\", \"--advertise-client-urls\", \"http://127.0.0.1:2379\", \"--log-level\", \"debug\"]\\n    ports:\\n    - containerPort: 2379\\n    livenessProbe:\\n      grpc:\\n        port: 2379\\n      initialDelaySeconds: 10\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML that sets up an etcd container with specific settings. The configuration defines a pod named \"etcd-with-grpc\" which runs a single container using the etcd image version 3.5.1-0 from the Kubernetes registry. The container executes the etcd server with command-line options specifying the data directory, listening address, and advertised client URL, along with a debug log level for detailed logging. It exposes port 2379 for client communication and includes a liveness probe utilizing gRPC on the same port to monitor the container\\'s health, with an initial delay of 10 seconds before the probe starts. This configuration ensures that the etcd instance is correctly set up for client interactions and is actively monitored for availability within a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\grpc-liveness.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3f91e19f-83ac-54dc-b73f-2183845faa9e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Secret of the type `kubernetes.io/service-account-token` with the name `secret-sa-sample`. It is annotated to associate it with a specific service account named `sa-name`. The `data` field contains a base64-encoded token, which in this case decodes to a sample string (e.g., \"bar\"). This secret is typically used for securely providing a token to authenticate a service account within the Kubernetes cluster, enabling the account to access API resources programmatically.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret-sa-sample\\n  annotations:\\n    kubernetes.io/service-account.name: \"sa-name\"\\ntype: kubernetes.io/service-account-token\\ndata:\\n  extra: YmFyCg==', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\serviceaccount-token-secret.yaml', 'summary': 'This YAML configuration defines a Kubernetes Secret of the type `kubernetes.io/service-account-token` with the name `secret-sa-sample`. It is annotated to associate it with a specific service account named `sa-name`. The `data` field contains a base64-encoded token, which in this case decodes to a sample string (e.g., \"bar\"). This secret is typically used for securely providing a token to authenticate a service account within the Kubernetes cluster, enabling the account to access API resources programmatically.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('3fced1db-38a1-5ac3-bc1f-fb8167b6aec0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Deployment named \"curl-deployment\" with a single replica pod. The pod uses a container based on the \"radial/busyboxplus:curl\" image, which includes basic Unix utilities along with curl for network testing. The container runs an infinite loop to keep it alive, allowing ongoing interactions or debugging.\\n\\nThe key feature of this deployment is the use of an attached secret-based volume (\"secret-volume\"). This volume mounts a Kubernetes secret called \"nginxsecret\" to the directory \"/etc/nginx/ssl\" inside the container. This setup enables secure access to sensitive data, such as SSL certificates or keys, within the container environment, facilitating secure communication or configuration for services like nginx. Overall, the YAML provides a minimal setup for deploying a container that can access secret data securely for tasks such as SSL/TLS operations or other secret-dependent processes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: curl-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: curlpod\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: curlpod\\n    spec:\\n      volumes:\\n      - name: secret-volume\\n        secret:\\n          secretName: nginxsecret\\n      containers:\\n      - name: curlpod\\n        command:\\n        - sh\\n        - -c\\n        - while true; do sleep 1; done\\n        image: radial/busyboxplus:curl\\n        volumeMounts:\\n        - mountPath: /etc/nginx/ssl\\n          name: secret-volume\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Deployment named \"curl-deployment\" with a single replica pod. The pod uses a container based on the \"radial/busyboxplus:curl\" image, which includes basic Unix utilities along with curl for network testing. The container runs an infinite loop to keep it alive, allowing ongoing interactions or debugging.\\n\\nThe key feature of this deployment is the use of an attached secret-based volume (\"secret-volume\"). This volume mounts a Kubernetes secret called \"nginxsecret\" to the directory \"/etc/nginx/ssl\" inside the container. This setup enables secure access to sensitive data, such as SSL certificates or keys, within the container environment, facilitating secure communication or configuration for services like nginx. Overall, the YAML provides a minimal setup for deploying a container that can access secret data securely for tasks such as SSL/TLS operations or other secret-dependent processes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\curlpod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('400d8edd-8c34-52f4-9415-f024991a6686'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The content is a Kubernetes pod configuration written in YAML. It defines a pod named \"cpu-demo-2\" within the \"cpu-example\" namespace, containing a single container. This container uses the \"vish/stress\" image, which is typically used for CPU and memory stress testing. The resource specifications set both limits and requests for CPU usage to 100 units, and the container is configured with the arguments \"-cpus 2,\" indicating it will utilize 2 CPU cores during execution. This setup is likely intended for testing CPU performance or resource allocation in a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: cpu-demo-2\\n  namespace: cpu-example\\nspec:\\n  containers:\\n  - name: cpu-demo-ctr-2\\n    image: vish/stress\\n    resources:\\n      limits:\\n        cpu: \"100\"\\n      requests:\\n        cpu: \"100\"\\n    args:\\n    - -cpus\\n    - \"2\"\\n', 'chunk': '1/1', 'summary': 'The content is a Kubernetes pod configuration written in YAML. It defines a pod named \"cpu-demo-2\" within the \"cpu-example\" namespace, containing a single container. This container uses the \"vish/stress\" image, which is typically used for CPU and memory stress testing. The resource specifications set both limits and requests for CPU usage to 100 units, and the container is configured with the arguments \"-cpus 2,\" indicating it will utilize 2 CPU cores during execution. This setup is likely intended for testing CPU performance or resource allocation in a Kubernetes cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('404217a0-7176-59e4-94f5-80fedfdd60ab'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes resource of type `IngressClass`, which specifies an ingress controller named `external-lb`. The `controller` field indicates the specific ingress controller implementation that will handle ingress resources associated with this class, identified here as `example.com/ingress-controller`. Additionally, it references a set of parameters stored in a custom resource, with details such as the API group `k8s.example.com`, kind `IngressParameters`, and the resource named `external-lb` in the `external-configuration` namespace. These parameters likely contain specific configuration details for the ingress controller to manage external load balancing or traffic routing accordingly.\\napiVersion: networking.k8s.io/v1\\nkind: IngressClass\\nmetadata:\\n  name: external-lb\\nspec:\\n  controller: example.com/ingress-controller\\n  parameters:\\n    apiGroup: k8s.example.com\\n    kind: IngressParameters\\n    name: external-lb\\n    namespace: external-configuration\\n    scope: Namespace\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\namespaced-params.yaml', 'summary': 'This content defines a Kubernetes resource of type `IngressClass`, which specifies an ingress controller named `external-lb`. The `controller` field indicates the specific ingress controller implementation that will handle ingress resources associated with this class, identified here as `example.com/ingress-controller`. Additionally, it references a set of parameters stored in a custom resource, with details such as the API group `k8s.example.com`, kind `IngressParameters`, and the resource named `external-lb` in the `external-configuration` namespace. These parameters likely contain specific configuration details for the ingress controller to manage external load balancing or traffic routing accordingly.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('409ffd4e-80a6-5a03-a413-5726f82e5e86'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Service resource for a Cassandra application. It specifies that the service will not have a cluster-internal IP (clusterIP: None), meaning it will be a headless service, typically used for stateful applications requiring direct pod-to-pod communication, such as Cassandra nodes. The service exposes port 9042, which is the default port used by Cassandra clients to connect to the database. The selector ensures that the service targets pods labeled with \"app: cassandra,\" enabling dynamic discovery and communication among Cassandra nodes in the cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  labels:\\n    app: cassandra\\n  name: cassandra\\nspec:\\n  clusterIP: None\\n  ports:\\n  - port: 9042\\n  selector:\\n    app: cassandra\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes configuration defines a Service resource for a Cassandra application. It specifies that the service will not have a cluster-internal IP (clusterIP: None), meaning it will be a headless service, typically used for stateful applications requiring direct pod-to-pod communication, such as Cassandra nodes. The service exposes port 9042, which is the default port used by Cassandra clients to connect to the database. The selector ensures that the service targets pods labeled with \"app: cassandra,\" enabling dynamic discovery and communication among Cassandra nodes in the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('414c3adb-d481-5c14-aa02-866bd8247c4c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration written in YAML, illustrating how to specify security contexts for container security. It defines a Pod named `security-context-demo` with a global `securityContext` setting that sets the user ID to 1000, group ID to 3000, and includes an additional supplementary group 4000. These settings determine the permissions and privileges of processes running inside the containers. The container within the Pod uses the `agnhost` image from the Kubernetes registry, running a simple command to sleep for an hour. Additionally, the container has a specific `securityContext` that disallows privilege escalation, meaning it cannot gain higher privileges than initially assigned. Overall, this configuration demonstrates how to enhance security by setting user and group IDs, supplementary groups, and restrictions on privilege escalation within containers.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo\\nspec:\\n  securityContext:\\n    runAsUser: 1000\\n    runAsGroup: 3000\\n    supplementalGroups: [4000]\\n  containers:\\n  - name: sec-ctx-demo\\n    image: registry.k8s.io/e2e-test-images/agnhost:2.45\\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\\n    securityContext:\\n      allowPrivilegeEscalation: false\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration written in YAML, illustrating how to specify security contexts for container security. It defines a Pod named `security-context-demo` with a global `securityContext` setting that sets the user ID to 1000, group ID to 3000, and includes an additional supplementary group 4000. These settings determine the permissions and privileges of processes running inside the containers. The container within the Pod uses the `agnhost` image from the Kubernetes registry, running a simple command to sleep for an hour. Additionally, the container has a specific `securityContext` that disallows privilege escalation, meaning it cannot gain higher privileges than initially assigned. Overall, this configuration demonstrates how to enhance security by setting user and group IDs, supplementary groups, and restrictions on privilege escalation within containers.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-5.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('418eb0f4-2a58-5fb9-bade-09735d682ebb'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"my-nginx\" that manages two replicas of an Nginx web server container. The configuration specifies that each container runs the Nginx image and exposes port 80. The deployment ensures that the specified number of pods (containers) are running and automatically handles scaling, updates, and self-healing of the application. This setup is commonly used to deploy and manage stateless web servers in a Kubernetes cluster efficiently.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      run: my-nginx\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        run: my-nginx\\n    spec:\\n      containers:\\n      - name: my-nginx\\n        image: nginx\\n        ports:\\n        - containerPort: 80\\n\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"my-nginx\" that manages two replicas of an Nginx web server container. The configuration specifies that each container runs the Nginx image and exposes port 80. The deployment ensures that the specified number of pods (containers) are running and automatically handles scaling, updates, and self-healing of the application. This setup is commonly used to deploy and manage stateless web servers in a Kubernetes cluster efficiently.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\run-my-nginx.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('41ec6dee-4be8-5316-989a-ea46c941d51a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ClusterRoleBinding in YAML format, which grants permissions at the cluster level. Specifically, it binds the \"secret-reader\" ClusterRole to the group named \"manager,\" allowing all members of this group to read secrets across all namespaces in the Kubernetes cluster. This configuration is useful for managing access control centrally, ensuring that users in the \"manager\" group have the necessary read permissions for secrets without granting them broader administrative rights.\\napiVersion: rbac.authorization.k8s.io/v1\\n# This cluster role binding allows anyone in the \"manager\" group to read secrets in any namespace.\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: read-secrets-global\\nsubjects:\\n- kind: Group\\n  name: manager # Name is case sensitive\\n  apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  kind: ClusterRole\\n  name: secret-reader\\n  apiGroup: rbac.authorization.k8s.io\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-clusterrolebinding.yaml', 'summary': 'This content defines a Kubernetes ClusterRoleBinding in YAML format, which grants permissions at the cluster level. Specifically, it binds the \"secret-reader\" ClusterRole to the group named \"manager,\" allowing all members of this group to read secrets across all namespaces in the Kubernetes cluster. This configuration is useful for managing access control centrally, ensuring that users in the \"manager\" group have the necessary read permissions for secrets without granting them broader administrative rights.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4256b030-480e-593d-9f39-a321c2116f2b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"constraints-mem-demo-3\" that runs an Nginx container. The resource section specifies memory limits and requests for the container: it requests 100Mi of memory, meaning it asks the cluster to allocate at least that much, and it limits the container to use no more than 800Mi of memory. This setup helps in managing resource allocation and ensuring the container does not exceed its designated memory usage.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-mem-demo-3\\nspec:\\n  containers:\\n  - name: constraints-mem-demo-3-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"800Mi\"\\n      requests:\\n        memory: \"100Mi\"\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"constraints-mem-demo-3\" that runs an Nginx container. The resource section specifies memory limits and requests for the container: it requests 100Mi of memory, meaning it asks the cluster to allocate at least that much, and it limits the container to use no more than 800Mi of memory. This setup helps in managing resource allocation and ensuring the container does not exceed its designated memory usage.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('429dde78-f236-57b3-a451-2ae2ccce0b73'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration in YAML format, which defines how to deploy an application in a Kubernetes cluster. It specifies a Deployment resource with the API version `apps/v1` and the kind `Deployment`. The deployment is labeled for identification, and it is named `hello-world`. The configuration requests five replicas, meaning it will run five instances of the containerized application to ensure high availability and load balancing. The selector matches the label `app.kubernetes.io/name: load-balancer-example` to identify the pods managed by this deployment. The pod template within the deployment specifies metadata labels and describes the containers to run, which in this case includes a single container running the image `gcr.io/google-samples/hello-app:2.0`. The container exposes port 8080, making the application accessible when combined with a service or load balancer. Overall, this deployment configuration automates the process of deploying multiple instances of a sample hello-world application in a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app.kubernetes.io/name: load-balancer-example\\n  name: hello-world\\nspec:\\n  replicas: 5\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: load-balancer-example\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: load-balancer-example\\n    spec:\\n      containers:\\n      - image: gcr.io/google-samples/hello-app:2.0\\n        name: hello-world\\n        ports:\\n        - containerPort: 8080\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes deployment configuration in YAML format, which defines how to deploy an application in a Kubernetes cluster. It specifies a Deployment resource with the API version `apps/v1` and the kind `Deployment`. The deployment is labeled for identification, and it is named `hello-world`. The configuration requests five replicas, meaning it will run five instances of the containerized application to ensure high availability and load balancing. The selector matches the label `app.kubernetes.io/name: load-balancer-example` to identify the pods managed by this deployment. The pod template within the deployment specifies metadata labels and describes the containers to run, which in this case includes a single container running the image `gcr.io/google-samples/hello-app:2.0`. The container exposes port 8080, making the application accessible when combined with a service or load balancer. Overall, this deployment configuration automates the process of deploying multiple instances of a sample hello-world application in a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\load-balancer-example.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('42b00f99-63bd-578d-8eea-bde894782b62'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Service configuration in YAML format. It creates a service named \"my-service\" that exposes an application labeled \"MyApp\" using TCP on port 80. The service forwards incoming traffic to the application\\'s container listening on port 9376. Essentially, it acts as a stable endpoint for accessing the app, enabling load balancing and network connectivity within the Kubernetes cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\nspec:\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n      targetPort: 9376\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Service configuration in YAML format. It creates a service named \"my-service\" that exposes an application labeled \"MyApp\" using TCP on port 80. The service forwards incoming traffic to the application\\'s container listening on port 9376. Essentially, it acts as a stable endpoint for accessing the app, enabling load balancing and network connectivity within the Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\simple-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('438e7c2d-40f3-520c-9790-67e5482be5c5'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes ValidatingAdmissionPolicy resource that enforces a validation rule on deployment resources. This policy is named \"deploy-replica-policy.example.com\" and applies to \"deployments\" within the \"apps\" API group during creation and update operations. The core validation expression specifies that the number of replicas for a deployment must be greater than one. If this condition is not met, the policy rejects the request, returning an error message \"must be replicated\" with the reason labeled as \"Invalid\". \\n\\nIt is important to note that the validation expression contains an error: it references \"object.replicas\" instead of the correct \"object.spec.replicas\". This mistake would prevent the policy from functioning as intended. Overall, this policy aims to ensure that all deployment resources are configured with more than one replica, promoting high availability through admission control in Kubernetes clusters.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"deploy-replica-policy.example.com\"\\nspec:\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  validations:\\n  - expression: \"object.replicas > 1\" # should be \"object.spec.replicas > 1\"\\n    message: \"must be replicated\"\\n    reason: Invalid\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes ValidatingAdmissionPolicy resource that enforces a validation rule on deployment resources. This policy is named \"deploy-replica-policy.example.com\" and applies to \"deployments\" within the \"apps\" API group during creation and update operations. The core validation expression specifies that the number of replicas for a deployment must be greater than one. If this condition is not met, the policy rejects the request, returning an error message \"must be replicated\" with the reason labeled as \"Invalid\". \\n\\nIt is important to note that the validation expression contains an error: it references \"object.replicas\" instead of the correct \"object.spec.replicas\". This mistake would prevent the policy from functioning as intended. Overall, this policy aims to ensure that all deployment resources are configured with more than one replica, promoting high availability through admission control in Kubernetes clusters.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\typechecking.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('44433bb9-0172-5190-a4ae-091ad820989e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content describes a Kubernetes DaemonSet configuration for deploying the konnectivity agent, which facilitates network communication within a Kubernetes cluster. This configuration ensures that the agent runs on nodes within the `kube-system` namespace, emphasizing critical system operations. The DaemonSet manages the deployment of a container running the konnectivity proxy agent, which connects to a specified proxy server IP and port, and includes parameters for logging, SSL credentials, and authentication tokens. It also sets up health checks via a liveness probe to monitor the agent\\'s status.\\n\\nThis YAML code specifies the setup of a DaemonSet with detailed container configurations: defining the container image, startup command, and arguments for connecting to the proxy server, as well as mounting a token volume for secure authentication. The code additionally configures health monitoring through an HTTP GET request to the `/healthz` endpoint on port 8134, ensuring the agent\\'s ongoing availability. The inclusion of tolerations and priority class ensures the agent\\'s proper placement and critical status within the cluster. Overall, this configuration provides a resilient and secure deployment mechanism for the konnectivity agent, essential for efficient network management in Kubernetes.\\napiVersion: apps/v1\\n# Alternatively, you can deploy the agents as Deployments. It is not necessary\\n# to have an agent on each node.\\nkind: DaemonSet\\nmetadata:\\n  labels:\\n    addonmanager.kubernetes.io/mode: Reconcile\\n    k8s-app: konnectivity-agent\\n  namespace: kube-system\\n  name: konnectivity-agent\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: konnectivity-agent\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: konnectivity-agent\\n    spec:\\n      priorityClassName: system-cluster-critical\\n      tolerations:\\n        - key: \"CriticalAddonsOnly\"\\n          operator: \"Exists\"\\n      containers:\\n        - image: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent:v0.0.37\\n          name: konnectivity-agent\\n          command: [\"/proxy-agent\"]\\n          args: [\\n                  \"--logtostderr=true\",\\n                  \"--ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\\n                  # Since the konnectivity server runs with hostNetwork=true,\\n                  # this is the IP address of the master machine.\\n                  \"--proxy-server-host=35.225.206.7\",\\n                  \"--proxy-server-port=8132\",\\n                  \"--admin-server-port=8133\",\\n                  \"--health-server-port=8134\",\\n                  \"--service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token\"\\n                  ]\\n          volumeMounts:\\n            - mountPath: /var/run/secrets/tokens\\n              name: konnectivity-agent-token\\n          livenessProbe:\\n            httpGet:\\n              port: 8134\\n              path: /healthz\\n            initialDelaySeconds: 15\\n            timeoutSeconds: 15\\n      serviceAccountName: konnectivity-agent\\n      volumes:\\n        - name: konnectivity-agent-token\\n          projected:\\n            sources:\\n              - serviceAccountToken:\\n                  path: konnectivity-agent-token\\n                  audience: system:konnectivity-server\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-agent.yaml', 'summary': \"The provided content describes a Kubernetes DaemonSet configuration for deploying the konnectivity agent, which facilitates network communication within a Kubernetes cluster. This configuration ensures that the agent runs on nodes within the `kube-system` namespace, emphasizing critical system operations. The DaemonSet manages the deployment of a container running the konnectivity proxy agent, which connects to a specified proxy server IP and port, and includes parameters for logging, SSL credentials, and authentication tokens. It also sets up health checks via a liveness probe to monitor the agent's status.\\n\\nThis YAML code specifies the setup of a DaemonSet with detailed container configurations: defining the container image, startup command, and arguments for connecting to the proxy server, as well as mounting a token volume for secure authentication. The code additionally configures health monitoring through an HTTP GET request to the `/healthz` endpoint on port 8134, ensuring the agent's ongoing availability. The inclusion of tolerations and priority class ensures the agent's proper placement and critical status within the cluster. Overall, this configuration provides a resilient and secure deployment mechanism for the konnectivity agent, essential for efficient network management in Kubernetes.\", 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('44db1005-f5b1-59c5-a47f-953f9cc8b37a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes ValidatingAdmissionPolicy configuration that enforces a policy on deployment replicas. It uses the admission registration API to define a policy named \"deploy-replica-policy.example.com,\" which validates create and update operations on deployments within the \"apps\" API group. The policy references a custom parameter kind \"ReplicaLimit\" from the \"rules.example.com/v1\" API version, which likely specifies maximum allowed replicas.\\n\\nThe policy includes a validation rule with an expression that checks whether the number of replicas specified in a deployment (object.spec.replicas) is less than or equal to a maximum value defined by the parameters (params.maxReplicas). If the condition fails, a message is generated indicating the replicas must not exceed the maximum limit, and the admission request is rejected with an \"Invalid\" reason.\\n\\nOverall, this declarative configuration applies a quota or limit on deployment replicas in Kubernetes, ensuring resource constraints are maintained as per organizational policies.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"deploy-replica-policy.example.com\"\\nspec:\\n  paramKind:\\n    apiVersion: rules.example.com/v1\\n    kind: ReplicaLimit\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  validations:\\n  - expression: \"object.spec.replicas <= params.maxReplicas\"\\n    messageExpression: \"\\'object.spec.replicas must be no greater than \\' + string(params.maxReplicas)\"\\n    reason: Invalid\\n\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes ValidatingAdmissionPolicy configuration that enforces a policy on deployment replicas. It uses the admission registration API to define a policy named \"deploy-replica-policy.example.com,\" which validates create and update operations on deployments within the \"apps\" API group. The policy references a custom parameter kind \"ReplicaLimit\" from the \"rules.example.com/v1\" API version, which likely specifies maximum allowed replicas.\\n\\nThe policy includes a validation rule with an expression that checks whether the number of replicas specified in a deployment (object.spec.replicas) is less than or equal to a maximum value defined by the parameters (params.maxReplicas). If the condition fails, a message is generated indicating the replicas must not exceed the maximum limit, and the admission request is rejected with an \"Invalid\" reason.\\n\\nOverall, this declarative configuration applies a quota or limit on deployment replicas in Kubernetes, ensuring resource constraints are maintained as per organizational policies.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\deployment-replicas-policy.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('45c39324-1bdd-5058-a231-f81befb02f9a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration that demonstrates the use of init containers to perform setup tasks before the main application container runs. The Pod has two containers: an init container named \"install\" that uses the BusyBox image to download a webpage and store it in a shared volume, and a main container running Nginx to serve that webpage. The init container executes a wget command to fetch content from \"http://info.cern.ch\" and saves it to a directory mounted as a volume. Once initialization is complete, the Nginx container mounts the same volume to serve the downloaded content. The configuration showcases how init containers can prepare the environment, ensuring that the application container starts with the necessary data or setup completed.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: init-demo\\nspec:\\n  containers:\\n  - name: nginx\\n    image: nginx\\n    ports:\\n    - containerPort: 80\\n    volumeMounts:\\n    - name: workdir\\n      mountPath: /usr/share/nginx/html\\n  # These containers are run during pod initialization\\n  initContainers:\\n  - name: install\\n    image: busybox:1.28\\n    command:\\n    - wget\\n    - \"-O\"\\n    - \"/work-dir/index.html\"\\n    - http://info.cern.ch\\n    volumeMounts:\\n    - name: workdir\\n      mountPath: \"/work-dir\"\\n  dnsPolicy: Default\\n  volumes:\\n  - name: workdir\\n    emptyDir: {}\\n\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration that demonstrates the use of init containers to perform setup tasks before the main application container runs. The Pod has two containers: an init container named \"install\" that uses the BusyBox image to download a webpage and store it in a shared volume, and a main container running Nginx to serve that webpage. The init container executes a wget command to fetch content from \"http://info.cern.ch\" and saves it to a directory mounted as a volume. Once initialization is complete, the Nginx container mounts the same volume to serve the downloaded content. The configuration showcases how init containers can prepare the environment, ensuring that the application container starts with the necessary data or setup completed.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\init-containers.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4615f8ec-b70d-5cf4-8293-bc8ebe489686'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content comprises Kubernetes manifests defining resources for deploying a ZooKeeper ensemble. The configuration includes two services, a PodDisruptionBudget, and a StatefulSet that manages three ZooKeeper pods. The `zk-hs` service is headless, allowing direct pod-to-pod communication essential for ZooKeeper quorum, with ports configured for server election and leader election. The `zk-cs` service is a cluster IP service exposing the client port 2181 for external ZooKeeper clients.\\n\\nThe PodDisruptionBudget (`zk-pdb`) ensures that at most one ZooKeeper pod is voluntarily disrupted during maintenance or updates, enhancing high availability. The core of the deployment is the StatefulSet `zk`, which manages three replicas with ordered, rolling updates, and pod anti-affinity policies to distribute pods across different nodes for fault tolerance. \\n\\nEach pod runs a container configured to start ZooKeeper with specific command-line parameters, including server count, data directories, and network ports. Readiness and liveness probes execute a custom \"zookeeper-ready\" command to monitor pod health. Persistent storage is configured via volume claim templates with 10Gi of storage, mounted at `/var/lib/zookeeper` to maintain data consistency across pod restarts. The container runs as user ID 1000 for security purposes. Overall, these manifests orchestrate a resilient, scalable ZooKeeper ensemble suitable for distributed applications requiring coordination services.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: zk-hs\\n  labels:\\n    app: zk\\nspec:\\n  ports:\\n  - port: 2888\\n    name: server\\n  - port: 3888\\n    name: leader-election\\n  clusterIP: None\\n  selector:\\n    app: zk\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: zk-cs\\n  labels:\\n    app: zk\\nspec:\\n  ports:\\n  - port: 2181\\n    name: client\\n  selector:\\n    app: zk\\n---\\napiVersion: policy/v1\\nkind: PodDisruptionBudget\\nmetadata:\\n  name: zk-pdb\\nspec:\\n  selector:\\n    matchLabels:\\n      app: zk\\n  maxUnavailable: 1\\n---\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: zk\\nspec:\\n  selector:\\n    matchLabels:\\n      app: zk\\n  serviceName: zk-hs\\n  replicas: 3\\n  updateStrategy:\\n    type: RollingUpdate\\n  podManagementPolicy: OrderedReady\\n  template:\\n    metadata:\\n      labels:\\n        app: zk\\n    spec:\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            - labelSelector:\\n                matchExpressions:\\n                  - key: \"app\"\\n                    operator: In\\n                    values:\\n                    - zk\\n              topologyKey: \"kubernetes.io/hostname\"\\n      containers:\\n      - name: kubernetes-zookeeper\\n        imagePullPolicy: Always\\n        image: \"registry.k8s.io/kubernetes-zookeeper:1.0-3.4.10\"\\n        resources:\\n          requests:\\n            memory: \"1Gi\"\\n            cpu: \"0.5\"\\n        ports:\\n        - containerPort: 2181\\n          name: client\\n        - containerPort: 2888\\n          name: server\\n        - containerPort: 3888\\n          name: leader-election\\n        command:\\n        - sh\\n        - -c\\n        - \"start-zookeeper \\\\\\n          --servers=3 \\\\\\n          --data_dir=/var/lib/zookeeper/data \\\\\\n          --data_log_dir=/var/lib/zookeeper/data/log \\\\\\n          --conf_dir=/opt/zookeeper/conf \\\\\\n          --client_port=2181 \\\\\\n          --election_port=3888 \\\\\\n          --server_port=2888 \\\\\\n          --tick_time=2000 \\\\\\n          --init_limit=10 \\\\\\n          --sync_limit=5 \\\\\\n          --heap=512M \\\\\\n          --max_client_cnxns=60 \\\\\\n          --snap_retain_count=3 \\\\\\n          --purge_interval=12 \\\\\\n          --max_session_timeout=40000 \\\\\\n          --min_session_timeout=4000 \\\\\\n          --log_level=INFO\"\\n        readinessProbe:\\n          exec:\\n            command:\\n            - sh\\n            - -c\\n            - \"zookeeper-ready 2181\"\\n          initialDelaySeconds: 10\\n          timeoutSeconds: 5\\n        livenessProbe:\\n          exec:\\n            command:\\n            - sh\\n            - -c\\n            - \"zookeeper-ready 2181\"\\n          initialDelaySeconds: 10\\n          timeoutSeconds: 5\\n        volumeMounts:\\n        - name: datadir\\n          mountPath: /var/lib/zookeeper\\n      securityContext:\\n        runAsUser: 1000\\n        fsGroup: 1000\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: datadir\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      resources:\\n        requests:\\n          storage: 10Gi\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\zookeeper\\\\zookeeper.yaml', 'summary': 'The provided content comprises Kubernetes manifests defining resources for deploying a ZooKeeper ensemble. The configuration includes two services, a PodDisruptionBudget, and a StatefulSet that manages three ZooKeeper pods. The `zk-hs` service is headless, allowing direct pod-to-pod communication essential for ZooKeeper quorum, with ports configured for server election and leader election. The `zk-cs` service is a cluster IP service exposing the client port 2181 for external ZooKeeper clients.\\n\\nThe PodDisruptionBudget (`zk-pdb`) ensures that at most one ZooKeeper pod is voluntarily disrupted during maintenance or updates, enhancing high availability. The core of the deployment is the StatefulSet `zk`, which manages three replicas with ordered, rolling updates, and pod anti-affinity policies to distribute pods across different nodes for fault tolerance. \\n\\nEach pod runs a container configured to start ZooKeeper with specific command-line parameters, including server count, data directories, and network ports. Readiness and liveness probes execute a custom \"zookeeper-ready\" command to monitor pod health. Persistent storage is configured via volume claim templates with 10Gi of storage, mounted at `/var/lib/zookeeper` to maintain data consistency across pod restarts. The container runs as user ID 1000 for security purposes. Overall, these manifests orchestrate a resilient, scalable ZooKeeper ensemble suitable for distributed applications requiring coordination services.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4645b9a9-cf85-57d1-b4dc-7d30c57d205e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration using YAML syntax. It specifies a pod named \"mypod\" with a label \"foo: bar\" and includes topology spread constraints to evenly distribute the pod across different zones and nodes based on the label \"foo: bar\". The constraints aim to ensure fault tolerance and balanced workload distribution, with a maximum skew of 1 indicating that pods should not be unevenly distributed beyond one pod difference between zones or nodes. If the constraints cannot be satisfied, scheduling is prevented. The pod contains a single container named \"pause\" that uses a minimal pause image from the registry, typically used as a placeholder or for testing networking and scheduling policies.\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: mypod\\n  labels:\\n    foo: bar\\nspec:\\n  topologySpreadConstraints:\\n  - maxSkew: 1\\n    topologyKey: zone\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  - maxSkew: 1\\n    topologyKey: node\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\two-constraints.yaml', 'summary': 'This content defines a Kubernetes Pod configuration using YAML syntax. It specifies a pod named \"mypod\" with a label \"foo: bar\" and includes topology spread constraints to evenly distribute the pod across different zones and nodes based on the label \"foo: bar\". The constraints aim to ensure fault tolerance and balanced workload distribution, with a maximum skew of 1 indicating that pods should not be unevenly distributed beyond one pod difference between zones or nodes. If the constraints cannot be satisfied, scheduling is prevented. The pod contains a single container named \"pause\" that uses a minimal pause image from the registry, typically used as a placeholder or for testing networking and scheduling policies.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4734f3d0-a9ac-52f5-9fd8-f2bc32abbdc9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes configuration defining a PersistentVolume (PV) and a PersistentVolumeClaim (PVC). The PV is configured for local storage, with a capacity of 20Gi, accessed in ReadWriteOnce mode, and stored at the host path \"/mnt/data\". It is labeled with \"type: local\" and uses a manual storage class. The PVC requests a matching 20Gi storage with the same \"manual\" storage class and access mode, enabling dynamic binding to the PV. This setup facilitates persistent storage for applications such as a MySQL database, ensuring data durability across pod restarts.\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: mysql-pv-volume\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 20Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \"/mnt/data\"\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mysql-pv-claim\\nspec:\\n  storageClassName: manual\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 20Gi\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes configuration defining a PersistentVolume (PV) and a PersistentVolumeClaim (PVC). The PV is configured for local storage, with a capacity of 20Gi, accessed in ReadWriteOnce mode, and stored at the host path \"/mnt/data\". It is labeled with \"type: local\" and uses a manual storage class. The PVC requests a matching 20Gi storage with the same \"manual\" storage class and access mode, enabling dynamic binding to the PV. This setup facilitates persistent storage for applications such as a MySQL database, ensuring data durability across pod restarts.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-pv.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('47a5f591-2edc-56cb-88db-afb125dec53f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This is a Kubernetes NetworkPolicy configuration that controls network access to Pods labeled with `app: nginx`. It specifies that only Pods with the label `access: \"true\"` are allowed to send ingress traffic to these nginx Pods. The policy helps secure the deployment by restricting access and ensuring that only authorized Pods can communicate with the nginx Pods, based on specified labels.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: access-nginx\\nspec:\\n  podSelector:\\n    matchLabels:\\n      app: nginx\\n  ingress:\\n  - from:\\n    - podSelector:\\n        matchLabels:\\n          access: \"true\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-policy.yaml', 'summary': 'This is a Kubernetes NetworkPolicy configuration that controls network access to Pods labeled with `app: nginx`. It specifies that only Pods with the label `access: \"true\"` are allowed to send ingress traffic to these nginx Pods. The policy helps secure the deployment by restricting access and ensuring that only authorized Pods can communicate with the nginx Pods, based on specified labels.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4931e5e2-e778-5b97-95bb-7a7d1d76e99a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Secret resource of the type `kubernetes.io/service-account-token`. It is used to create a token linked to a specific service account, in this case, named `myserviceaccount`. The secret is named `mysecretname`, and it is annotated with the service account name, establishing an association between the secret and the service account. This type of secret is typically used for authentication purposes, allowing pods or components to securely access the Kubernetes API using the credentials generated for the service account.\\napiVersion: v1\\nkind: Secret\\ntype: kubernetes.io/service-account-token\\nmetadata:\\n  name: mysecretname\\n  annotations:\\n    kubernetes.io/service-account.name: myserviceaccount\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\serviceaccount\\\\mysecretname.yaml', 'summary': 'This content defines a Kubernetes Secret resource of the type `kubernetes.io/service-account-token`. It is used to create a token linked to a specific service account, in this case, named `myserviceaccount`. The secret is named `mysecretname`, and it is annotated with the service account name, establishing an association between the secret and the service account. This type of secret is typically used for authentication purposes, allowing pods or components to securely access the Kubernetes API using the credentials generated for the service account.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('494b77fc-71ac-5437-bb9c-e78e136eb610'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"pod-quota-demo,\" which manages three replicas of a pod running the Nginx container image. The deployment specifies a label \"purpose: quota-demo\" to identify its pods, enabling resource management and scheduling. This configuration is used to deploy a scalable, load-balanced group of Nginx pods for testing or demonstration purposes in a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: pod-quota-demo\\nspec:\\n  selector:\\n    matchLabels:\\n      purpose: quota-demo\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        purpose: quota-demo\\n    spec:\\n      containers:\\n      - name: pod-quota-demo\\n        image: nginx\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod-deployment.yaml', 'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"pod-quota-demo,\" which manages three replicas of a pod running the Nginx container image. The deployment specifies a label \"purpose: quota-demo\" to identify its pods, enabling resource management and scheduling. This configuration is used to deploy a scalable, load-balanced group of Nginx pods for testing or demonstration purposes in a Kubernetes environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4b4d7990-2ec2-513f-9d7d-e4b35ba27487'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"private-reg.\" The Pod contains a single container called \"private-reg-container\" which uses a placeholder image (\"<your-private-image>\") that is intended to be replaced with a private Docker image. To access this private image, the configuration specifies an \"imagePullSecrets\" entry named \"regcred,\" which is used for authentication credentials. \\n\\nThe purpose of this configuration is to securely pull a private container image from a private registry, utilizing the provided secret (\"regcred\") to authenticate. This setup is common when deploying containers that are stored in private registries, ensuring secure image retrieval within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: private-reg\\nspec:\\n  containers:\\n  - name: private-reg-container\\n    image: <your-private-image>\\n  imagePullSecrets:\\n  - name: regcred\\n\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"private-reg.\" The Pod contains a single container called \"private-reg-container\" which uses a placeholder image (\"<your-private-image>\") that is intended to be replaced with a private Docker image. To access this private image, the configuration specifies an \"imagePullSecrets\" entry named \"regcred,\" which is used for authentication credentials. \\n\\nThe purpose of this configuration is to securely pull a private container image from a private registry, utilizing the provided secret (\"regcred\") to authenticate. This setup is common when deploying containers that are stored in private registries, ensuring secure image retrieval within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\private-reg-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4d04ba79-7e93-59d3-bff8-91fee37085dc'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Ingress resource configuration that manages external access to services within a cluster. The Ingress is named \"ingress-resource-backend\" and specifies a default backend, which routes unspecified requests to a StorageBucket resource called \"static-assets.\" Additionally, it defines a rule for requests targeting the \"/icons\" path, directing those requests to another StorageBucket named \"icon-assets.\" This setup leverages custom resources using API groups and kinds, indicating an extension of the standard Kubernetes API to integrate storage buckets directly into ingress routing.\\n\\nIn essence, this configuration enables external HTTP requests to be routed dynamically to specific storage buckets based on URL paths, facilitating organized and flexible access to static assets stored in cloud or cluster-managed storage. The use of custom resource definitions (CRDs) for StorageBucket objects illustrates a tailored approach to manage backend storage resources via Kubernetes ingress rules.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: ingress-resource-backend\\nspec:\\n  defaultBackend:\\n    resource:\\n      apiGroup: k8s.example.com\\n      kind: StorageBucket\\n      name: static-assets\\n  rules:\\n    - http:\\n        paths:\\n          - path: /icons\\n            pathType: ImplementationSpecific\\n            backend:\\n              resource:\\n                apiGroup: k8s.example.com\\n                kind: StorageBucket\\n                name: icon-assets\\n', 'chunk': '1/1', 'summary': 'This content describes a Kubernetes Ingress resource configuration that manages external access to services within a cluster. The Ingress is named \"ingress-resource-backend\" and specifies a default backend, which routes unspecified requests to a StorageBucket resource called \"static-assets.\" Additionally, it defines a rule for requests targeting the \"/icons\" path, directing those requests to another StorageBucket named \"icon-assets.\" This setup leverages custom resources using API groups and kinds, indicating an extension of the standard Kubernetes API to integrate storage buckets directly into ingress routing.\\n\\nIn essence, this configuration enables external HTTP requests to be routed dynamically to specific storage buckets based on URL paths, facilitating organized and flexible access to static assets stored in cloud or cluster-managed storage. The use of custom resource definitions (CRDs) for StorageBucket objects illustrates a tailored approach to manage backend storage resources via Kubernetes ingress rules.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\ingress-resource-backend.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4e3b8a6e-f509-551b-8020-32b4954eac0a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a comprehensive Kubernetes ConfigMap that configures Fluentd for log collection, parsing, and forwarding in a cloud environment, especially for Google Cloud Logging. It contains multiple configuration files detailing how logs from various sources such as Docker containers, system services, and Kubernetes components should be tailed, parsed (including multiline logs), and enriched with metadata. The configuration specifies input sources with tailored formats for different log types, including JSON, syslog, and custom multiline formats for large logs like kubelet and API server logs. It also demonstrates filters, such as JSON parsing and exception detection, and record reformers to tag logs with Kubernetes pod and container information.\\n\\nMoreover, the output section defines multiple Fluentd store directives: one set pushing logs to Google Cloud via the `google_cloud` plugin with buffer management for reliability and throughput, and a Prometheus plugin for monitoring Fluentd metrics. Separate buffers are designated for container logs and system logs, with control over buffer size, retry policy, and thread concurrency. When executed, this configuration ensures robust collection, parsing, tagging, forwarding, and monitoring of logs across a Kubernetes cluster, facilitating efficient log management and observability in cloud-native environments.\\n\\nAdditionally, the content includes a detailed explanation of the log collection process, buffer management strategies, and the differentiation between container and system logs, but it does not contain executable code beyond Fluentd configuration snippets.\\napiVersion: v1\\nkind: ConfigMap\\ndata:\\n  containers.input.conf: |-\\n    # This configuration file for Fluentd is used\\n    # to watch changes to Docker log files that live in the\\n    # directory /var/lib/docker/containers/ and are symbolically\\n    # linked to from the /var/log/containers directory using names that capture the\\n    # pod name and container name. These logs are then submitted to\\n    # Google Cloud Logging which assumes the installation of the cloud-logging plug-in.\\n    #\\n    # Example\\n    # =======\\n    # A line in the Docker log file might look like this JSON:\\n    #\\n    # {\"log\":\"2014/09/25 21:15:03 Got request with path wombat\\\\\\\\n\",\\n    #  \"stream\":\"stderr\",\\n    #   \"time\":\"2014-09-25T21:15:03.499185026Z\"}\\n    #\\n    # The record reformer is used to write the tag to focus on the pod name\\n    # and the Kubernetes container name. For example a Docker container\\'s logs\\n    # might be in the directory:\\n    #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\\n    # and in the file:\\n    #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\\n    # where 997599971ee6... is the Docker ID of the running container.\\n    # The Kubernetes kubelet makes a symbolic link to this file on the host machine\\n    # in the /var/log/containers directory which includes the pod name and the Kubernetes\\n    # container name:\\n    #    synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\\n    #    ->\\n    #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\\n    # The /var/log directory on the host is mapped to the /var/log directory in the container\\n    # running this instance of Fluentd and we end up collecting the file:\\n    #   /var/log/containers/synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\\n    # This results in the tag:\\n    #  var.log.containers.synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\\n    # The record reformer is used is discard the var.log.containers prefix and\\n    # the Docker container ID suffix and \"kubernetes.\" is pre-pended giving the tag:\\n    #   kubernetes.synthetic-logger-0.25lps-pod_default-synth-lgr\\n    # Tag is then parsed by google_cloud plugin and translated to the metadata,\\n    # visible in the log viewer\\n\\n    # Example:\\n    # {\"log\":\"[info:2016-02-16T16:04:05.930-08:00] Some log text here\\\\n\",\"stream\":\"stdout\",\"time\":\"2016-02-17T00:04:05.931087621Z\"}\\n    <source>\\n      type tail\\n      format json\\n      time_key time\\n      path /var/log/containers/*.log\\n      pos_file /var/log/gcp-containers.log.pos\\n      time_format %Y-%m-%dT%H:%M:%S.%N%Z\\n      tag reform.*\\n      read_from_head true\\n    </source>\\n\\n    <filter reform.**>\\n      type parser\\n      format /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<log>.*)/\\n      reserve_data true\\n      suppress_parse_error_log true\\n      key_name log\\n    </filter>\\n\\n    <match reform.**>\\n      type record_reformer\\n      enable_ruby true\\n      tag raw.kubernetes.${tag_suffix[4].split(\\'-\\')[0..-2].join(\\'-\\')}\\n    </match>\\n\\n    # Detect exceptions in the log output and forward them as one log entry.\\n    <match raw.kubernetes.**>\\n      @type copy\\n\\n      <store>\\n        @type prometheus\\n\\n        <metric>\\n          type counter\\n          name logging_line_count\\n          desc Total number of lines generated by application containers\\n          <labels>\\n            tag ${tag}\\n          </labels>\\n        </metric>\\n      </store>\\n      <store>\\n        @type detect_exceptions\\n\\n        remove_tag_prefix raw\\n        message log\\n        stream stream\\n        multiline_flush_interval 5\\n        max_bytes 500000\\n        max_lines 1000\\n      </store>\\n    </match>\\n  system.input.conf: |-\\n    # Example:\\n    # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script\\n    <source>\\n      type tail\\n      format syslog\\n      path /var/log/startupscript.log\\n      pos_file /var/log/gcp-startupscript.log.pos\\n      tag startupscript\\n    </source>\\n\\n    # Examples:\\n    # time=\"2016-02-04T06:51:03.053580605Z\" level=info msg=\"GET /containers/json\"\\n    # time=\"2016-02-04T07:53:57.505612354Z\" level=error msg=\"HTTP Error\" err=\"No such image: -f\" statusCode=404\\n    <source>\\n      type tail\\n      format /^time=\"(?<time>[^)]*)\" level=(?<severity>[^ ]*) msg=\"(?<message>[^\"]*)\"( err=\"(?<error>[^\"]*)\")?( statusCode=($<status_code>\\\\d+))?/\\n      path /var/log/docker.log\\n      pos_file /var/log/gcp-docker.log.pos\\n      tag docker\\n    </source>\\n\\n    # Example:\\n    # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal\\n    <source>\\n      type tail\\n      # Not parsing this, because it doesn\\'t have anything particularly useful to\\n      # parse out of it (like severities).\\n      format none\\n      path /var/log/etcd.log\\n      pos_file /var/log/gcp-etcd.log.pos\\n      tag etcd\\n    </source>\\n\\n    # Multi-line parsing is required for all the kube logs because very large log\\n    # statements, such as those that include entire object bodies, get split into\\n    # multiple lines by glog.\\n\\n    # Example:\\n    # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/kubelet.log\\n      pos_file /var/log/gcp-kubelet.log.pos\\n      tag kubelet\\n    </source>\\n\\n    # Example:\\n    # I1118 21:26:53.975789       6 proxier.go:1096] Port \"nodePort for kube-system/default-http-backend:http\" (:31429/tcp) was open before and is still needed\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/kube-proxy.log\\n      pos_file /var/log/gcp-kube-proxy.log.pos\\n      tag kube-proxy\\n    </source>\\n\\n    # Example:\\n    # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/kube-apiserver.log\\n      pos_file /var/log/gcp-kube-apiserver.log.pos\\n      tag kube-apiserver\\n    </source>\\n\\n    # Example:\\n    # 2017-02-09T00:15:57.992775796Z AUDIT: id=\"90c73c7c-97d6-4b65-9461-f94606ff825f\" ip=\"104.132.1.72\" method=\"GET\" user=\"kubecfg\" as=\"<self>\" asgroups=\"<lookup>\" namespace=\"default\" uri=\"/api/v1/namespaces/default/pods\"\\n    # 2017-02-09T00:15:57.993528822Z AUDIT: id=\"90c73c7c-97d6-4b65-9461-f94606ff825f\" response=\"200\"\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\S+\\\\s+AUDIT:/\\n      # Fields must be explicitly captured by name to be parsed into the record.\\n      # Fields may not always be present, and order may change, so this just looks\\n      # for a list of key=\"\\\\\"quoted\\\\\" value\" pairs separated by spaces.\\n      # Unknown fields are ignored.\\n      # Note: We can\\'t separate query/response lines as format1/format2 because\\n      #       they don\\'t always come one after the other for a given query.\\n      # TODO: Maybe add a JSON output mode to audit log so we can get rid of this?\\n      format1 /^(?<time>\\\\S+) AUDIT:(?: (?:id=\"(?<id>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|ip=\"(?<ip>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|method=\"(?<method>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|user=\"(?<user>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|groups=\"(?<groups>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|as=\"(?<as>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|asgroups=\"(?<asgroups>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|namespace=\"(?<namespace>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|uri=\"(?<uri>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|response=\"(?<response>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|\\\\w+=\"(?:[^\"\\\\\\\\]|\\\\\\\\.)*\"))*/\\n      time_format %FT%T.%L%Z\\n      path /var/log/kube-apiserver-audit.log\\n      pos_file /var/log/gcp-kube-apiserver-audit.log.pos\\n      tag kube-apiserver-audit\\n    </source>\\n\\n    # Example:\\n    # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn\\'t need update for service kube-system/kubernetes-dashboard\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/kube-controller-manager.log\\n      pos_file /var/log/gcp-kube-controller-manager.log.pos\\n      tag kube-controller-manager\\n    </source>\\n\\n    # Example:\\n    # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/kube-scheduler.log\\n      pos_file /var/log/gcp-kube-scheduler.log.pos\\n      tag kube-scheduler\\n    </source>\\n\\n    # Example:\\n    # I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/rescheduler.log\\n      pos_file /var/log/gcp-rescheduler.log.pos\\n      tag rescheduler\\n    </source>\\n\\n    # Example:\\n    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/glbc.log\\n      pos_file /var/log/gcp-glbc.log.pos\\n      tag glbc\\n    </source>\\n\\n    # Example:\\n    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\\n    <source>\\n      type tail\\n      format multiline\\n      multiline_flush_interval 5s\\n      format_firstline /^\\\\w\\\\d{4}/\\n      format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\\n      time_format %m%d %H:%M:%S.%N\\n      path /var/log/cluster-autoscaler.log\\n      pos_file /var/log/gcp-cluster-autoscaler.log.pos\\n      tag cluster-autoscaler\\n    </source>\\n\\n    # Logs from systemd-journal for interesting services.\\n    <source>\\n      type systemd\\n      filters [{ \"_SYSTEMD_UNIT\": \"docker.service\" }]\\n      pos_file /var/log/gcp-journald-docker.pos\\n      read_from_head true\\n      tag docker\\n    </source>\\n\\n    <source>\\n      type systemd\\n      filters [{ \"_SYSTEMD_UNIT\": \"kubelet.service\" }]\\n      pos_file /var/log/gcp-journald-kubelet.pos\\n      read_from_head true\\n      tag kubelet\\n    </source>\\n  monitoring.conf: |-\\n    # Prometheus monitoring\\n    <source>\\n      @type prometheus\\n      port 80\\n    </source>\\n\\n    <source>\\n      @type prometheus_monitor\\n    </source>\\n  output.conf: |-\\n    # We use 2 output stanzas - one to handle the container logs and one to handle\\n    # the node daemon logs, the latter of which explicitly sends its logs to the\\n    # compute.googleapis.com service rather than container.googleapis.com to keep\\n    # them separate since most users don\\'t care about the node logs.\\n    <match kubernetes.**>\\n      @type copy\\n\\n      <store>\\n        @type google_cloud\\n\\n        # Set the buffer type to file to improve the reliability and reduce the memory consumption\\n        buffer_type file\\n        buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer\\n        # Set queue_full action to block because we want to pause gracefully\\n        # in case of the off-the-limits load instead of throwing an exception\\n        buffer_queue_full_action block\\n        # Set the chunk limit conservatively to avoid exceeding the GCL limit\\n        # of 10MiB per write request.\\n        buffer_chunk_limit 2M\\n        # Cap the combined memory usage of this buffer and the one below to\\n        # 2MiB/chunk * (6 + 2) chunks = 16 MiB\\n        buffer_queue_limit 6\\n        # Never wait more than 5 seconds before flushing logs in the non-error case.\\n        flush_interval 5s\\n        # Never wait longer than 30 seconds between retries.\\n        max_retry_wait 30\\n        # Disable the limit on the number of retries (retry forever).\\n        disable_retry_limit\\n        # Use multiple threads for processing.\\n        num_threads 2\\n      </store>\\n      <store>\\n        @type prometheus\\n\\n        <metric>\\n          type counter\\n          name logging_entry_count\\n          desc Total number of log entries generated by either an application container or a system component\\n          <labels>\\n            tag ${tag}\\n            component container\\n          </labels>\\n        </metric>\\n      </store>\\n    </match>\\n\\n    # Keep a smaller buffer here since these logs are less important than the user\\'s\\n    # container logs.\\n    <match **>\\n      @type copy\\n\\n      <store>\\n        @type google_cloud\\n\\n        detect_subservice false\\n        buffer_type file\\n        buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer\\n        buffer_queue_full_action block\\n        buffer_chunk_limit 2M\\n        buffer_queue_limit 2\\n        flush_interval 5s\\n        max_retry_wait 30\\n        disable_retry_limit\\n        num_threads 2\\n      </store>\\n      <store>\\n        @type prometheus\\n\\n        <metric>\\n          type counter\\n          name logging_entry_count\\n          desc Total number of log entries generated by either an application container or a system component\\n          <labels>\\n            tag ${tag}\\n            component system\\n          </labels>\\n        </metric>\\n      </store>\\n    </match>\\nmetadata:\\n  name: fluentd-gcp-config\\n  labels:\\n    addonmanager.kubernetes.io/mode: Reconcile\\n', 'chunk': '1/1', 'summary': 'This content provides a comprehensive Kubernetes ConfigMap that configures Fluentd for log collection, parsing, and forwarding in a cloud environment, especially for Google Cloud Logging. It contains multiple configuration files detailing how logs from various sources such as Docker containers, system services, and Kubernetes components should be tailed, parsed (including multiline logs), and enriched with metadata. The configuration specifies input sources with tailored formats for different log types, including JSON, syslog, and custom multiline formats for large logs like kubelet and API server logs. It also demonstrates filters, such as JSON parsing and exception detection, and record reformers to tag logs with Kubernetes pod and container information.\\n\\nMoreover, the output section defines multiple Fluentd store directives: one set pushing logs to Google Cloud via the `google_cloud` plugin with buffer management for reliability and throughput, and a Prometheus plugin for monitoring Fluentd metrics. Separate buffers are designated for container logs and system logs, with control over buffer size, retry policy, and thread concurrency. When executed, this configuration ensures robust collection, parsing, tagging, forwarding, and monitoring of logs across a Kubernetes cluster, facilitating efficient log management and observability in cloud-native environments.\\n\\nAdditionally, the content includes a detailed explanation of the log collection process, buffer management strategies, and the differentiation between container and system logs, but it does not contain executable code beyond Fluentd configuration snippets.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\fluentd-gcp-configmap.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4edd08dc-cc10-5070-9398-e2c47a9e0945'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"redis-master\" that runs a Redis container. The container is labeled \"master\" and uses the official Redis image. An environment variable named \"MASTER\" is set to \"true\" to identify this container as the primary Redis instance. The container exposes port 6379, which is the default Redis port, allowing communication with other services or containers. Overall, this configuration is used to deploy a single Redis master node within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: redis-master\\n  labels:\\n    app: redis\\nspec:\\n  containers:\\n    - name: master\\n      image: redis\\n      env:\\n        - name: MASTER\\n          value: \"true\"\\n      ports:\\n        - containerPort: 6379\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\redis-pod.yaml', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"redis-master\" that runs a Redis container. The container is labeled \"master\" and uses the official Redis image. An environment variable named \"MASTER\" is set to \"true\" to identify this container as the primary Redis instance. The container exposes port 6379, which is the default Redis port, allowing communication with other services or containers. Overall, this configuration is used to deploy a single Redis master node within a Kubernetes cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4f087fb4-7896-5818-8a39-3bdc92a77130'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML content defines a Kubernetes Pod that demonstrates the use of the Downward API volume type. The Pod, named `kubernetes-downwardapi-volume-example`, includes metadata labels and annotations, which are mounted into the container through a special volume called `podinfo`. The volume leverages the Downward API to expose the Pod\\'s labels and annotations as files within the container\\'s filesystem at `/etc/podinfo/labels` and `/etc/podinfo/annotations`. \\n\\nThe container, based on the BusyBox image, runs a shell script in an infinite loop, periodically checking for the presence of these files. If found, it outputs their contents to the terminal every 5 seconds. This setup illustrates how Kubernetes can dynamically expose Pod metadata inside containers, useful for configuration, management, or debugging purposes. The code\\'s main purpose is to show how to mount and read Pod labels and annotations via the Downward API, enabling containers to adapt based on their runtime metadata.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubernetes-downwardapi-volume-example\\n  labels:\\n    zone: us-est-coast\\n    cluster: test-cluster1\\n    rack: rack-22\\n  annotations:\\n    build: two\\n    builder: john-doe\\nspec:\\n  containers:\\n    - name: client-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [\"sh\", \"-c\"]\\n      args:\\n      - while true; do\\n          if [[ -e /etc/podinfo/labels ]]; then\\n            echo -en \\'\\\\n\\\\n\\'; cat /etc/podinfo/labels; fi;\\n          if [[ -e /etc/podinfo/annotations ]]; then\\n            echo -en \\'\\\\n\\\\n\\'; cat /etc/podinfo/annotations; fi;\\n          sleep 5;\\n        done;\\n      volumeMounts:\\n        - name: podinfo\\n          mountPath: /etc/podinfo\\n  volumes:\\n    - name: podinfo\\n      downwardAPI:\\n        items:\\n          - path: \"labels\"\\n            fieldRef:\\n              fieldPath: metadata.labels\\n          - path: \"annotations\"\\n            fieldRef:\\n              fieldPath: metadata.annotations\\n\\n', 'subchunk': '1/1', 'summary': \"This YAML content defines a Kubernetes Pod that demonstrates the use of the Downward API volume type. The Pod, named `kubernetes-downwardapi-volume-example`, includes metadata labels and annotations, which are mounted into the container through a special volume called `podinfo`. The volume leverages the Downward API to expose the Pod's labels and annotations as files within the container's filesystem at `/etc/podinfo/labels` and `/etc/podinfo/annotations`. \\n\\nThe container, based on the BusyBox image, runs a shell script in an infinite loop, periodically checking for the presence of these files. If found, it outputs their contents to the terminal every 5 seconds. This setup illustrates how Kubernetes can dynamically expose Pod metadata inside containers, useful for configuration, management, or debugging purposes. The code's main purpose is to show how to mount and read Pod labels and annotations via the Downward API, enabling containers to adapt based on their runtime metadata.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4f150909-c035-54dd-974b-362b5dbac1d3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes StatefulSet configuration for deploying a Cassandra database cluster with three replicas. The StatefulSet ensures each Cassandra node is uniquely identifiable and maintains persistent storage, facilitated by volumeClaimTemplates that request 1Gi of SSD storage from a custom StorageClass named \"fast.\" The pods run the Cassandra container with specific resource limits and environment variables configuring seed nodes, cluster name, data center, and rack information. The configuration includes preStop lifecycle hooks to gracefully drain each node before shutdown, as well as readiness probes to verify container health through a custom script. The StorageClass \"fast\" uses the minikube-hostpath provisioner with SSD storage capabilities, ensuring high-performance disk I/O suitable for a database cluster. Overall, this setup enables scalable, resilient deployment of Cassandra within a Kubernetes environment with persistent, high-speed storage.\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: cassandra\\n  labels:\\n    app: cassandra\\nspec:\\n  serviceName: cassandra\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: cassandra\\n  template:\\n    metadata:\\n      labels:\\n        app: cassandra\\n    spec:\\n      terminationGracePeriodSeconds: 500\\n      containers:\\n      - name: cassandra\\n        image: gcr.io/google-samples/cassandra:v13\\n        imagePullPolicy: Always\\n        ports:\\n        - containerPort: 7000\\n          name: intra-node\\n        - containerPort: 7001\\n          name: tls-intra-node\\n        - containerPort: 7199\\n          name: jmx\\n        - containerPort: 9042\\n          name: cql\\n        resources:\\n          limits:\\n            cpu: \"500m\"\\n            memory: 1Gi\\n          requests:\\n            cpu: \"500m\"\\n            memory: 1Gi\\n        securityContext:\\n          capabilities:\\n            add:\\n              - IPC_LOCK\\n        lifecycle:\\n          preStop:\\n            exec:\\n              command: \\n              - /bin/sh\\n              - -c\\n              - nodetool drain\\n        env:\\n          - name: MAX_HEAP_SIZE\\n            value: 512M\\n          - name: HEAP_NEWSIZE\\n            value: 100M\\n          - name: CASSANDRA_SEEDS\\n            value: \"cassandra-0.cassandra.default.svc.cluster.local\"\\n          - name: CASSANDRA_CLUSTER_NAME\\n            value: \"K8Demo\"\\n          - name: CASSANDRA_DC\\n            value: \"DC1-K8Demo\"\\n          - name: CASSANDRA_RACK\\n            value: \"Rack1-K8Demo\"\\n          - name: POD_IP\\n            valueFrom:\\n              fieldRef:\\n                fieldPath: status.podIP\\n        readinessProbe:\\n          exec:\\n            command:\\n            - /bin/bash\\n            - -c\\n            - /ready-probe.sh\\n          initialDelaySeconds: 15\\n          timeoutSeconds: 5\\n        # These volume mounts are persistent. They are like inline claims,\\n        # but not exactly because the names need to match exactly one of\\n        # the stateful pod volumes.\\n        volumeMounts:\\n        - name: cassandra-data\\n          mountPath: /cassandra_data\\n  # These are converted to volume claims by the controller\\n  # and mounted at the paths mentioned above.\\n  # do not use these in production until ssd GCEPersistentDisk or other ssd pd\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: cassandra-data\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      storageClassName: fast\\n      resources:\\n        requests:\\n          storage: 1Gi\\n---\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: fast\\nprovisioner: k8s.io/minikube-hostpath\\nparameters:\\n  type: pd-ssd\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes StatefulSet configuration for deploying a Cassandra database cluster with three replicas. The StatefulSet ensures each Cassandra node is uniquely identifiable and maintains persistent storage, facilitated by volumeClaimTemplates that request 1Gi of SSD storage from a custom StorageClass named \"fast.\" The pods run the Cassandra container with specific resource limits and environment variables configuring seed nodes, cluster name, data center, and rack information. The configuration includes preStop lifecycle hooks to gracefully drain each node before shutdown, as well as readiness probes to verify container health through a custom script. The StorageClass \"fast\" uses the minikube-hostpath provisioner with SSD storage capabilities, ensuring high-performance disk I/O suitable for a database cluster. Overall, this setup enables scalable, resilient deployment of Cassandra within a Kubernetes environment with persistent, high-speed storage.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-statefulset.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4f968e80-40a7-58a7-86d8-de109d687b26'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes DaemonSet configuration that deploys the node-problem-detector, a tool for monitoring node health in a cluster. This configuration specifies that the DaemonSet runs on all nodes within the kube-system namespace, with labels for management and version control. The specification ensures the container runs with hostNetwork enabled and uses privileged security context to access host-level features. The container uses the node-problem-detector image from the Kubernetes registry, with resource limits and requests specified to manage CPU and memory usage. It mounts the host’s log directory into the container at /log, enabling the detector to monitor system logs for problems, which is crucial for maintaining node health and proactive issue resolution in Kubernetes clusters.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: node-problem-detector-v0.1\\n  namespace: kube-system\\n  labels:\\n    k8s-app: node-problem-detector\\n    version: v0.1\\n    kubernetes.io/cluster-service: \"true\"\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: node-problem-detector  \\n      version: v0.1\\n      kubernetes.io/cluster-service: \"true\"\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: node-problem-detector\\n        version: v0.1\\n        kubernetes.io/cluster-service: \"true\"\\n    spec:\\n      hostNetwork: true\\n      containers:\\n      - name: node-problem-detector\\n        image: registry.k8s.io/node-problem-detector:v0.1\\n        securityContext:\\n          privileged: true\\n        resources:\\n          limits:\\n            cpu: \"200m\"\\n            memory: \"100Mi\"\\n          requests:\\n            cpu: \"20m\"\\n            memory: \"20Mi\"\\n        volumeMounts:\\n        - name: log\\n          mountPath: /log\\n          readOnly: true\\n      volumes:\\n      - name: log\\n        hostPath:\\n          path: /var/log/', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes DaemonSet configuration that deploys the node-problem-detector, a tool for monitoring node health in a cluster. This configuration specifies that the DaemonSet runs on all nodes within the kube-system namespace, with labels for management and version control. The specification ensures the container runs with hostNetwork enabled and uses privileged security context to access host-level features. The container uses the node-problem-detector image from the Kubernetes registry, with resource limits and requests specified to manage CPU and memory usage. It mounts the host’s log directory into the container at /log, enabling the detector to monitor system logs for problems, which is crucial for maintaining node health and proactive issue resolution in Kubernetes clusters.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\node-problem-detector.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4ff4c425-2ef1-58e9-9201-8e87eb193e80'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages four replicas of an Nginx web server container, instead of the previously configured two. The deployment specifies the use of the \"nginx:1.16.1\" Docker image and maps port 80 of the container to the network, ensuring multiple instances of Nginx are running for load balancing or high availability. The configuration utilizes labels for identification and selector matching to manage the pods efficiently.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 4 # Update the replicas from 2 to 4\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1\\n        ports:\\n        - containerPort: 80\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-scale.yaml', 'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages four replicas of an Nginx web server container, instead of the previously configured two. The deployment specifies the use of the \"nginx:1.16.1\" Docker image and maps port 80 of the container to the network, ensuring multiple instances of Nginx are running for load balancing or high availability. The configuration utilizes labels for identification and selector matching to manage the pods efficiently.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('4ff719e5-154a-5bf5-a377-5ae51e6825d1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes DaemonSet configuration for deploying Fluentd as a log collector across all nodes in a cluster. It specifies the Fluentd container image, environment variables, resource limits, volume mounts, and node selectors, ensuring Fluentd runs as a critical, node-level daemon. The configuration includes a command to run Fluentd with specific arguments and a liveness probe to monitor its health, which checks if Fluentd’s buffers are being updated within a certain timeframe or if it has become unresponsive. Volumes are mounted from the host to facilitate access to log files and configuration data, and tolerations ensure it can run on master nodes if necessary.\\n\\nThe code details how Fluentd is deployed with a rolling update strategy, prioritizes continuous log collection, and includes health checks to automatically restart if the service hangs or stops processing logs. The liveness probe script examines the timestamp of log buffer modifications and deletes buffers if they are stale, prompting container restarts to recover. Overall, this configuration ensures robust, automated, and resilient log collection on Kubernetes nodes, facilitating efficient log aggregation and monitoring.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluentd-gcp-v2.0\\n  labels:\\n    k8s-app: fluentd-gcp\\n    kubernetes.io/cluster-service: \"true\"\\n    addonmanager.kubernetes.io/mode: Reconcile\\n    version: v2.0\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: fluentd-gcp\\n      kubernetes.io/cluster-service: \"true\"\\n      version: v2.0\\n  updateStrategy:\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: fluentd-gcp\\n        kubernetes.io/cluster-service: \"true\"\\n        version: v2.0\\n      # This annotation ensures that fluentd does not get evicted if the node\\n      # supports critical pod annotation based priority scheme.\\n      # Note that this does not guarantee admission on the nodes (#40573).\\n      annotations:\\n        scheduler.alpha.kubernetes.io/critical-pod: \\'\\'\\n    spec:\\n      dnsPolicy: Default\\n      containers:\\n      - name: fluentd-gcp\\n        image: registry.k8s.io/fluentd-gcp:2.0.2\\n        # If fluentd consumes its own logs, the following situation may happen:\\n        # fluentd fails to send a chunk to the server => writes it to the log =>\\n        # tries to send this message to the server => fails to send a chunk and so on.\\n        # Writing to a file, which is not exported to the back-end prevents it.\\n        # It also allows to increase the fluentd verbosity by default.\\n        command:\\n          - \\'/bin/sh\\'\\n          - \\'-c\\'\\n          - \\'/run.sh $FLUENTD_ARGS 2>&1 >>/var/log/fluentd.log\\'\\n        env:\\n        - name: FLUENTD_ARGS\\n          value: --no-supervisor\\n        resources:\\n          limits:\\n            memory: 300Mi\\n          requests:\\n            cpu: 100m\\n            memory: 200Mi\\n        volumeMounts:\\n        - name: varlog\\n          mountPath: /var/log\\n        - name: varlibdockercontainers\\n          mountPath: /var/lib/docker/containers\\n          readOnly: true\\n        - name: libsystemddir\\n          mountPath: /host/lib\\n          readOnly: true\\n        - name: config-volume\\n          mountPath: /etc/fluent/config.d\\n        # Liveness probe is aimed to help in situations where fluentd\\n        # silently hangs for no apparent reasons until manual restart.\\n        # The idea of this probe is that if fluentd is not queueing or\\n        # flushing chunks for 5 minutes, something is not right. If\\n        # you want to change the fluentd configuration, reducing amount of\\n        # logs fluentd collects, consider changing the threshold or turning\\n        # liveness probe off completely.\\n        livenessProbe:\\n          initialDelaySeconds: 600\\n          periodSeconds: 60\\n          exec:\\n            command:\\n            - \\'/bin/sh\\'\\n            - \\'-c\\'\\n            - >\\n              LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300};\\n              STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};\\n              if [ ! -e /var/log/fluentd-buffers ];\\n              then\\n                exit 1;\\n              fi;\\n              LAST_MODIFIED_DATE=`stat /var/log/fluentd-buffers | grep Modify | sed -r \"s/Modify: (.*)/\\\\1/\"`;\\n              LAST_MODIFIED_TIMESTAMP=`date -d \"$LAST_MODIFIED_DATE\" +%s`;\\n              if [ `date +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $STUCK_THRESHOLD_SECONDS` ];\\n              then\\n                rm -rf /var/log/fluentd-buffers;\\n                exit 1;\\n              fi;\\n              if [ `date +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $LIVENESS_THRESHOLD_SECONDS` ];\\n              then\\n                exit 1;\\n              fi;\\n      nodeSelector:\\n        beta.kubernetes.io/fluentd-ds-ready: \"true\"\\n      tolerations:\\n      - key: \"node.alpha.kubernetes.io/ismaster\"\\n        effect: \"NoSchedule\"\\n      terminationGracePeriodSeconds: 30\\n      volumes:\\n      - name: varlog\\n        hostPath:\\n          path: /var/log\\n      - name: varlibdockercontainers\\n        hostPath:\\n          path: /var/lib/docker/containers\\n      - name: libsystemddir\\n        hostPath:\\n          path: /usr/lib64\\n      - name: config-volume\\n        configMap:\\n          name: fluentd-gcp-config\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes DaemonSet configuration for deploying Fluentd as a log collector across all nodes in a cluster. It specifies the Fluentd container image, environment variables, resource limits, volume mounts, and node selectors, ensuring Fluentd runs as a critical, node-level daemon. The configuration includes a command to run Fluentd with specific arguments and a liveness probe to monitor its health, which checks if Fluentd’s buffers are being updated within a certain timeframe or if it has become unresponsive. Volumes are mounted from the host to facilitate access to log files and configuration data, and tolerations ensure it can run on master nodes if necessary.\\n\\nThe code details how Fluentd is deployed with a rolling update strategy, prioritizes continuous log collection, and includes health checks to automatically restart if the service hangs or stops processing logs. The liveness probe script examines the timestamp of log buffer modifications and deletes buffers if they are stale, prompting container restarts to recover. Overall, this configuration ensures robust, automated, and resilient log collection on Kubernetes nodes, facilitating efficient log aggregation and monitoring.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\fluentd-gcp-ds.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('502cfabb-6a5e-55ad-8650-1cc2f9a55405'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ResourceQuota, which is used to control resource consumption within a namespace. Specifically, it sets a scope selector based on the PriorityClass, matching resources that have the \"cluster-services\" priority. This configuration helps ensure that resources associated with critical cluster services are managed and limited according to the specified scope, promoting resource governance and fairness across the cluster.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: pods-cluster-services\\nspec:\\n  scopeSelector:\\n    matchExpressions:\\n      - operator : In\\n        scopeName: PriorityClass\\n        values: [\"cluster-services\"]', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes ResourceQuota, which is used to control resource consumption within a namespace. Specifically, it sets a scope selector based on the PriorityClass, matching resources that have the \"cluster-services\" priority. This configuration helps ensure that resources associated with critical cluster services are managed and limited according to the specified scope, promoting resource governance and fairness across the cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\priority-class-resourcequota.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('503b44ba-bc46-5607-920b-b0502fa43b9b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content describes a Kubernetes configuration comprising two main resources: a Service and a Deployment. The Service, named \"my-nginx,\" exposes the application on two ports—8080 (mapped to container port 80) and 443—using the NodePort type, which allows external traffic to access the service via the node’s IP and specified port. It labels the service for identification and uses selectors to route traffic to the correct pods.\\n\\nThe Deployment, also named \"my-nginx,\" manages the lifecycle of the nginx application. It specifies one replica, meaning a single pod instance will run the containerized nginx server. The pod template within the Deployment defines volume mounts for secrets and configuration data: a secret volume named \"secret-volume\" (likely containing SSL certificates) and a ConfigMap volume named \"configmap-volume\" (containing nginx configuration). The container uses an image \"bprashanth/nginxhttps:1.0\" that likely supports HTTPS, and it exposes ports 80 and 443 within the container, mounting the secret and config map at paths relevant for SSL certificates and nginx configuration files, respectively. This setup enables a secure nginx server with external access and containerized configuration management.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-nginx\\n  labels:\\n    run: my-nginx\\nspec:\\n  type: NodePort\\n  ports:\\n  - port: 8080\\n    targetPort: 80\\n    protocol: TCP\\n    name: http\\n  - port: 443\\n    protocol: TCP\\n    name: https\\n  selector:\\n    run: my-nginx\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      run: my-nginx\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        run: my-nginx\\n    spec:\\n      volumes:\\n      - name: secret-volume\\n        secret:\\n          secretName: nginxsecret\\n      - name: configmap-volume\\n        configMap:\\n          name: nginxconfigmap\\n      containers:\\n      - name: nginxhttps\\n        image: bprashanth/nginxhttps:1.0\\n        ports:\\n        - containerPort: 443\\n        - containerPort: 80\\n        volumeMounts:\\n        - mountPath: /etc/nginx/ssl\\n          name: secret-volume\\n        - mountPath: /etc/nginx/conf.d\\n          name: configmap-volume\\n', 'chunk': '1/1', 'summary': 'The provided content describes a Kubernetes configuration comprising two main resources: a Service and a Deployment. The Service, named \"my-nginx,\" exposes the application on two ports—8080 (mapped to container port 80) and 443—using the NodePort type, which allows external traffic to access the service via the node’s IP and specified port. It labels the service for identification and uses selectors to route traffic to the correct pods.\\n\\nThe Deployment, also named \"my-nginx,\" manages the lifecycle of the nginx application. It specifies one replica, meaning a single pod instance will run the containerized nginx server. The pod template within the Deployment defines volume mounts for secrets and configuration data: a secret volume named \"secret-volume\" (likely containing SSL certificates) and a ConfigMap volume named \"configmap-volume\" (containing nginx configuration). The container uses an image \"bprashanth/nginxhttps:1.0\" that likely supports HTTPS, and it exposes ports 80 and 443 within the container, mounting the secret and config map at paths relevant for SSL certificates and nginx configuration files, respectively. This setup enables a secure nginx server with external access and containerized configuration management.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-secure-app.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('50768a8e-5caf-5cdf-8c34-6ec817f73f4b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ClusterRole resource named \"custom:aggregate-to-edit:endpoints\" using YAML configuration. The ClusterRole grants permissions related to the \"endpoints\" resource within the core API group, specifically allowing creation, deletion, patching, and updating endpoints. The annotations highlight a security consideration: previously, write permissions to endpoints were restricted in newer Kubernetes versions (post 1.22) due to CVE-2021-25740, as these permissions could be exploited to expose internal backend IPs or bypass security policies. The ClusterRole’s rules enable users to manipulate endpoint resources, which can impact network configurations and security if misused. The labels indicate that this role can be aggregated into higher-privilege roles like edit, supporting flexible RBAC configurations.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  annotations:\\n    kubernetes.io/description: |-\\n      Add endpoints write permissions to the edit and admin roles. This was\\n      removed by default in 1.22 because of CVE-2021-25740. See\\n      https://issue.k8s.io/103675. This can allow writers to direct LoadBalancer\\n      or Ingress implementations to expose backend IPs that would not otherwise\\n      be accessible, and can circumvent network policies or security controls\\n      intended to prevent/isolate access to those backends.\\n      EndpointSlices were never included in the edit or admin roles, so there\\n      is nothing to restore for the EndpointSlice API.\\n  labels:\\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\\n  name: custom:aggregate-to-edit:endpoints # you can change this if you wish\\nrules:\\n  - apiGroups: [\"\"]\\n    resources: [\"endpoints\"]\\n    verbs: [\"create\", \"delete\", \"deletecollection\", \"patch\", \"update\"]\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes ClusterRole resource named \"custom:aggregate-to-edit:endpoints\" using YAML configuration. The ClusterRole grants permissions related to the \"endpoints\" resource within the core API group, specifically allowing creation, deletion, patching, and updating endpoints. The annotations highlight a security consideration: previously, write permissions to endpoints were restricted in newer Kubernetes versions (post 1.22) due to CVE-2021-25740, as these permissions could be exploited to expose internal backend IPs or bypass security policies. The ClusterRole’s rules enable users to manipulate endpoint resources, which can impact network configurations and security if misused. The labels indicate that this role can be aggregated into higher-privilege roles like edit, supporting flexible RBAC configurations.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\endpoints-aggregated.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('51a8786c-94fb-5dfd-b0ba-ff79461b7204'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML, defining a pod named \"rro\" with specific volume and container settings. It creates a hostPath volume that references the host directory \"/mnt\" and mounts this volume into the container at three different paths with varying access permissions. The container uses the BusyBox image and runs the \"sleep infinity\" command to keep it running indefinitely.\\n\\nThe configuration details how the volume is mounted:\\n- At \"/mnt-rro\" as read-only with no mount propagation and recursive read-only enabled, meaning the container can read from the directory but not modify it or propagate mounts.\\n- At \"/mnt-ro\" as read-only, allowing the container to read but not write.\\n- At \"/mnt-rw\" without read-only restrictions, permitting both read and write operations.\\n\\nThis setup exemplifies different access controls for shared storage, useful for scenarios requiring strict read-only access or writable mounts in containerized environments.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: rro\\nspec:\\n  volumes:\\n    - name: mnt\\n      hostPath:\\n        # tmpfs is mounted on /mnt/tmpfs\\n        path: /mnt\\n  containers:\\n    - name: busybox\\n      image: busybox\\n      args: [\"sleep\", \"infinity\"]\\n      volumeMounts:\\n        # /mnt-rro/tmpfs is not writable\\n        - name: mnt\\n          mountPath: /mnt-rro\\n          readOnly: true\\n          mountPropagation: None\\n          recursiveReadOnly: Enabled\\n        # /mnt-ro/tmpfs is writable\\n        - name: mnt\\n          mountPath: /mnt-ro\\n          readOnly: true\\n        # /mnt-rw/tmpfs is writable\\n        - name: mnt\\n          mountPath: /mnt-rw\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML, defining a pod named \"rro\" with specific volume and container settings. It creates a hostPath volume that references the host directory \"/mnt\" and mounts this volume into the container at three different paths with varying access permissions. The container uses the BusyBox image and runs the \"sleep infinity\" command to keep it running indefinitely.\\n\\nThe configuration details how the volume is mounted:\\n- At \"/mnt-rro\" as read-only with no mount propagation and recursive read-only enabled, meaning the container can read from the directory but not modify it or propagate mounts.\\n- At \"/mnt-ro\" as read-only, allowing the container to read but not write.\\n- At \"/mnt-rw\" without read-only restrictions, permitting both read and write operations.\\n\\nThis setup exemplifies different access controls for shared storage, useful for scenarios requiring strict read-only access or writable mounts in containerized environments.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\rro.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('51cc205d-1138-58a6-a1a2-f109ba380931'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes namespace named \"development.\" It specifies the API version as v1 and sets the kind to \"Namespace,\" which organizes resources within the cluster. The metadata section assigns the name \"development\" to the namespace and labels it accordingly, facilitating resource management and identification within the Kubernetes environment. This YAML configuration is used to create a dedicated environment for development activities, isolating resources from other environments like staging or production.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: development\\n  labels:\\n    name: development\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-dev.yaml', 'summary': 'This content defines a Kubernetes namespace named \"development.\" It specifies the API version as v1 and sets the kind to \"Namespace,\" which organizes resources within the cluster. The metadata section assigns the name \"development\" to the namespace and labels it accordingly, facilitating resource management and identification within the Kubernetes environment. This YAML configuration is used to create a dedicated environment for development activities, isolating resources from other environments like staging or production.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('52e4b4b1-a424-59b4-b607-35a765fbe05d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Deployment manifest written in YAML, which defines how to deploy an application on a Kubernetes cluster. The deployment creates two replicas of an Nginx container, ensuring high availability. The deployment specifies that the containers should have resource limits, capping memory usage at 128Mi and CPU at 500m, to prevent resource exhaustion. The container will listen on port 80. Overall, this YAML configuration automates the deployment, scaling, and resource management of the Nginx application within a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx\\n        resources:\\n          limits:\\n            memory: \"128Mi\"\\n            cpu: \"500m\"\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Deployment manifest written in YAML, which defines how to deploy an application on a Kubernetes cluster. The deployment creates two replicas of an Nginx container, ensuring high availability. The deployment specifies that the containers should have resource limits, capping memory usage at 128Mi and CPU at 500m, to prevent resource exhaustion. The container will listen on port 80. Overall, this YAML configuration automates the deployment, scaling, and resource management of the Nginx application within a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-with-request.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5394d611-4a3e-51fe-a92e-b3a6c3deac57'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes YAML configuration defines a Pod named `constraints-cpu-demo-3` with a single container running the `nginx` image. The container has specified resource constraints, including a CPU request of 100 millicores (0.1 CPU) and a limit of 800 millicores (0.8 CPU). This setup enforces CPU resource allocation, ensuring the container has guaranteed CPU availability while preventing it from exceeding the specified limit, which helps manage resource utilization and maintain cluster stability.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo-3\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-3-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"800m\"\\n      requests:\\n        cpu: \"100m\"\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes YAML configuration defines a Pod named `constraints-cpu-demo-3` with a single container running the `nginx` image. The container has specified resource constraints, including a CPU request of 100 millicores (0.1 CPU) and a limit of 800 millicores (0.8 CPU). This setup enforces CPU resource allocation, ensuring the container has guaranteed CPU availability while preventing it from exceeding the specified limit, which helps manage resource utilization and maintain cluster stability.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('546a4c64-bc5e-5a2b-8c23-ddc98d483278'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes service configuration written in YAML. It defines a service named \"frontend\" that targets pods with specific labels (app: guestbook, tier: frontend). The service exposes port 80 to handle incoming network traffic. The configuration hints at the possibility of exposing the service externally by uncommenting the \"type: LoadBalancer\" line, which would provision an external load-balanced IP address if supported by the cluster. \\n\\nThis setup is part of a tutorial for deploying a Guestbook application on Google Kubernetes Engine, illustrating how to create a service that routes traffic to frontend pods. The YAML configuration automates the process of load balancing and service discovery within a Kubernetes cluster, ensuring that application components can communicate efficiently and are accessible over the network.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: frontend\\n  labels:\\n    app: guestbook\\n    tier: frontend\\nspec:\\n  # if your cluster supports it, uncomment the following to automatically create\\n  # an external load-balanced IP for the frontend service.\\n  # type: LoadBalancer\\n  #type: LoadBalancer\\n  ports:\\n    # the port that this service should serve on\\n  - port: 80\\n  selector:\\n    app: guestbook\\n    tier: frontend', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes service configuration written in YAML. It defines a service named \"frontend\" that targets pods with specific labels (app: guestbook, tier: frontend). The service exposes port 80 to handle incoming network traffic. The configuration hints at the possibility of exposing the service externally by uncommenting the \"type: LoadBalancer\" line, which would provision an external load-balanced IP address if supported by the cluster. \\n\\nThis setup is part of a tutorial for deploying a Guestbook application on Google Kubernetes Engine, illustrating how to create a service that routes traffic to frontend pods. The YAML configuration automates the process of load balancing and service discovery within a Kubernetes cluster, ensuring that application components can communicate efficiently and are accessible over the network.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5476d584-3f04-558e-9a5d-04e5fee73951'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes pod configuration in YAML format. It defines a pod named \"audit-pod\" with specific metadata, including labels and annotations for security profiles. The pod contains a single container using the Hashicorp http-echo image, configured to output a specific message (\"just made some syscalls!\") when started. Additionally, the container\\'s security context disallows privilege escalation, enhancing security. The annotation references a seccomp profile (localhost/profiles/audit.json), which is used for sandboxing system calls to improve security auditing and containment within the container. Overall, this configuration demonstrates how to set up a secure, basic pod with custom security settings and behavior.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: audit-pod\\n  labels:\\n    app: audit-pod\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/audit.json\\nspec:\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:0.2.3\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'chunk': '1/1', 'summary': 'This content is a Kubernetes pod configuration in YAML format. It defines a pod named \"audit-pod\" with specific metadata, including labels and annotations for security profiles. The pod contains a single container using the Hashicorp http-echo image, configured to output a specific message (\"just made some syscalls!\") when started. Additionally, the container\\'s security context disallows privilege escalation, enhancing security. The annotation references a seccomp profile (localhost/profiles/audit.json), which is used for sandboxing system calls to improve security auditing and containment within the container. Overall, this configuration demonstrates how to set up a secure, basic pod with custom security settings and behavior.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\audit-pod.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('55ce471a-86b4-502f-9261-12c985236302'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration that utilizes node affinity rules to determine on which nodes the pod can be scheduled. It includes both required and preferred scheduling constraints. The requiredDuringSchedulingIgnoredDuringExecution section enforces that the pod must run on nodes located in specific zones labeled as \"antarctica-east1\" or \"antarctica-west1\". The preferredDuringSchedulingIgnoredDuringExecution section indicates a slight preference for nodes with an additional label key \"another-node-label-key\" having the value \"another-node-label-value,\" but it is not mandatory. The pod contains a single container using the \"pause\" image, which is commonly used for testing or as a placeholder. Overall, this configuration enables precise placement of the pod based on node labels, balancing strict requirements with preferences to optimize scheduling.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: with-node-affinity\\nspec:\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:\\n        - matchExpressions:\\n          - key: topology.kubernetes.io/zone\\n            operator: In\\n            values:\\n            - antarctica-east1\\n            - antarctica-west1\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 1\\n        preference:\\n          matchExpressions:\\n          - key: another-node-label-key\\n            operator: In\\n            values:\\n            - another-node-label-value\\n  containers:\\n  - name: with-node-affinity\\n    image: registry.k8s.io/pause:3.8', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Pod configuration that utilizes node affinity rules to determine on which nodes the pod can be scheduled. It includes both required and preferred scheduling constraints. The requiredDuringSchedulingIgnoredDuringExecution section enforces that the pod must run on nodes located in specific zones labeled as \"antarctica-east1\" or \"antarctica-west1\". The preferredDuringSchedulingIgnoredDuringExecution section indicates a slight preference for nodes with an additional label key \"another-node-label-key\" having the value \"another-node-label-value,\" but it is not mandatory. The pod contains a single container using the \"pause\" image, which is commonly used for testing or as a placeholder. Overall, this configuration enables precise placement of the pod based on node labels, balancing strict requirements with preferences to optimize scheduling.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-node-affinity.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5761089b-40e8-5893-9b06-59ff50dcebe0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes NetworkPolicy resource written in YAML. It defines a policy named \"multi-port-egress\" in the default namespace, targeting pods labeled with \"role: db.\" The policy specifies an egress rule, allowing the selected pods to send outbound traffic to IP addresses within the 10.0.0.0/24 range. The rule limits this outbound traffic to TCP protocol on ports from 32000 to 32768, effectively controlling egress access by permitting only specific port ranges to external IPs within the designated CIDR block.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: multi-port-egress\\n  namespace: default\\nspec:\\n  podSelector:\\n    matchLabels:\\n      role: db\\n  policyTypes:\\n    - Egress\\n  egress:\\n    - to:\\n        - ipBlock:\\n            cidr: 10.0.0.0/24\\n      ports:\\n        - protocol: TCP\\n          port: 32000\\n          endPort: 32768\\n\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes NetworkPolicy resource written in YAML. It defines a policy named \"multi-port-egress\" in the default namespace, targeting pods labeled with \"role: db.\" The policy specifies an egress rule, allowing the selected pods to send outbound traffic to IP addresses within the 10.0.0.0/24 range. The rule limits this outbound traffic to TCP protocol on ports from 32000 to 32768, effectively controlling egress access by permitting only specific port ranges to external IPs within the designated CIDR block.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\networkpolicy-multiport-egress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('57f87fc8-d7dd-5375-9693-20fbeea9e662'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes StorageClass resource named \"portworx-io-priority-high\". It specifies the use of the Portworx volume provisioner (noted as deprecated), with parameters to configure the storage behavior. The parameters include setting the replica count to 1, a snapshot interval of 70 seconds, and high-priority input/output operations, which can improve performance for workloads requiring low latency. Overall, this configuration allows Kubernetes to dynamically provision persistent storage with specific performance and replication settings using Portworx.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: portworx-io-priority-high\\nprovisioner: kubernetes.io/portworx-volume # This provisioner is deprecated\\nparameters:\\n  repl: \"1\"\\n  snap_interval: \"70\"\\n  priority_io: \"high\"\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes StorageClass resource named \"portworx-io-priority-high\". It specifies the use of the Portworx volume provisioner (noted as deprecated), with parameters to configure the storage behavior. The parameters include setting the replica count to 1, a snapshot interval of 70 seconds, and high-priority input/output operations, which can improve performance for workloads requiring low latency. Overall, this configuration allows Kubernetes to dynamically provision persistent storage with specific performance and replication settings using Portworx.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-portworx-volume.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5876acb6-4739-5cc5-b672-37c305a372fe'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Deployment resource called \"my-nginx\" that manages three replicas of an Nginx web server container. The deployment uses the Nginx image version 1.14.2 and exposes port 80 within each container. The purpose of this configuration is to ensure high availability and scalability of the Nginx service by running multiple identical containers that can be managed and updated collectively by Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Deployment resource called \"my-nginx\" that manages three replicas of an Nginx web server container. The deployment uses the Nginx image version 1.14.2 and exposes port 80 within each container. The purpose of this configuration is to ensure high availability and scalability of the Nginx service by running multiple identical containers that can be managed and updated collectively by Kubernetes.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('59753e05-3bae-5165-b7c7-6b1c0ff496a3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes PriorityClass resource named \"placeholder,\" which is used to assign a negative priority value (-1000) to specific Pods. The purpose of this negative priority is to enable overprovisioning by ensuring these placeholder Pods have lower priority compared to other workload Pods. This allows the system to pre-allocate resources efficiently while maintaining the ability to preempt placeholder Pods if higher-priority Pods need resources. The PriorityClass is not set as the global default, meaning it applies only where explicitly specified.\\napiVersion: scheduling.k8s.io/v1\\nkind: PriorityClass\\nmetadata:\\n  name: placeholder # these Pods represent placeholder capacity\\nvalue: -1000\\nglobalDefault: false\\ndescription: \"Negative priority for placeholder pods to enable overprovisioning.\"', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes PriorityClass resource named \"placeholder,\" which is used to assign a negative priority value (-1000) to specific Pods. The purpose of this negative priority is to enable overprovisioning by ensuring these placeholder Pods have lower priority compared to other workload Pods. This allows the system to pre-allocate resources efficiently while maintaining the ability to preempt placeholder Pods if higher-priority Pods need resources. The PriorityClass is not set as the global default, meaning it applies only where explicitly specified.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priorityclass\\\\low-priority-class.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5991d3bc-e77e-5e38-9eec-4894856771a2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"env-configmap\" that includes a single container named \"app\" running the \"busybox:latest\" image. The container executes the command \"/bin/sh -c printenv\", which prints out all environment variables accessible within the container. The environment variables are sourced from a ConfigMap called \"myconfigmap\" using the envFrom field. This setup demonstrates how to inject environment variables into a container via a ConfigMap, enabling dynamic configuration management within Kubernetes pods.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: env-configmap\\nspec:\\n  containers:\\n    - name: app\\n      command: [\"/bin/sh\", \"-c\", \"printenv\"]\\n      image: busybox:latest\\n      envFrom:\\n        - configMapRef:\\n            name: myconfigmap\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"env-configmap\" that includes a single container named \"app\" running the \"busybox:latest\" image. The container executes the command \"/bin/sh -c printenv\", which prints out all environment variables accessible within the container. The environment variables are sourced from a ConfigMap called \"myconfigmap\" using the envFrom field. This setup demonstrates how to inject environment variables into a container via a ConfigMap, enabling dynamic configuration management within Kubernetes pods.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\env-configmap.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5a46768b-3c5b-58e0-af9a-82175b0a9467'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides Kubernetes YAML configurations for managing MySQL services within a cluster. It defines two services: a headless service named \"mysql\" and a client-facing service called \"mysql-read.\" The headless \"mysql\" service exposes MySQL instances for stable DNS entries using `clusterIP: None`, which is essential for StatefulSets to maintain persistent network identities of individual MySQL pods in a cluster. The second service, \"mysql-read,\" allows clients to connect to any MySQL instance for read operations, facilitating load distribution and redundancy. However, for write operations, clients need to connect directly to the primary instance, `mysql-0.mysql`, as specified in the comments.\\n\\nThe configurations ensure proper service discovery and load balancing in a Kubernetes environment, enabling scalable, resilient database deployments with clear separation between read and write traffic.\\n# Headless service for stable DNS entries of StatefulSet members.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: mysql\\n  labels:\\n    app: mysql\\n    app.kubernetes.io/name: mysql\\nspec:\\n  ports:\\n  - name: mysql\\n    port: 3306\\n  clusterIP: None\\n  selector:\\n    app: mysql\\n---\\n# Client service for connecting to any MySQL instance for reads.\\n# For writes, you must instead connect to the primary: mysql-0.mysql.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: mysql-read\\n  labels:\\n    app: mysql\\n    app.kubernetes.io/name: mysql\\n    readonly: \"true\"\\nspec:\\n  ports:\\n  - name: mysql\\n    port: 3306\\n  selector:\\n    app: mysql\\n', 'subchunk': '1/1', 'summary': 'This content provides Kubernetes YAML configurations for managing MySQL services within a cluster. It defines two services: a headless service named \"mysql\" and a client-facing service called \"mysql-read.\" The headless \"mysql\" service exposes MySQL instances for stable DNS entries using `clusterIP: None`, which is essential for StatefulSets to maintain persistent network identities of individual MySQL pods in a cluster. The second service, \"mysql-read,\" allows clients to connect to any MySQL instance for read operations, facilitating load distribution and redundancy. However, for write operations, clients need to connect directly to the primary instance, `mysql-0.mysql`, as specified in the comments.\\n\\nThe configurations ensure proper service discovery and load balancing in a Kubernetes environment, enabling scalable, resilient database deployments with clear separation between read and write traffic.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-services.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5ce64bff-2631-520d-8d27-327e407377b4'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration written in YAML. It defines a Pod named \"image-volume\" that contains a single container named \"shell,\" which uses the Debian image and executes the command `sleep infinity` to keep the container running indefinitely. A volume named \"volume\" is configured and mounted inside the container at the path `/volume`. \\n\\nAdditionally, the volume is specified to use an image from Quay.io (`quay.io/crio/artifact:v1`) with a pull policy set to `IfNotPresent`, which means the image will only be downloaded if it isn\\'t already available locally. This setup illustrates how Kubernetes allows mounting external images as volumes inside containers, facilitating shared data or specialized storage needs.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: image-volume\\nspec:\\n  containers:\\n  - name: shell\\n    command: [\"sleep\", \"infinity\"]\\n    image: debian\\n    volumeMounts:\\n    - name: volume\\n      mountPath: /volume\\n  volumes:\\n  - name: volume\\n    image:\\n      reference: quay.io/crio/artifact:v1\\n      pullPolicy: IfNotPresent\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration written in YAML. It defines a Pod named \"image-volume\" that contains a single container named \"shell,\" which uses the Debian image and executes the command `sleep infinity` to keep the container running indefinitely. A volume named \"volume\" is configured and mounted inside the container at the path `/volume`. \\n\\nAdditionally, the volume is specified to use an image from Quay.io (`quay.io/crio/artifact:v1`) with a pull policy set to `IfNotPresent`, which means the image will only be downloaded if it isn\\'t already available locally. This setup illustrates how Kubernetes allows mounting external images as volumes inside containers, facilitating shared data or specialized storage needs.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\image-volumes.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5e76665f-83f1-5003-b456-e4be476848c8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration using a YAML manifest. It specifies the API version, kind, metadata, and specifications for the pod. The key aspect is the use of a custom scheduler named \"my-scheduler,\" which overrides the default Kubernetes scheduler. The pod includes a single container based on the \"pause:3.8\" image, typically used as a placeholder for testing or scheduling. This configuration demonstrates how to assign a specific scheduler to a pod, allowing for customized scheduling strategies beyond the default behavior.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: annotation-second-scheduler\\n  labels:\\n    name: multischeduler-example\\nspec:\\n  schedulerName: my-scheduler\\n  containers:\\n  - name: pod-with-second-annotation-container\\n    image: registry.k8s.io/pause:3.8\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Pod configuration using a YAML manifest. It specifies the API version, kind, metadata, and specifications for the pod. The key aspect is the use of a custom scheduler named \"my-scheduler,\" which overrides the default Kubernetes scheduler. The pod includes a single container based on the \"pause:3.8\" image, typically used as a placeholder for testing or scheduling. This configuration demonstrates how to assign a specific scheduler to a pod, allowing for customized scheduling strategies beyond the default behavior.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5ed3e48f-de83-58e0-a53e-e476ab86b275'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Job that executes multiple parallel tasks with indexed completions. The Job is set to run five total completions, with three pods working simultaneously, and each task is identified by a unique `JOB_COMPLETION_INDEX`. The template specifies an `initContainer` that initializes data based on this index by selecting a specific item from a list and writing it to a shared volume. The main container then reads this data and performs a reverse operation (`rev` command) on the content of `/input/data.txt`. The volume mounted as `emptyDir` facilitates sharing data between the init container and the main worker container within each pod. Overall, this setup ensures each parallel task processes a distinct item from the list based on its index, demonstrating indexed parallelism in Kubernetes Jobs.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: \\'indexed-job\\'\\nspec:\\n  completions: 5\\n  parallelism: 3\\n  completionMode: Indexed\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      initContainers:\\n      - name: \\'input\\'\\n        image: \\'docker.io/library/bash\\'\\n        command:\\n        - \"bash\"\\n        - \"-c\"\\n        - |\\n          items=(foo bar baz qux xyz)\\n          echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt\\n        volumeMounts:\\n        - mountPath: /input\\n          name: input\\n      containers:\\n      - name: \\'worker\\'\\n        image: \\'docker.io/library/busybox\\'\\n        command:\\n        - \"rev\"\\n        - \"/input/data.txt\"\\n        volumeMounts:\\n        - mountPath: /input\\n          name: input\\n      volumes:\\n      - name: input\\n        emptyDir: {}\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Job that executes multiple parallel tasks with indexed completions. The Job is set to run five total completions, with three pods working simultaneously, and each task is identified by a unique `JOB_COMPLETION_INDEX`. The template specifies an `initContainer` that initializes data based on this index by selecting a specific item from a list and writing it to a shared volume. The main container then reads this data and performs a reverse operation (`rev` command) on the content of `/input/data.txt`. The volume mounted as `emptyDir` facilitates sharing data between the init container and the main worker container within each pod. Overall, this setup ensures each parallel task processes a distinct item from the list based on its index, demonstrating indexed parallelism in Kubernetes Jobs.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5f0adfa1-c028-59fa-9991-f99c199664b2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content defines a Kubernetes StorageClass configuration, which specifies how persistent storage should be provisioned dynamically within a cluster. This particular StorageClass, named \"ebs-sc,\" uses the Amazon Elastic Block Store (EBS) CSI driver (\"ebs.csi.aws.com\") to create storage volumes. It employs the \"WaitForFirstConsumer\" volume binding mode, ensuring that volume provisioning is deferred until a pod requests storage, enabling topology-aware provisioning. \\n\\nThe parameters detail the EBS volume specifications, including the filesystem type (XFS), volume type (io1), and IOPS configuration (50 IOPS per GB), with encryption enabled. Additional tags are specified for resource identification and management. The \"allowedTopologies\" section restricts volume provisioning to a specific AWS zone (\"us-east-2c\"). Overall, this configuration enables precise control over EBS volume creation in Kubernetes, aligning storage resources with workload placement and performance requirements.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: ebs-sc\\nprovisioner: ebs.csi.aws.com\\nvolumeBindingMode: WaitForFirstConsumer\\nparameters:\\n  csi.storage.k8s.io/fstype: xfs\\n  type: io1\\n  iopsPerGB: \"50\"\\n  encrypted: \"true\"\\n  tagSpecification_1: \"key1=value1\"\\n  tagSpecification_2: \"key2=value2\"\\nallowedTopologies:\\n- matchLabelExpressions:\\n  - key: topology.ebs.csi.aws.com/zone\\n    values:\\n    - us-east-2c\\n', 'subchunk': '1/1', 'summary': 'The provided content defines a Kubernetes StorageClass configuration, which specifies how persistent storage should be provisioned dynamically within a cluster. This particular StorageClass, named \"ebs-sc,\" uses the Amazon Elastic Block Store (EBS) CSI driver (\"ebs.csi.aws.com\") to create storage volumes. It employs the \"WaitForFirstConsumer\" volume binding mode, ensuring that volume provisioning is deferred until a pod requests storage, enabling topology-aware provisioning. \\n\\nThe parameters detail the EBS volume specifications, including the filesystem type (XFS), volume type (io1), and IOPS configuration (50 IOPS per GB), with encryption enabled. Additional tags are specified for resource identification and management. The \"allowedTopologies\" section restricts volume provisioning to a specific AWS zone (\"us-east-2c\"). Overall, this configuration enables precise control over EBS volume creation in Kubernetes, aligning storage resources with workload placement and performance requirements.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-aws-ebs.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5f2c1b1e-710f-503c-8124-80a3d5bc6844'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Job configuration in YAML format. The job is named \"process-item-$ITEM\" and is labeled under the \"jobgroup: jobexample\" for organizational purposes. The Job\\'s pod template specifies a container named \"c\" that uses the lightweight \"busybox:1.28\" image. The container\\'s command executes a shell script that outputs \"Processing item $ITEM\" and then pauses for 5 seconds, simulating a processing task. The restart policy is set to \"Never,\" meaning the pod won\\'t restart automatically after completion or failure. Overall, this configuration enables the execution of a simple, one-time processing job with dynamic item processing indicated by the variable `$ITEM`.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: process-item-$ITEM\\n  labels:\\n    jobgroup: jobexample\\nspec:\\n  template:\\n    metadata:\\n      name: jobexample\\n      labels:\\n        jobgroup: jobexample\\n    spec:\\n      containers:\\n      - name: c\\n        image: busybox:1.28\\n        command: [\"sh\", \"-c\", \"echo Processing item $ITEM && sleep 5\"]\\n      restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Job configuration in YAML format. The job is named \"process-item-$ITEM\" and is labeled under the \"jobgroup: jobexample\" for organizational purposes. The Job\\'s pod template specifies a container named \"c\" that uses the lightweight \"busybox:1.28\" image. The container\\'s command executes a shell script that outputs \"Processing item $ITEM\" and then pauses for 5 seconds, simulating a processing task. The restart policy is set to \"Never,\" meaning the pod won\\'t restart automatically after completion or failure. Overall, this configuration enables the execution of a simple, one-time processing job with dynamic item processing indicated by the variable `$ITEM`.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-tmpl.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('5fe6a087-c135-5155-872b-df9cf5c29eb8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML that demonstrates how to use a ConfigMap to inject configuration data into a container. The Pod, named \"dapi-test-pod,\" runs a single container based on the BusyBox image. The container executes a command to list the contents of the \"/etc/config\" directory. The configuration mounts a volume called \"config-volume\" to this directory, and this volume is linked to a ConfigMap named \"special-config.\" When the Pod runs, the ConfigMap\\'s data will be available inside the container at \"/etc/config,\" allowing applications or scripts to access configuration files or data stored within the ConfigMap. The restart policy is set to \"Never,\" meaning the Pod will not automatically restart if it terminates. Overall, this setup illustrates how to provision configuration data into a container dynamically using ConfigMaps in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ]\\n      volumeMounts:\\n      - name: config-volume\\n        mountPath: /etc/config\\n  volumes:\\n    - name: config-volume\\n      configMap:\\n        # Provide the name of the ConfigMap containing the files you want\\n        # to add to the container\\n        name: special-config\\n  restartPolicy: Never\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML that demonstrates how to use a ConfigMap to inject configuration data into a container. The Pod, named \"dapi-test-pod,\" runs a single container based on the BusyBox image. The container executes a command to list the contents of the \"/etc/config\" directory. The configuration mounts a volume called \"config-volume\" to this directory, and this volume is linked to a ConfigMap named \"special-config.\" When the Pod runs, the ConfigMap\\'s data will be available inside the container at \"/etc/config,\" allowing applications or scripts to access configuration files or data stored within the ConfigMap. The restart policy is set to \"Never,\" meaning the Pod will not automatically restart if it terminates. Overall, this setup illustrates how to provision configuration data into a container dynamically using ConfigMaps in Kubernetes.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-volume.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('60226e59-d91d-5ab9-a369-214fd522a549'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes DaemonSet, which ensures that a copy of a specific pod runs on all suitable nodes in the cluster. The DaemonSet is named \"my-daemonset\" and is labeled with \"app: foo\" for identification. It specifies a selector to match pods with this label and uses a pod template that includes a container running the \"microsoft/windowsservercore:1709\" image. Additionally, the nodeSelector is used to schedule the pods only on Windows nodes, identified by the label \"kubernetes.io/os: windows.\" This setup is useful for deploying system-level services, agents, or monitoring tools uniformly across all Windows-based nodes in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: my-daemonset\\n  labels:\\n    app: foo\\nspec:\\n  selector:\\n    matchLabels:\\n      app: foo\\n  template:\\n    metadata:\\n      labels:\\n        app: foo\\n    spec:\\n      containers:\\n      - name: foo\\n        image: microsoft/windowsservercore:1709\\n      nodeSelector:\\n        kubernetes.io/os: windows\\n\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes DaemonSet, which ensures that a copy of a specific pod runs on all suitable nodes in the cluster. The DaemonSet is named \"my-daemonset\" and is labeled with \"app: foo\" for identification. It specifies a selector to match pods with this label and uses a pod template that includes a container running the \"microsoft/windowsservercore:1709\" image. Additionally, the nodeSelector is used to schedule the pods only on Windows nodes, identified by the label \"kubernetes.io/os: windows.\" This setup is useful for deploying system-level services, agents, or monitoring tools uniformly across all Windows-based nodes in a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\daemonset.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('60acf62f-0d01-56bc-ae58-f5337f98bea2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with the name \"nginx.\" The Pod includes a container running the official nginx image, with the image pull policy set to only pull the image if it is not already present on the node. Additionally, the Pod is labeled with \"env: test\" for environment identification. A node selector is specified to ensure the Pod is scheduled only on nodes that have a label \"disktype\" set to \"ssd,\" which helps control the placement of the Pod based on node characteristics.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\n  labels:\\n    env: test\\nspec:\\n  containers:\\n  - name: nginx\\n    image: nginx\\n    imagePullPolicy: IfNotPresent\\n  nodeSelector:\\n    disktype: ssd\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod with the name \"nginx.\" The Pod includes a container running the official nginx image, with the image pull policy set to only pull the image if it is not already present on the node. Additionally, the Pod is labeled with \"env: test\" for environment identification. A node selector is specified to ensure the Pod is scheduled only on nodes that have a label \"disktype\" set to \"ssd,\" which helps control the placement of the Pod based on node characteristics.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('60ed3ecc-f397-598e-8746-c3b8f16f2dc0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration manifest that describes the deployment of a \"konnectivity-server\" in the \"kube-system\" namespace. This server runs as a containerized application using the image `registry.k8s.io/kas-network-proxy/proxy-server:v0.0.37`, configured to operate with host networking. The Pod includes a command and multiple arguments focused on setting up the Konnectivity server, which facilitates secure communication between the Kubernetes API server and nodes, particularly under network constraints or constraints like Firewall/NAT.\\n\\nThe container\\'s configuration includes a liveness probe to monitor health via a local HTTP GET request to `/healthz` on port 8134, ensuring the server remains responsive. Several ports are exposed and mapped between the container and host, specifically for agent, admin, and health functionalities. Volumes are mounted to provide necessary certificates, configuration files, and support Unix domain sockets, all tied to specific paths on the host for security and accessibility.\\n\\nThe overall setup ensures that the Konnectivity server is correctly configured with SSL certificates, proper ports, and operational parameters for secure and reliable communication within a Kubernetes cluster. This configuration is crucial for Kubernetes clusters where network policies or firewalls limit direct communications, enabling a secure proxy-based communication channel.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: konnectivity-server\\n  namespace: kube-system\\nspec:\\n  priorityClassName: system-cluster-critical\\n  hostNetwork: true\\n  containers:\\n  - name: konnectivity-server-container\\n    image: registry.k8s.io/kas-network-proxy/proxy-server:v0.0.37\\n    command: [\"/proxy-server\"]\\n    args: [\\n            \"--logtostderr=true\",\\n            # This needs to be consistent with the value set in egressSelectorConfiguration.\\n            \"--uds-name=/etc/kubernetes/konnectivity-server/konnectivity-server.socket\",\\n            \"--delete-existing-uds-file\",\\n            # The following two lines assume the Konnectivity server is\\n            # deployed on the same machine as the apiserver, and the certs and\\n            # key of the API Server are at the specified location.\\n            \"--cluster-cert=/etc/kubernetes/pki/apiserver.crt\",\\n            \"--cluster-key=/etc/kubernetes/pki/apiserver.key\",\\n            # This needs to be consistent with the value set in egressSelectorConfiguration.\\n            \"--mode=grpc\",\\n            \"--server-port=0\",\\n            \"--agent-port=8132\",\\n            \"--admin-port=8133\",\\n            \"--health-port=8134\",\\n            \"--agent-namespace=kube-system\",\\n            \"--agent-service-account=konnectivity-agent\",\\n            \"--kubeconfig=/etc/kubernetes/konnectivity-server.conf\",\\n            \"--authentication-audience=system:konnectivity-server\"\\n            ]\\n    livenessProbe:\\n      httpGet:\\n        scheme: HTTP\\n        host: 127.0.0.1\\n        port: 8134\\n        path: /healthz\\n      initialDelaySeconds: 30\\n      timeoutSeconds: 60\\n    ports:\\n    - name: agentport\\n      containerPort: 8132\\n      hostPort: 8132\\n    - name: adminport\\n      containerPort: 8133\\n      hostPort: 8133\\n    - name: healthport\\n      containerPort: 8134\\n      hostPort: 8134\\n    volumeMounts:\\n    - name: k8s-certs\\n      mountPath: /etc/kubernetes/pki\\n      readOnly: true\\n    - name: kubeconfig\\n      mountPath: /etc/kubernetes/konnectivity-server.conf\\n      readOnly: true\\n    - name: konnectivity-uds\\n      mountPath: /etc/kubernetes/konnectivity-server\\n      readOnly: false\\n  volumes:\\n  - name: k8s-certs\\n    hostPath:\\n      path: /etc/kubernetes/pki\\n  - name: kubeconfig\\n    hostPath:\\n      path: /etc/kubernetes/konnectivity-server.conf\\n      type: FileOrCreate\\n  - name: konnectivity-uds\\n    hostPath:\\n      path: /etc/kubernetes/konnectivity-server\\n      type: DirectoryOrCreate\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-server.yaml', 'summary': 'The provided content is a Kubernetes Pod configuration manifest that describes the deployment of a \"konnectivity-server\" in the \"kube-system\" namespace. This server runs as a containerized application using the image `registry.k8s.io/kas-network-proxy/proxy-server:v0.0.37`, configured to operate with host networking. The Pod includes a command and multiple arguments focused on setting up the Konnectivity server, which facilitates secure communication between the Kubernetes API server and nodes, particularly under network constraints or constraints like Firewall/NAT.\\n\\nThe container\\'s configuration includes a liveness probe to monitor health via a local HTTP GET request to `/healthz` on port 8134, ensuring the server remains responsive. Several ports are exposed and mapped between the container and host, specifically for agent, admin, and health functionalities. Volumes are mounted to provide necessary certificates, configuration files, and support Unix domain sockets, all tied to specific paths on the host for security and accessibility.\\n\\nThe overall setup ensures that the Konnectivity server is correctly configured with SSL certificates, proper ports, and operational parameters for secure and reliable communication within a Kubernetes cluster. This configuration is crucial for Kubernetes clusters where network policies or firewalls limit direct communications, enabling a secure proxy-based communication channel.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('62fd7d1e-1cd1-51a6-9676-bfd2cb5a41e6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ValidatingAdmissionPolicy in YAML format, which is a resource used to enforce custom validation rules during the creation or update of Kubernetes resources. The policy is named \"demo-policy.example.com\" and specifies a failure policy set to \"Fail,\" meaning invalid requests will be rejected. It targets \"deployments\" resources within the \"apps\" API group for CREATE and UPDATE operations. The validation rule uses an expression to ensure that the number of replicas specified in the deployment does not exceed five. This policy helps enforce resource constraints and maintain consistency within the cluster by validating deployment specifications before they are applied.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"demo-policy.example.com\"\\nspec:\\n  failurePolicy: Fail\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  validations:\\n    - expression: \"object.spec.replicas <= 5\"', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ValidatingAdmissionPolicy in YAML format, which is a resource used to enforce custom validation rules during the creation or update of Kubernetes resources. The policy is named \"demo-policy.example.com\" and specifies a failure policy set to \"Fail,\" meaning invalid requests will be rejected. It targets \"deployments\" resources within the \"apps\" API group for CREATE and UPDATE operations. The validation rule uses an expression to ensure that the number of replicas specified in the deployment does not exceed five. This policy helps enforce resource constraints and maintain consistency within the cluster by validating deployment specifications before they are applied.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\basic-example-policy.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('635445c4-9318-5471-a13b-47dbdf52d422'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML file describes a Kubernetes deployment configuration for running an Nginx web server. It specifies that three replicas of the Nginx container should be created to ensure high availability and load balancing. The deployment uses the `nginx:1.14.2` Docker image, and the labels help Kubernetes identify and manage the pods associated with this deployment. This setup ensures consistent, scalable deployment of the Nginx server within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n', 'chunk': '1/1', 'summary': 'This YAML file describes a Kubernetes deployment configuration for running an Nginx web server. It specifies that three replicas of the Nginx container should be created to ensure high availability and load balancing. The deployment uses the `nginx:1.14.2` Docker image, and the labels help Kubernetes identify and manage the pods associated with this deployment. This setup ensures consistent, scalable deployment of the Nginx server within a Kubernetes cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6362cdfe-90f6-5339-a21c-e11c01bcc7a9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content describes a Kubernetes deployment setup for a WordPress application, including a Service, PersistentVolumeClaim, and Deployment configuration. The Service exposes the WordPress frontend on port 80 and uses a LoadBalancer to facilitate external access. The PersistentVolumeClaim requests 20 GiB of persistent storage to retain the website data. The Deployment manages the WordPress pod, which runs the official WordPress Docker image with Apache, and sets environment variables to connect to a MySQL database securely by referencing a secret. It also mounts the persistent storage volume to ensure data persistence across pod restarts. This configuration together deploys a scalable, persistent WordPress environment in Kubernetes, with external accessibility and database connectivity managed through environment variables and secrets.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: wordpress\\n  labels:\\n    app: wordpress\\nspec:\\n  ports:\\n    - port: 80\\n  selector:\\n    app: wordpress\\n    tier: frontend\\n  type: LoadBalancer\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: wp-pv-claim\\n  labels:\\n    app: wordpress\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 20Gi\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: wordpress\\n  labels:\\n    app: wordpress\\nspec:\\n  selector:\\n    matchLabels:\\n      app: wordpress\\n      tier: frontend\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      labels:\\n        app: wordpress\\n        tier: frontend\\n    spec:\\n      containers:\\n      - image: wordpress:6.2.1-apache\\n        name: wordpress\\n        env:\\n        - name: WORDPRESS_DB_HOST\\n          value: wordpress-mysql\\n        - name: WORDPRESS_DB_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: mysql-pass\\n              key: password\\n        - name: WORDPRESS_DB_USER\\n          value: wordpress\\n        ports:\\n        - containerPort: 80\\n          name: wordpress\\n        volumeMounts:\\n        - name: wordpress-persistent-storage\\n          mountPath: /var/www/html\\n      volumes:\\n      - name: wordpress-persistent-storage\\n        persistentVolumeClaim:\\n          claimName: wp-pv-claim\\n', 'subchunk': '1/1', 'summary': 'The provided content describes a Kubernetes deployment setup for a WordPress application, including a Service, PersistentVolumeClaim, and Deployment configuration. The Service exposes the WordPress frontend on port 80 and uses a LoadBalancer to facilitate external access. The PersistentVolumeClaim requests 20 GiB of persistent storage to retain the website data. The Deployment manages the WordPress pod, which runs the official WordPress Docker image with Apache, and sets environment variables to connect to a MySQL database securely by referencing a secret. It also mounts the persistent storage volume to ensure data persistence across pod restarts. This configuration together deploys a scalable, persistent WordPress environment in Kubernetes, with external accessibility and database connectivity managed through environment variables and secrets.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\wordpress-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('64e6f5f0-2b4a-5b99-8e7d-ebfa74694912'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ClusterRole resource called `system:kube-scheduler`, which establishes specific permissions required for the kube-scheduler component in a Kubernetes cluster. It includes metadata with annotations and labels for identification and automatic updates. The role specifies a set of rules granting permissions on various resources within the cluster, particularly related to `leases` in the `coordination.k8s.io` API group and `endpoints` in the core API group. For `leases`, it permits creating, getting, updating, and for certain named leases (`kube-scheduler` and `my-scheduler`), it allows retrieving and updating them. For `endpoints`, it permits deleting, getting, patching, and updating these resources, also limited to specific resource names. This role is integral to controlling the scheduler\\'s ability to manage lease objects and endpoints, ensuring proper coordination and operation within the cluster.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  annotations:\\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\\n  labels:\\n    kubernetes.io/bootstrapping: rbac-defaults\\n  name: system:kube-scheduler\\nrules:\\n  - apiGroups:\\n      - coordination.k8s.io\\n    resources:\\n      - leases\\n    verbs:\\n      - create\\n  - apiGroups:\\n      - coordination.k8s.io\\n    resourceNames:\\n      - kube-scheduler\\n      - my-scheduler\\n    resources:\\n      - leases\\n    verbs:\\n      - get\\n      - update\\n  - apiGroups:\\n      - \"\"\\n    resourceNames:\\n      - kube-scheduler\\n      - my-scheduler\\n    resources:\\n      - endpoints\\n    verbs:\\n      - delete\\n      - get\\n      - patch\\n      - update\\n', 'chunk': '1/1', 'summary': \"This content defines a Kubernetes ClusterRole resource called `system:kube-scheduler`, which establishes specific permissions required for the kube-scheduler component in a Kubernetes cluster. It includes metadata with annotations and labels for identification and automatic updates. The role specifies a set of rules granting permissions on various resources within the cluster, particularly related to `leases` in the `coordination.k8s.io` API group and `endpoints` in the core API group. For `leases`, it permits creating, getting, updating, and for certain named leases (`kube-scheduler` and `my-scheduler`), it allows retrieving and updating them. For `endpoints`, it permits deleting, getting, patching, and updating these resources, also limited to specific resource names. This role is integral to controlling the scheduler's ability to manage lease objects and endpoints, ensuring proper coordination and operation within the cluster.\", 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\clusterrole.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('664d4f4d-6350-5fee-b7f9-9c85d4ac35ff'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ClusterRole named \"secret-reader\" using YAML syntax. It grants permissions at the cluster level to read secret resources. Specifically, it allows actions such as \"get,\" \"watch,\" and \"list\" on secrets, which are necessary for monitoring or accessing secret data within the cluster. Since ClusterRoles are not namespace-specific, there is no namespace specified in the metadata. Overall, this configuration is used to create a role with read-only access to secrets across the entire Kubernetes cluster, enabling controlled access management for various users or service accounts.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  # \"namespace\" omitted since ClusterRoles are not namespaced\\n  name: secret-reader\\nrules:\\n- apiGroups: [\"\"]\\n  #\\n  # at the HTTP level, the name of the resource for accessing Secret\\n  # objects is \"secrets\"\\n  resources: [\"secrets\"]\\n  verbs: [\"get\", \"watch\", \"list\"]\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes ClusterRole named \"secret-reader\" using YAML syntax. It grants permissions at the cluster level to read secret resources. Specifically, it allows actions such as \"get,\" \"watch,\" and \"list\" on secrets, which are necessary for monitoring or accessing secret data within the cluster. Since ClusterRoles are not namespace-specific, there is no namespace specified in the metadata. Overall, this configuration is used to create a role with read-only access to secrets across the entire Kubernetes cluster, enabling controlled access management for various users or service accounts.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-clusterrole.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('66ced0b3-cd0c-5d5c-af21-7894052b0fb2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Service named \"my-nginx-svc\" that exposes an nginx application. The service is set to use the \"LoadBalancer\" type, which provisions an external load balancer to route traffic to the application. It listens on port 80 and uses a label selector to target pods with the label \"app: nginx\". This setup enables external access to the nginx instances through the cloud provider\\'s load balancing infrastructure, providing scalability and high availability.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-nginx-svc\\n  labels:\\n    app: nginx\\nspec:\\n  type: LoadBalancer\\n  ports:\\n  - port: 80\\n  selector:\\n    app: nginx\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Service named \"my-nginx-svc\" that exposes an nginx application. The service is set to use the \"LoadBalancer\" type, which provisions an external load balancer to route traffic to the application. It listens on port 80 and uses a label selector to target pods with the label \"app: nginx\". This setup enables external access to the nginx instances through the cloud provider\\'s load balancing infrastructure, providing scalability and high availability.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-svc.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('66db38c8-8234-5479-b40a-42e928ff1991'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Ingress resource, which manages external access to services within a Kubernetes cluster. The YAML configuration specifies the use of the \"nginx\" ingress class and sets up routing rules for incoming HTTP traffic aimed at the host \"hello-world.example.\" All requests to the root path (\"/\") are directed to the \"web\" service on port 8080. This setup enables external clients to access the internal \"web\" service through a simplified and centralized ingress point.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: example-ingress\\nspec:\\n  ingressClassName: nginx\\n  rules:\\n    - host: hello-world.example\\n      http:\\n        paths:\\n          - path: /\\n            pathType: Prefix\\n            backend:\\n              service:\\n                name: web\\n                port:\\n                  number: 8080', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Ingress resource, which manages external access to services within a Kubernetes cluster. The YAML configuration specifies the use of the \"nginx\" ingress class and sets up routing rules for incoming HTTP traffic aimed at the host \"hello-world.example.\" All requests to the root path (\"/\") are directed to the \"web\" service on port 8080. This setup enables external clients to access the internal \"web\" service through a simplified and centralized ingress point.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\example-ingress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('67a305cb-1da4-5990-85de-05dd2e6db3be'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Service configuration in YAML format. It defines a service named \"my-nginx\" that exposes port 80 using the TCP protocol. The service uses a label selector to identify the pods it manages, specifically those labeled with \"run: my-nginx.\" This configuration allows external access to the associated pods, enabling load balancing and network traffic routing to ensure the nginx application runs smoothly within the Kubernetes cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-nginx\\n  labels:\\n    run: my-nginx\\nspec:\\n  ports:\\n  - port: 80\\n    protocol: TCP\\n  selector:\\n    run: my-nginx\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Service configuration in YAML format. It defines a service named \"my-nginx\" that exposes port 80 using the TCP protocol. The service uses a label selector to identify the pods it manages, specifically those labeled with \"run: my-nginx.\" This configuration allows external access to the associated pods, enabling load balancing and network traffic routing to ensure the nginx application runs smoothly within the Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-svc.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('67eb694e-a93b-54af-a709-366d8794c28e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes ResourceQuota configuration, which is used to set limits on resource consumption within a namespace. It specifies a quota named \"object-quota-demo\" with restrictions on certain resources. Specifically, it limits the number of persistent volume claims to 1, the number of load balancer services to 2, and the number of node port services to 0. This configuration helps manage and control resource usage in a Kubernetes cluster, ensuring that no single namespace consumes excessive resources.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: object-quota-demo\\nspec:\\n  hard:\\n    persistentvolumeclaims: \"1\"\\n    services.loadbalancers: \"2\"\\n    services.nodeports: \"0\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects.yaml', 'summary': 'The provided content is a Kubernetes ResourceQuota configuration, which is used to set limits on resource consumption within a namespace. It specifies a quota named \"object-quota-demo\" with restrictions on certain resources. Specifically, it limits the number of persistent volume claims to 1, the number of load balancer services to 2, and the number of node port services to 0. This configuration helps manage and control resource usage in a Kubernetes cluster, ensuring that no single namespace consumes excessive resources.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('68317efb-1775-5391-ba9b-34ac2fff4666'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod manifest configured to demonstrate resource management. It defines a Pod named \"memory-demo\" within the \"pod-resources-example\" namespace, specifying memory requests and limits of 100Mi and 200Mi, respectively. The Pod contains a single container running the nginx image, but it executes the \"stress\" command with arguments to allocate 150MB of virtual memory. This setup is used to simulate memory consumption and test how Kubernetes manages resource allocation and limits for the container. Overall, it demonstrates resource requests, limits, and how to run a container with a custom command to stress-test memory resources.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: memory-demo\\n  namespace: pod-resources-example\\nspec:\\n  resources:\\n    requests:\\n      memory: \"100Mi\"\\n    limits:\\n      memory: \"200Mi\"\\n  containers:\\n  - name: memory-demo-ctr\\n    image: nginx\\n    command: [\"stress\"]\\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"]\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod manifest configured to demonstrate resource management. It defines a Pod named \"memory-demo\" within the \"pod-resources-example\" namespace, specifying memory requests and limits of 100Mi and 200Mi, respectively. The Pod contains a single container running the nginx image, but it executes the \"stress\" command with arguments to allocate 150MB of virtual memory. This setup is used to simulate memory consumption and test how Kubernetes manages resource allocation and limits for the container. Overall, it demonstrates resource requests, limits, and how to run a container with a custom command to stress-test memory resources.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-memory-request-limit.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('68d852fa-f8af-5724-8743-66d26d02ebbe'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Pod configuration with node affinity rules that influence pod scheduling to nodes based on specific criteria. The configuration includes both required and preferred node affinity settings: the required rule ensures the pod runs only on nodes with the label `kubernetes.io/os` set to `linux`, while the preferred rules specify additional node preferences with varying weights, favoring nodes labeled `label-2` over `label-1`. The pod contains a single container using the `pause:3.8` image, which is typically used for testing or as a placeholder. Overall, this configuration demonstrates how to control pod placement through node affinity, balancing strict requirements with preferred node labels to optimize scheduling based on label preferences and weights.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: with-affinity-preferred-weight\\nspec:\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:\\n        - matchExpressions:\\n          - key: kubernetes.io/os\\n            operator: In\\n            values:\\n            - linux\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 1\\n        preference:\\n          matchExpressions:\\n          - key: label-1\\n            operator: In\\n            values:\\n            - key-1\\n      - weight: 50\\n        preference:\\n          matchExpressions:\\n          - key: label-2\\n            operator: In\\n            values:\\n            - key-2\\n  containers:\\n  - name: with-node-affinity\\n    image: registry.k8s.io/pause:3.8\\n', 'subchunk': '1/1', 'summary': 'This content describes a Kubernetes Pod configuration with node affinity rules that influence pod scheduling to nodes based on specific criteria. The configuration includes both required and preferred node affinity settings: the required rule ensures the pod runs only on nodes with the label `kubernetes.io/os` set to `linux`, while the preferred rules specify additional node preferences with varying weights, favoring nodes labeled `label-2` over `label-1`. The pod contains a single container using the `pause:3.8` image, which is typically used for testing or as a placeholder. Overall, this configuration demonstrates how to control pod placement through node affinity, balancing strict requirements with preferred node labels to optimize scheduling based on label preferences and weights.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-affinity-preferred-weight.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6a09bdd6-9ded-5be3-9bc2-778324671d3e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Pod named `constraints-mem-demo` that runs an Nginx container. The configuration specifies resource management parameters: the container requests a minimum of 600MiB of memory, ensuring the scheduler allocates resources accordingly, and is limited to a maximum of 800MiB of memory, preventing it from consuming too many resources. This setup helps maintain resource efficiency and stability within the cluster by controlling the container\\'s memory usage.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-mem-demo\\nspec:\\n  containers:\\n  - name: constraints-mem-demo-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"800Mi\"\\n      requests:\\n        memory: \"600Mi\"\\n', 'subchunk': '1/1', 'summary': \"This Kubernetes configuration defines a Pod named `constraints-mem-demo` that runs an Nginx container. The configuration specifies resource management parameters: the container requests a minimum of 600MiB of memory, ensuring the scheduler allocates resources accordingly, and is limited to a maximum of 800MiB of memory, preventing it from consuming too many resources. This setup helps maintain resource efficiency and stability within the cluster by controlling the container's memory usage.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6a9358cd-e81e-5ec8-98b8-00f959677497'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes manifest that defines a secret of type `kubernetes.io/basic-auth`. It contains sensitive authentication data, specifically a username and password, which are stored securely within the cluster. The secret is named `secret-basic-auth`, and it includes the credentials `admin` as the username and `t0p-Secret` as the password. This secret can be used to manage basic HTTP authentication for applications running within the Kubernetes environment, enabling secure access to services that require such credentials.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret-basic-auth\\ntype: kubernetes.io/basic-auth\\nstringData:\\n  username: admin # required field for kubernetes.io/basic-auth\\n  password: t0p-Secret # required field for kubernetes.io/basic-auth', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\basicauth-secret.yaml', 'summary': 'This content is a Kubernetes manifest that defines a secret of type `kubernetes.io/basic-auth`. It contains sensitive authentication data, specifically a username and password, which are stored securely within the cluster. The secret is named `secret-basic-auth`, and it includes the credentials `admin` as the username and `t0p-Secret` as the password. This secret can be used to manage basic HTTP authentication for applications running within the Kubernetes environment, enabling secure access to services that require such credentials.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6ab21065-7293-5abd-ac89-59649f2dfbbb'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes LimitRange resource, which sets default resource constraints at the namespace level. Specifically, it specifies memory limits for containers, setting a default maximum of 512Mi (mebibytes) and a default request of 256Mi. This ensures that containers in the namespace do not exceed these memory boundaries unless explicitly overridden, helping manage resource allocation and prevent resource contention within the cluster.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: mem-limit-range\\nspec:\\n  limits:\\n  - default:\\n      memory: 512Mi\\n    defaultRequest:\\n      memory: 256Mi\\n    type: Container\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes LimitRange resource, which sets default resource constraints at the namespace level. Specifically, it specifies memory limits for containers, setting a default maximum of 512Mi (mebibytes) and a default request of 256Mi. This ensures that containers in the namespace do not exceed these memory boundaries unless explicitly overridden, helping manage resource allocation and prevent resource contention within the cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6c3a77fe-33d1-596b-8bc2-6dd99c4ea096'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a Kubernetes Deployment configuration for a frontend application, specifically a guestbook app. It defines a deployment named \"frontend\" that manages three replicas of a containerized PHP-Redis application. The deployment uses labels to identify its components and specifies a pod template that includes a container running a specific image hosted in Google\\'s container registry. The container environment variable `GET_HOSTS_FROM` is set to \"dns,\" indicating that hostname resolution is handled via DNS. Resource requests allocate 100 millicpus and 100Mi of memory per container to ensure resource management. The container exposes port 80 for HTTP traffic. This configuration enables scalable, manageable deployment of a web frontend in a Kubernetes cluster.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n        app: guestbook\\n        tier: frontend\\n  template:\\n    metadata:\\n      labels:\\n        app: guestbook\\n        tier: frontend\\n    spec:\\n      containers:\\n      - name: php-redis\\n        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\\n        env:\\n        - name: GET_HOSTS_FROM\\n          value: \"dns\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'This content provides a Kubernetes Deployment configuration for a frontend application, specifically a guestbook app. It defines a deployment named \"frontend\" that manages three replicas of a containerized PHP-Redis application. The deployment uses labels to identify its components and specifies a pod template that includes a container running a specific image hosted in Google\\'s container registry. The container environment variable `GET_HOSTS_FROM` is set to \"dns,\" indicating that hostname resolution is handled via DNS. Resource requests allocate 100 millicpus and 100Mi of memory per container to ensure resource management. The container exposes port 80 for HTTP traffic. This configuration enables scalable, manageable deployment of a web frontend in a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6c764e81-5b58-5e30-b57f-a823ac97d2dd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes YAML manifest defines a NetworkPolicy named \"default-deny-egress.\" Its purpose is to control the egress traffic from pods within the cluster. The \"podSelector: {}\" indicates that the policy applies to all pods, as it does not specify any label selectors. The \"policyTypes\" field set to \"Egress\" signifies that this policy is focused on controlling outbound network traffic from the selected pods.\\n\\nBy default, Kubernetes policies are permissive, but this configuration, being a \"default-deny\" policy for egress, will block all outbound connections from all pods unless additional rules are specified to allow specific traffic. This setup is essential for enhancing security by restricting how pods communicate externally, and it serves as a foundational step for implementing more granular network policies in a Kubernetes environment.\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: default-deny-egress\\nspec:\\n  podSelector: {}\\n  policyTypes:\\n  - Egress\\n', 'chunk': '1/1', 'summary': 'This Kubernetes YAML manifest defines a NetworkPolicy named \"default-deny-egress.\" Its purpose is to control the egress traffic from pods within the cluster. The \"podSelector: {}\" indicates that the policy applies to all pods, as it does not specify any label selectors. The \"policyTypes\" field set to \"Egress\" signifies that this policy is focused on controlling outbound network traffic from the selected pods.\\n\\nBy default, Kubernetes policies are permissive, but this configuration, being a \"default-deny\" policy for egress, will block all outbound connections from all pods unless additional rules are specified to allow specific traffic. This setup is essential for enhancing security by restricting how pods communicate externally, and it serves as a foundational step for implementing more granular network policies in a Kubernetes environment.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-egress.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6c821245-eb60-50ad-a900-0bbc66bf7d1e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes LimitRange resource called `limit-memory-ratio-pod`. It sets a constraint on the memory request and limit ratios for pods in a namespace. Specifically, it enforces that the maximum ratio of memory limit to memory request for any pod does not exceed 2:1. This ensures that pods do not allocate excessively high memory limits relative to their requests, helping improve resource management and prevent over-provisioning in the cluster.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: limit-memory-ratio-pod\\nspec:\\n  limits:\\n  - maxLimitRequestRatio:\\n      memory: 2\\n    type: Pod\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes LimitRange resource called `limit-memory-ratio-pod`. It sets a constraint on the memory request and limit ratios for pods in a namespace. Specifically, it enforces that the maximum ratio of memory limit to memory request for any pod does not exceed 2:1. This ensures that pods do not allocate excessively high memory limits relative to their requests, helping improve resource management and prevent over-provisioning in the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-memory-ratio-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6cd40388-7fcb-547e-beab-acecf4ee4ad8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes deployment configuration defines a deployment named \"nginx-deployment\" that manages three replicas of an Nginx container. It specifies the use of the Nginx version 1.14.2 image and exposes port 80 on each container. The deployment uses label selectors to identify the pods it manages and ensures high availability by maintaining three identical pods running the Nginx server. This configuration is used for deploying scalable and consistent web server instances in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\nginx-deployment.yaml', 'summary': 'This Kubernetes deployment configuration defines a deployment named \"nginx-deployment\" that manages three replicas of an Nginx container. It specifies the use of the Nginx version 1.14.2 image and exposes port 80 on each container. The deployment uses label selectors to identify the pods it manages and ensures high availability by maintaining three identical pods running the Nginx server. This configuration is used for deploying scalable and consistent web server instances in a Kubernetes cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6d56e992-2f7b-5b81-8f18-69ae15123d1f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Service manifest in YAML format. It creates a service named \"my-service\" that is designed to expose pods labeled with \"app.kubernetes.io/name: MyApp\" on port 80 using TCP protocol. The service specifies \"PreferDualStack\" for the ipFamilyPolicy, indicating a preference for dual-stack (IPv4 and IPv6) support, allowing it to handle both IP address families if available. This configuration helps ensure flexible and scalable network connectivity for the associated application pods within the Kubernetes cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\n  labels:\\n    app.kubernetes.io/name: MyApp\\nspec:\\n  ipFamilyPolicy: PreferDualStack\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-preferred-svc.yaml', 'summary': 'This content defines a Kubernetes Service manifest in YAML format. It creates a service named \"my-service\" that is designed to expose pods labeled with \"app.kubernetes.io/name: MyApp\" on port 80 using TCP protocol. The service specifies \"PreferDualStack\" for the ipFamilyPolicy, indicating a preference for dual-stack (IPv4 and IPv6) support, allowing it to handle both IP address families if available. This configuration helps ensure flexible and scalable network connectivity for the associated application pods within the Kubernetes cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6f0e3375-40d1-5fa9-b9e6-889d8c3606c2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes YAML configuration defines a Job resource named \"job-pod-failure-policy-example\" designed to run multiple parallel containerized tasks. It specifies that 12 completions are needed with a maximum of 3 parallel pods. The job runs a single container based on the Bash 5 image, executing a simple script that outputs \"Hello world!\", sleeps for 5 seconds, and then exits with status code 42. The key feature of this configuration is the Pod Failure Policy, which manages how pod failures are handled. It includes rules that specify actions based on pod exit codes and conditions: if the container named \"main\" exits with code 42, the specified action is to Fail the entire Job; otherwise, it can ignore certain pod conditions like disruptions. The backoff limit is set to 6, meaning the job will retry failed pods up to six times before giving up. Overall, this setup demonstrates advanced failure handling in Kubernetes Jobs, including custom policies for pod failures based on exit codes and conditions.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-pod-failure-policy-example\\nspec:\\n  completions: 12\\n  parallelism: 3\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      containers:\\n      - name: main\\n        image: docker.io/library/bash:5\\n        command: [\"bash\"]        # example command simulating a bug which triggers the FailJob action\\n        args:\\n        - -c\\n        - echo \"Hello world!\" && sleep 5 && exit 42\\n  backoffLimit: 6\\n  podFailurePolicy:\\n    rules:\\n    - action: FailJob\\n      onExitCodes:\\n        containerName: main      # optional\\n        operator: In             # one of: In, NotIn\\n        values: [42]\\n    - action: Ignore             # one of: Ignore, FailJob, Count\\n      onPodConditions:\\n      - type: DisruptionTarget   # indicates Pod disruption\\n', 'chunk': '1/1', 'summary': 'This Kubernetes YAML configuration defines a Job resource named \"job-pod-failure-policy-example\" designed to run multiple parallel containerized tasks. It specifies that 12 completions are needed with a maximum of 3 parallel pods. The job runs a single container based on the Bash 5 image, executing a simple script that outputs \"Hello world!\", sleeps for 5 seconds, and then exits with status code 42. The key feature of this configuration is the Pod Failure Policy, which manages how pod failures are handled. It includes rules that specify actions based on pod exit codes and conditions: if the container named \"main\" exits with code 42, the specified action is to Fail the entire Job; otherwise, it can ignore certain pod conditions like disruptions. The backoff limit is set to 6, meaning the job will retry failed pods up to six times before giving up. Overall, this setup demonstrates advanced failure handling in Kubernetes Jobs, including custom policies for pod failures based on exit codes and conditions.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-example.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6f16f258-8643-5e8e-b5a0-22a274491285'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration written in YAML. It defines a Pod named \"dapi-test-pod\" with a single container named \"test-container\" that uses the BusyBox image. The container executes the command `/bin/sh -c env`, which lists the environment variables when the container runs. The environment variables `SPECIAL_LEVEL_KEY` and `LOG_LEVEL` are populated from external ConfigMaps (`special-config` and `env-config`, respectively), specifically from keys within those ConfigMaps (`special.how` and `log_level`). The `restartPolicy` is set to \"Never,\" indicating the Pod will not restart automatically upon termination. Overall, this configuration illustrates how to set environment variables for a container using ConfigMaps in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\\n      env:\\n        - name: SPECIAL_LEVEL_KEY\\n          valueFrom:\\n            configMapKeyRef:\\n              name: special-config\\n              key: special.how\\n        - name: LOG_LEVEL\\n          valueFrom:\\n            configMapKeyRef:\\n              name: env-config\\n              key: log_level\\n  restartPolicy: Never\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration written in YAML. It defines a Pod named \"dapi-test-pod\" with a single container named \"test-container\" that uses the BusyBox image. The container executes the command `/bin/sh -c env`, which lists the environment variables when the container runs. The environment variables `SPECIAL_LEVEL_KEY` and `LOG_LEVEL` are populated from external ConfigMaps (`special-config` and `env-config`, respectively), specifically from keys within those ConfigMaps (`special.how` and `log_level`). The `restartPolicy` is set to \"Never,\" indicating the Pod will not restart automatically upon termination. Overall, this configuration illustrates how to set environment variables for a container using ConfigMaps in Kubernetes.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-multiple-configmap-env-variable.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('6f816614-f001-5118-979c-02ccb80a882a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content presents a Kubernetes Pod specification manifest that defines a pod named \"dependent-envars-demo\" running a BusyBox container. The container executes a shell script repeatedly every 30 seconds, printing the current values of specific environment variables, which include both static values and variables referencing other variables. The environment variables set within the container include SERVICE_PORT, SERVICE_IP, PROTOCOL, SERVICE_ADDRESS, UNCHANGED_REFERENCE, and ESCAPED_REFERENCE, demonstrating how environment variables can be initialized directly or through variable substitution.\\n\\nThe code\\'s primary function is to continuously display the values of these environment variables to observe how they are interpreted and expanded within the container. The interesting aspect is the differentiation between variable expansion syntax: the use of `$()` for immediate expansion (e.g., UNCHANGED_REFERENCE) and the double dollar sign `$$()` in ESCAPED_REFERENCE to escape the dollar sign, preventing expansion during container startup and allowing for later interpretation. This setup highlights how environment variable referencing and string interpolation work within container environments, especially in Kubernetes manifests.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dependent-envars-demo\\nspec:\\n  containers:\\n    - name: dependent-envars-demo\\n      args:\\n        - while true; do echo -en \\'\\\\n\\'; printf UNCHANGED_REFERENCE=$UNCHANGED_REFERENCE\\'\\\\n\\'; printf SERVICE_ADDRESS=$SERVICE_ADDRESS\\'\\\\n\\';printf ESCAPED_REFERENCE=$ESCAPED_REFERENCE\\'\\\\n\\'; sleep 30; done;\\n      command:\\n        - sh\\n        - -c\\n      image: busybox:1.28\\n      env:\\n        - name: SERVICE_PORT\\n          value: \"80\"\\n        - name: SERVICE_IP\\n          value: \"172.17.0.1\"\\n        - name: UNCHANGED_REFERENCE\\n          value: \"$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)\"\\n        - name: PROTOCOL\\n          value: \"https\"\\n        - name: SERVICE_ADDRESS\\n          value: \"$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)\"\\n        - name: ESCAPED_REFERENCE\\n          value: \"$$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)\"\\n', 'chunk': '1/1', 'summary': 'This content presents a Kubernetes Pod specification manifest that defines a pod named \"dependent-envars-demo\" running a BusyBox container. The container executes a shell script repeatedly every 30 seconds, printing the current values of specific environment variables, which include both static values and variables referencing other variables. The environment variables set within the container include SERVICE_PORT, SERVICE_IP, PROTOCOL, SERVICE_ADDRESS, UNCHANGED_REFERENCE, and ESCAPED_REFERENCE, demonstrating how environment variables can be initialized directly or through variable substitution.\\n\\nThe code\\'s primary function is to continuously display the values of these environment variables to observe how they are interpreted and expanded within the container. The interesting aspect is the differentiation between variable expansion syntax: the use of `$()` for immediate expansion (e.g., UNCHANGED_REFERENCE) and the double dollar sign `$$()` in ESCAPED_REFERENCE to escape the dollar sign, preventing expansion during container startup and allowing for later interpretation. This setup highlights how environment variable referencing and string interpolation work within container environments, especially in Kubernetes manifests.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dependent-envars.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7102e56c-fbe8-5b72-a3f9-bce5a8e93c65'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"default-pod\" with specific security settings. It includes a securityContext that applies a seccomp profile of type \"RuntimeDefault\" to enforce system call restrictions for increased security. The Pod contains a single container using the \"hashicorp/http-echo:1.0\" image, configured to respond with the text \"just made some more syscalls!\" when invoked. Additionally, the container\\'s securityContext disallows privilege escalation, enhancing container isolation and security. Overall, the configuration emphasizes security best practices in containerized environments by configuring seccomp profiles and privilege controls.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-pod\\n  labels:\\n    app: default-pod\\nspec:\\n  securityContext:\\n    seccompProfile:\\n      type: RuntimeDefault\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:1.0\\n    args:\\n    - \"-text=just made some more syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\default-pod.yaml', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"default-pod\" with specific security settings. It includes a securityContext that applies a seccomp profile of type \"RuntimeDefault\" to enforce system call restrictions for increased security. The Pod contains a single container using the \"hashicorp/http-echo:1.0\" image, configured to respond with the text \"just made some more syscalls!\" when invoked. Additionally, the container\\'s securityContext disallows privilege escalation, enhancing container isolation and security. Overall, the configuration emphasizes security best practices in containerized environments by configuring seccomp profiles and privilege controls.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('71410c19-15ab-5768-be73-29c99526a1e5'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration using YAML. The Pod is named `security-context-demo-3` and contains a single container named `sec-ctx-3`, which runs the Docker image `gcr.io/google-samples/hello-app:2.0`. The configuration specifies the basic structure for deploying a simple application inside a Kubernetes environment. It does not include any security context settings or additional configurations, focusing solely on defining thePod and the container it runs.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo-3\\nspec:\\n  containers:\\n  - name: sec-ctx-3\\n    image: gcr.io/google-samples/hello-app:2.0\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Pod configuration using YAML. The Pod is named `security-context-demo-3` and contains a single container named `sec-ctx-3`, which runs the Docker image `gcr.io/google-samples/hello-app:2.0`. The configuration specifies the basic structure for deploying a simple application inside a Kubernetes environment. It does not include any security context settings or additional configurations, focusing solely on defining thePod and the container it runs.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('753e460b-fcb0-5a78-9efb-9f14ad1f3d64'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes LimitRange resource named \"mem-min-max-demo-lr.\" It specifies resource constraints for containers within a namespace, particularly setting a minimum and maximum memory limit. The configuration ensures that containers cannot request less than 500MiB of memory nor allocate more than 1GiB. This helps in managing resource allocation and preventing containers from using excessive or insufficient memory, promoting efficient cluster resource utilization.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: mem-min-max-demo-lr\\nspec:\\n  limits:\\n  - max:\\n      memory: 1Gi\\n    min:\\n      memory: 500Mi\\n    type: Container\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes LimitRange resource named \"mem-min-max-demo-lr.\" It specifies resource constraints for containers within a namespace, particularly setting a minimum and maximum memory limit. The configuration ensures that containers cannot request less than 500MiB of memory nor allocate more than 1GiB. This helps in managing resource allocation and preventing containers from using excessive or insufficient memory, promoting efficient cluster resource utilization.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('75f2aeb5-8e68-532b-992a-a0750a50caea'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration for a MongoDB instance. It uses the `apps/v1` API and defines a deployment named \"mongo\" with metadata labels for identification. The deployment specifies a single replica of the MongoDB container, which uses the official `mongo:4.2` image. The container is configured to bind to all network interfaces (`--bind_ip 0.0.0.0`) to allow external connections. Resource requests are set low with 100 millicores of CPU and 100Mi of memory to ensure minimal resource consumption. The container exposes port 27017, which is the default port for MongoDB, allowing other services to connect to the database. Overall, this configuration automates the deployment and management of a MongoDB server in a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: mongo\\n  labels:\\n    app.kubernetes.io/name: mongo\\n    app.kubernetes.io/component: backend\\nspec:\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: mongo\\n      app.kubernetes.io/component: backend\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: mongo\\n        app.kubernetes.io/component: backend\\n    spec:\\n      containers:\\n      - name: mongo\\n        image: mongo:4.2\\n        args:\\n          - --bind_ip\\n          - 0.0.0.0\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 27017\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration for a MongoDB instance. It uses the `apps/v1` API and defines a deployment named \"mongo\" with metadata labels for identification. The deployment specifies a single replica of the MongoDB container, which uses the official `mongo:4.2` image. The container is configured to bind to all network interfaces (`--bind_ip 0.0.0.0`) to allow external connections. Resource requests are set low with 100 millicores of CPU and 100Mi of memory to ensure minimal resource consumption. The container exposes port 27017, which is the default port for MongoDB, allowing other services to connect to the database. Overall, this configuration automates the deployment and management of a MongoDB server in a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('76b3334a-8c7f-5afd-9736-fc80d639050b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration manifest written in YAML, specifying how a pod is set up to use a hostPath volume on a Windows node. It defines a Pod named \"hostpath-volume-pod\" with a single container that runs the Windows Server Core image. The configuration mounts a directory from the host machine (\"C:\\\\\\\\etc\\\\\\\\foo\") to the container at \"C:\\\\\\\\etc\\\\\\\\foo\" as a read-only volume. The node selector ensures the pod runs on a Windows-based node. The volume is configured as a hostPath volume, which allows the container to access and use files from a specific directory on the host system. This setup is useful for sharing data between the host and container directly through the filesystem.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: hostpath-volume-pod\\nspec:\\n  containers:\\n  - name: my-hostpath-volume-pod\\n    image: microsoft/windowsservercore:1709\\n    volumeMounts:\\n    - name: foo\\n      mountPath: \"C:\\\\\\\\etc\\\\\\\\foo\"\\n      readOnly: true\\n  nodeSelector:\\n    kubernetes.io/os: windows\\n  volumes:\\n  - name: foo\\n    hostPath:\\n     path: \"C:\\\\\\\\etc\\\\\\\\foo\"\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration manifest written in YAML, specifying how a pod is set up to use a hostPath volume on a Windows node. It defines a Pod named \"hostpath-volume-pod\" with a single container that runs the Windows Server Core image. The configuration mounts a directory from the host machine (\"C:\\\\\\\\etc\\\\\\\\foo\") to the container at \"C:\\\\\\\\etc\\\\\\\\foo\" as a read-only volume. The node selector ensures the pod runs on a Windows-based node. The volume is configured as a hostPath volume, which allows the container to access and use files from a specific directory on the host system. This setup is useful for sharing data between the host and container directly through the filesystem.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\hostpath-volume-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('776fbb76-3b90-5ca4-9456-04a236108b6a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Deployment manifest written in YAML, which automates the deployment and management of containerized applications. It specifies a deployment named \"nginx-deployment\" that creates and manages pods running the Nginx web server. The deployment uses the label \"app: nginx\" to identify the pods it manages, and it ensures that each pod runs the specified container with the image \"nginx:1.14.2\". The container is configured to listen on port 80, which is typical for web servers. Additionally, the deployment includes the \"minReadySeconds\" parameter set to 5 seconds, which requires that pods be ready for at least this duration before they are considered available for serving traffic. This YAML template provides a concise, declarative way to deploy and manage a scalable web server environment in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  minReadySeconds: 5\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Deployment manifest written in YAML, which automates the deployment and management of containerized applications. It specifies a deployment named \"nginx-deployment\" that creates and manages pods running the Nginx web server. The deployment uses the label \"app: nginx\" to identify the pods it manages, and it ensures that each pod runs the specified container with the image \"nginx:1.14.2\". The container is configured to listen on port 80, which is typical for web servers. Additionally, the deployment includes the \"minReadySeconds\" parameter set to 5 seconds, which requires that pods be ready for at least this duration before they are considered available for serving traffic. This YAML template provides a concise, declarative way to deploy and manage a scalable web server environment in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\simple_deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('77886595-3952-553f-b310-3b74799a832d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Manifest defining a PersistentVolumeClaim (PVC). The PVC requests storage resources from the cluster, specifying a storage class named \"manual,\" which indicates the manual provisioning of storage. It is configured with an access mode of \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node. The claim requests 3 GiB of storage. This configuration allows a pod to dynamically claim persistent storage matching these specifications, facilitating data persistence in containerized applications.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: pvc-quota-demo\\nspec:\\n  storageClassName: manual\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 3Gi\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Manifest defining a PersistentVolumeClaim (PVC). The PVC requests storage resources from the cluster, specifying a storage class named \"manual,\" which indicates the manual provisioning of storage. It is configured with an access mode of \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node. The claim requests 3 GiB of storage. This configuration allows a pod to dynamically claim persistent storage matching these specifications, facilitating data persistence in containerized applications.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects-pvc.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7936a34b-ca90-5815-ad61-b9d7b73fa02a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes LimitRange resource for containers within a namespace. It sets resource constraints on CPU and memory for containers, specifying minimum, maximum, default, and default request values. The limits ensure containers do not consume more than 800 millicores of CPU or 1Gi of memory, while minimums are set at 100 millicores and 99Mi. Default values (700m CPU, 900Mi memory) are applied if no specific requests are made, and default requests (110m CPU, 111Mi memory) define the initial resource allocation when a container is scheduled. This configuration helps manage resource allocation and prevent containers from over-consuming or being starved of resources.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: limit-mem-cpu-per-container\\nspec:\\n  limits:\\n  - max:\\n      cpu: \"800m\"\\n      memory: \"1Gi\"\\n    min:\\n      cpu: \"100m\"\\n      memory: \"99Mi\"\\n    default:\\n      cpu: \"700m\"\\n      memory: \"900Mi\"\\n    defaultRequest:\\n      cpu: \"110m\"\\n      memory: \"111Mi\"\\n    type: Container\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes LimitRange resource for containers within a namespace. It sets resource constraints on CPU and memory for containers, specifying minimum, maximum, default, and default request values. The limits ensure containers do not consume more than 800 millicores of CPU or 1Gi of memory, while minimums are set at 100 millicores and 99Mi. Default values (700m CPU, 900Mi memory) are applied if no specific requests are made, and default requests (110m CPU, 111Mi memory) define the initial resource allocation when a container is scheduled. This configuration helps manage resource allocation and prevent containers from over-consuming or being starved of resources.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-mem-cpu-container.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7a5127f7-d381-5372-a3fc-b07849c87733'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Pod named `run-as-username-pod-demo` designed to run on Windows nodes. The Spec specifies a security context that sets the `runAsUserName` to \"ContainerUser\", ensuring that the container executes under this specific Windows user account. The Pod contains a single container based on the Windows Server Core image, which runs an ongoing `ping localhost` command to keep the container active. The `nodeSelector` ensures the Pod is scheduled only on Windows nodes. Overall, this configuration demonstrates how to specify a custom Windows user for container execution within a Kubernetes Pod, emphasizing security and user management in Windows-based containers.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: run-as-username-pod-demo\\nspec:\\n  securityContext:\\n    windowsOptions:\\n      runAsUserName: \"ContainerUser\"\\n  containers:\\n  - name: run-as-username-demo\\n    image: mcr.microsoft.com/windows/servercore:ltsc2019\\n    command: [\"ping\", \"-t\", \"localhost\"]\\n  nodeSelector:\\n    kubernetes.io/os: windows\\n', 'chunk': '1/1', 'summary': 'This Kubernetes configuration defines a Pod named `run-as-username-pod-demo` designed to run on Windows nodes. The Spec specifies a security context that sets the `runAsUserName` to \"ContainerUser\", ensuring that the container executes under this specific Windows user account. The Pod contains a single container based on the Windows Server Core image, which runs an ongoing `ping localhost` command to keep the container active. The `nodeSelector` ensures the Pod is scheduled only on Windows nodes. Overall, this configuration demonstrates how to specify a custom Windows user for container execution within a Kubernetes Pod, emphasizing security and user management in Windows-based containers.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\run-as-username-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7c2b6689-c4fe-5a4f-ae3f-ac533897c20f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with the name \"default-mem-demo.\" The Pod contains a single container named \"default-mem-demo-ctr,\" which uses the \"nginx\" image. Essentially, this setup creates a lightweight, isolated environment running an Nginx web server, typically used for serving static content or load testing purposes within a Kubernetes cluster. The configuration is straightforward and provides the basis for deploying a basic web server container in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-mem-demo\\nspec:\\n  containers:\\n  - name: default-mem-demo-ctr\\n    image: nginx\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod with the name \"default-mem-demo.\" The Pod contains a single container named \"default-mem-demo-ctr,\" which uses the \"nginx\" image. Essentially, this setup creates a lightweight, isolated environment running an Nginx web server, typically used for serving static content or load testing purposes within a Kubernetes cluster. The configuration is straightforward and provides the basis for deploying a basic web server container in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7c91b689-dc32-5e9b-a95c-5568e0968d0c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes YAML configuration that defines a ResourceQuota, which limits resource usage within a namespace. Specifically, it sets a quota named \"pod-demo\" that restricts the number of pods to a maximum of 2. This ensures that no more than two pods can be created in the namespace, helping manage resource allocation and prevent overconsumption.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: pod-demo\\nspec:\\n  hard:\\n    pods: \"2\"\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes YAML configuration that defines a ResourceQuota, which limits resource usage within a namespace. Specifically, it sets a quota named \"pod-demo\" that restricts the number of pods to a maximum of 2. This ensures that no more than two pods can be created in the namespace, helping manage resource allocation and prevent overconsumption.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7d97b3d5-6e4c-5174-ad9a-4a3875a83c4b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a ConfigMap in Kubernetes, which is used to store configuration data in key-value pairs. The ConfigMap is named \"company-name-20240312\" and contains a single entry with the key \"company_name\" and the value \"Fiktivesunternehmen GmbH,\" representing a fictional company\\'s name. The `immutable: true` setting indicates that this configuration cannot be modified after creation, providing stability and ensuring consistency across applications that rely on this data. This is useful for storing static configuration data that should remain unchanged during the lifecycle of the deployment.\\napiVersion: v1\\ndata:\\n  company_name: \"Fiktivesunternehmen GmbH\" # new fictional company name\\nkind: ConfigMap\\nimmutable: true\\nmetadata:\\n  name: company-name-20240312', 'chunk': '1/1', 'summary': 'This content defines a ConfigMap in Kubernetes, which is used to store configuration data in key-value pairs. The ConfigMap is named \"company-name-20240312\" and contains a single entry with the key \"company_name\" and the value \"Fiktivesunternehmen GmbH,\" representing a fictional company\\'s name. The `immutable: true` setting indicates that this configuration cannot be modified after creation, providing stability and ensuring consistency across applications that rely on this data. This is useful for storing static configuration data that should remain unchanged during the lifecycle of the deployment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\new-immutable-configmap.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('7e85a6c6-b235-5fcf-9250-7ff23f930c9b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content defines a Kubernetes StorageClass that configures persistent storage using AWS Elastic File System (EFS) through the Container Storage Interface (CSI) driver. It specifies the API version and metadata, including the StorageClass name \"efs-sc.\" The key parameters include the provisioner \"efs.csi.aws.com,\" which indicates the use of AWS EFS, and configuration options such as the provisioning mode \"efs-ap,\" the specific EFS file system ID \"fs-92107410,\" and directory permissions set to \"700.\" This StorageClass enables Kubernetes workloads to dynamically provision and manage EFS-based persistent storage with specified access permissions.\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: efs-sc\\nprovisioner: efs.csi.aws.com\\nparameters:\\n  provisioningMode: efs-ap\\n  fileSystemId: fs-92107410\\n  directoryPerms: \"700\"\\n', 'subchunk': '1/1', 'summary': 'The provided content defines a Kubernetes StorageClass that configures persistent storage using AWS Elastic File System (EFS) through the Container Storage Interface (CSI) driver. It specifies the API version and metadata, including the StorageClass name \"efs-sc.\" The key parameters include the provisioner \"efs.csi.aws.com,\" which indicates the use of AWS EFS, and configuration options such as the provisioning mode \"efs-ap,\" the specific EFS file system ID \"fs-92107410,\" and directory permissions set to \"700.\" This StorageClass enables Kubernetes workloads to dynamically provision and manage EFS-based persistent storage with specified access permissions.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-aws-efs.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('825a2ec9-4b33-56b5-940f-98290bab5a79'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"busybox3\" that contains a single container called \"busybox-cnt01\" using the \"busybox:1.28\" image. The container executes the \"sleep 3600\" command, meaning it will sleep for one hour. The resource management specifies a memory request of 100Mi and a memory limit of 300Mi, ensuring the container has guaranteed minimum resources while preventing it from exceeding the limit. This setup is useful for running lightweight, resource-controlled containerized processes within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: busybox3\\nspec:\\n  containers:\\n  - name: busybox-cnt01\\n    image: busybox:1.28\\n    command: [\"sleep\", \"3600\"]\\n    resources:\\n      limits:\\n        memory: \"300Mi\"\\n      requests:\\n        memory: \"100Mi\"\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"busybox3\" that contains a single container called \"busybox-cnt01\" using the \"busybox:1.28\" image. The container executes the \"sleep 3600\" command, meaning it will sleep for one hour. The resource management specifies a memory request of 100Mi and a memory limit of 300Mi, ensuring the container has guaranteed minimum resources while preventing it from exceeding the limit. This setup is useful for running lightweight, resource-controlled containerized processes within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('829b68c1-4dff-5245-a85f-1c0409560f08'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes ClusterRole named \"csr-approver\" using RBAC (Role-Based Access Control). This ClusterRole grants specific permissions related to managing certificate signing requests (CSRs). It allows users to perform actions such as retrieving, listing, and watching CSRs, updating CSR approvals, and approving signers associated with a particular signer name, in this case, \"example.com/my-signer-name.\" The role is designed to enable a user or service account to handle CSR approval processes securely and with fine-grained control within the cluster.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: csr-approver\\nrules:\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests/approval\\n  verbs:\\n  - update\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - signers\\n  resourceNames:\\n  - example.com/my-signer-name # example.com/* can be used to authorize for all signers in the \\'example.com\\' domain\\n  verbs:\\n  - approve\\n\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes ClusterRole named \"csr-approver\" using RBAC (Role-Based Access Control). This ClusterRole grants specific permissions related to managing certificate signing requests (CSRs). It allows users to perform actions such as retrieving, listing, and watching CSRs, updating CSR approvals, and approving signers associated with a particular signer name, in this case, \"example.com/my-signer-name.\" The role is designed to enable a user or service account to handle CSR approval processes securely and with fine-grained control within the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-approve.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('830688e1-452a-583b-a0f7-af46f8305c4e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code is a YAML configuration for a Kubernetes Pod. It defines a pod named \"default-cpu-demo-2\" which contains a single container running the nginx image. The configuration specifies resource limits, particularly setting a CPU limit of 1 core for the container. This ensures that the container can utilize up to one CPU but no more, helping to manage resource allocation and prevent the container from overusing CPU resources.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-cpu-demo-2\\nspec:\\n  containers:\\n  - name: default-cpu-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"1\"\\n', 'subchunk': '1/1', 'summary': 'The provided code is a YAML configuration for a Kubernetes Pod. It defines a pod named \"default-cpu-demo-2\" which contains a single container running the nginx image. The configuration specifies resource limits, particularly setting a CPU limit of 1 core for the container. This ensures that the container can utilize up to one CPU but no more, helping to manage resource allocation and prevent the container from overusing CPU resources.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('84f0f583-6e55-5081-9a56-91ac29e3bca0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes manifest that defines a new namespace called \"my-restricted-namespace.\" It includes metadata labels related to pod security policies, enforcing a restricted security profile and warning about potential security issues. These labels help implement and monitor security standards within the namespace by guiding Kubernetes to apply specific security restrictions and alerts, thus enhancing the cluster\\'s security posture.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: my-restricted-namespace\\n  labels:\\n    pod-security.kubernetes.io/enforce: restricted\\n    pod-security.kubernetes.io/enforce-version: latest\\n    pod-security.kubernetes.io/warn: restricted\\n    pod-security.kubernetes.io/warn-version: latest', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes manifest that defines a new namespace called \"my-restricted-namespace.\" It includes metadata labels related to pod security policies, enforcing a restricted security profile and warning about potential security issues. These labels help implement and monitor security standards within the namespace by guiding Kubernetes to apply specific security restrictions and alerts, thus enhancing the cluster\\'s security posture.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-restricted.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('84f71ce4-06ee-51be-9227-bbcc6eff7e28'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ValidatingAdmissionPolicy, which is used to enforce validation rules on resource creation and updates within a cluster. The policy is configured to fail requests that do not comply with specified constraints, ensuring strict validation. It applies to all resource types and versions, regardless of the API group, and only on CREATE and UPDATE operations. The policy includes several match conditions to filter out certain requests, such as those related to leases, requests from non-node users, and RBAC-related requests. The validation rule specifically enforces that objects with names containing \\'demo\\' must reside in the \\'demo\\' namespace, or otherwise, the request is permitted. This setup allows for flexible, rule-based admission control in Kubernetes clusters, enhancing security and consistency.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"demo-policy.example.com\"\\nspec:\\n  failurePolicy: Fail\\n  matchConstraints:\\n    resourceRules:\\n      - apiGroups:   [\"*\"]\\n        apiVersions: [\"*\"]\\n        operations:  [\"CREATE\", \"UPDATE\"]\\n        resources:   [\"*\"]\\n  matchConditions:\\n    - name: \\'exclude-leases\\' # Each match condition must have a unique name\\n      expression: \\'!(request.resource.group == \"coordination.k8s.io\" && request.resource.resource == \"leases\")\\' # Match non-lease resources.\\n    - name: \\'exclude-kubelet-requests\\'\\n      expression: \\'!(\"system:nodes\" in request.userInfo.groups)\\' # Match requests made by non-node users.\\n    - name: \\'rbac\\' # Skip RBAC requests.\\n      expression: \\'request.resource.group != \"rbac.authorization.k8s.io\"\\'\\n  validations:\\n    - expression: \"!object.metadata.name.contains(\\'demo\\') || object.metadata.namespace == \\'demo\\'\"\\n\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-match-conditions.yaml', 'summary': \"This content defines a Kubernetes ValidatingAdmissionPolicy, which is used to enforce validation rules on resource creation and updates within a cluster. The policy is configured to fail requests that do not comply with specified constraints, ensuring strict validation. It applies to all resource types and versions, regardless of the API group, and only on CREATE and UPDATE operations. The policy includes several match conditions to filter out certain requests, such as those related to leases, requests from non-node users, and RBAC-related requests. The validation rule specifically enforces that objects with names containing 'demo' must reside in the 'demo' namespace, or otherwise, the request is permitted. This setup allows for flexible, rule-based admission control in Kubernetes clusters, enhancing security and consistency.\", 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('85186593-a30f-510a-bf17-7b58a6a7bab5'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes deployment configuration written in YAML. It specifies a deployment named \"hello-world\" that manages two replicas of a containerized application. The deployment uses the label \"run: load-balancer-example\" for selecting pods and defines a pod template with the same label. The pod runs a container named \"hello-world\" based on the image \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\" and exposes port 8080 via TCP. This setup ensures high availability by running multiple instances of the application, which a load balancer can distribute traffic to.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: hello-world\\nspec:\\n  selector:\\n    matchLabels:\\n      run: load-balancer-example\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        run: load-balancer-example\\n    spec:\\n      containers:\\n        - name: hello-world\\n          image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\\n          ports:\\n            - containerPort: 8080\\n              protocol: TCP\\n', 'chunk': '1/1', 'summary': 'This content describes a Kubernetes deployment configuration written in YAML. It specifies a deployment named \"hello-world\" that manages two replicas of a containerized application. The deployment uses the label \"run: load-balancer-example\" for selecting pods and defines a pod template with the same label. The pod runs a container named \"hello-world\" based on the image \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\" and exposes port 8080 via TCP. This setup ensures high availability by running multiple instances of the application, which a load balancer can distribute traffic to.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\hello-application.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8532a512-c050-5a04-a814-db711186562e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Ingress resource configuration, which manages external access to services within a Kubernetes cluster. It includes two rules: the first directs traffic aimed at \"foo.bar.com\" with the path prefix \"/bar\" to a backend service named \"service1\" on port 80; the second handles traffic for any subdomain of \"*.foo.com\" with the path prefix \"/foo,\" routing it to \"service2\" on port 80. This configuration enables hostname-based routing, directing different URL patterns to specific services inside the cluster, thereby facilitating traffic management and load balancing.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: ingress-wildcard-host\\nspec:\\n  rules:\\n  - host: \"foo.bar.com\"\\n    http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/bar\"\\n        backend:\\n          service:\\n            name: service1\\n            port:\\n              number: 80\\n  - host: \"*.foo.com\"\\n    http:\\n      paths:\\n      - pathType: Prefix\\n        path: \"/foo\"\\n        backend:\\n          service:\\n            name: service2\\n            port:\\n              number: 80\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\ingress-wildcard-host.yaml', 'summary': 'The provided content is a Kubernetes Ingress resource configuration, which manages external access to services within a Kubernetes cluster. It includes two rules: the first directs traffic aimed at \"foo.bar.com\" with the path prefix \"/bar\" to a backend service named \"service1\" on port 80; the second handles traffic for any subdomain of \"*.foo.com\" with the path prefix \"/foo,\" routing it to \"service2\" on port 80. This configuration enables hostname-based routing, directing different URL patterns to specific services inside the cluster, thereby facilitating traffic management and load balancing.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8545beac-e45b-598c-969c-528fc1991026'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Role configuration written in YAML, which defines permissions within a specific namespace. The role named \"pod-reader\" is created in the \"default\" namespace and grants read-only access to pods. Specifically, it allows actions such as \"get,\" \"watch,\" and \"list\" on \"pods\" resources within the core API group. This role helps control access by enabling users or service accounts to view pod information without the ability to modify it, supporting proper access management in Kubernetes clusters.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  namespace: default\\n  name: pod-reader\\nrules:\\n- apiGroups: [\"\"] # \"\" indicates the core API group\\n  resources: [\"pods\"]\\n  verbs: [\"get\", \"watch\", \"list\"]\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-role.yaml', 'summary': 'This content is a Kubernetes Role configuration written in YAML, which defines permissions within a specific namespace. The role named \"pod-reader\" is created in the \"default\" namespace and grants read-only access to pods. Specifically, it allows actions such as \"get,\" \"watch,\" and \"list\" on \"pods\" resources within the core API group. This role helps control access by enabling users or service accounts to view pod information without the ability to modify it, supporting proper access management in Kubernetes clusters.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('859f7902-29ac-578e-a705-294356519534'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"goproxy\" with a single container also named \"goproxy\" that uses the image \"registry.k8s.io/goproxy:0.1\". The configuration exposes port 8080 for the container and sets up health monitoring probes: a readiness probe and a liveness probe, both using TCP socket checks on port 8080. These probes ensure the container is functioning properly before receiving traffic and continuously during operation, with initial delays of 15 seconds and checks every 10 seconds. Overall, this configuration deploys a Go-based proxy server in a Kubernetes environment with health checks to maintain reliability.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: goproxy\\n  labels:\\n    app: goproxy\\nspec:\\n  containers:\\n  - name: goproxy\\n    image: registry.k8s.io/goproxy:0.1\\n    ports:\\n    - containerPort: 8080\\n    readinessProbe:\\n      tcpSocket:\\n        port: 8080\\n      initialDelaySeconds: 15\\n      periodSeconds: 10\\n    livenessProbe:\\n      tcpSocket:\\n        port: 8080\\n      initialDelaySeconds: 15\\n      periodSeconds: 10\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\tcp-liveness-readiness.yaml', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"goproxy\" with a single container also named \"goproxy\" that uses the image \"registry.k8s.io/goproxy:0.1\". The configuration exposes port 8080 for the container and sets up health monitoring probes: a readiness probe and a liveness probe, both using TCP socket checks on port 8080. These probes ensure the container is functioning properly before receiving traffic and continuously during operation, with initial delays of 15 seconds and checks every 10 seconds. Overall, this configuration deploys a Go-based proxy server in a Kubernetes environment with health checks to maintain reliability.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('85e432eb-f340-5743-a180-ac13ba77cc4a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod that demonstrates the use of a projected volume. The Pod, named \"test-projected-volume,\" runs a BusyBox container which executes a long sleep command to keep it alive. A key feature of this configuration is the volume mount at \"/projected-volume,\" which is mounted as read-only.\\n\\nThe volume named \"all-in-one\" uses a projected volume source, combining multiple data sources into a single volume. In this case, the projected volume aggregates two secrets named \"user\" and \"pass,\" allowing the container to access these secrets seamlessly from a common directory path.\\n\\nThis setup illustrates how Kubernetes allows combining multiple secret sources into a single volume, simplifying the management and access of sensitive data within containers. The container can then read the secrets as files within the specified mount path, but cannot modify them.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: test-projected-volume\\nspec:\\n  containers:\\n  - name: test-projected-volume\\n    image: busybox:1.28\\n    args:\\n    - sleep\\n    - \"86400\"\\n    volumeMounts:\\n    - name: all-in-one\\n      mountPath: \"/projected-volume\"\\n      readOnly: true\\n  volumes:\\n  - name: all-in-one\\n    projected:\\n      sources:\\n      - secret:\\n          name: user\\n      - secret:\\n          name: pass\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod that demonstrates the use of a projected volume. The Pod, named \"test-projected-volume,\" runs a BusyBox container which executes a long sleep command to keep it alive. A key feature of this configuration is the volume mount at \"/projected-volume,\" which is mounted as read-only.\\n\\nThe volume named \"all-in-one\" uses a projected volume source, combining multiple data sources into a single volume. In this case, the projected volume aggregates two secrets named \"user\" and \"pass,\" allowing the container to access these secrets seamlessly from a common directory path.\\n\\nThis setup illustrates how Kubernetes allows combining multiple secret sources into a single volume, simplifying the management and access of sensitive data within containers. The container can then read the secrets as files within the specified mount path, but cannot modify them.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('866211a6-66dd-51d2-99c9-a4443c24fd18'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration written in YAML. It defines a pod named `annotation-default-scheduler` with a label `multischeduler-example`. The specification specifies that the pod will use the default Kubernetes scheduler (`default-scheduler`) to decide where to run the pod in the cluster. The pod contains a single container using the `registry.k8s.io/pause:3.8` image, which is commonly used as a placeholder or \"pause\" container for testing or setting up custom scheduling behaviors.\\n\\nThis configuration illustrates how to explicitly assign the default scheduler to a pod and provides a basic template for deploying a simple container in Kubernetes, primarily useful in scenarios where scheduler behavior needs to be explicitly controlled or tested.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: annotation-default-scheduler\\n  labels:\\n    name: multischeduler-example\\nspec:\\n  schedulerName: default-scheduler\\n  containers:\\n  - name: pod-with-default-annotation-container\\n    image: registry.k8s.io/pause:3.8\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration written in YAML. It defines a pod named `annotation-default-scheduler` with a label `multischeduler-example`. The specification specifies that the pod will use the default Kubernetes scheduler (`default-scheduler`) to decide where to run the pod in the cluster. The pod contains a single container using the `registry.k8s.io/pause:3.8` image, which is commonly used as a placeholder or \"pause\" container for testing or setting up custom scheduling behaviors.\\n\\nThis configuration illustrates how to explicitly assign the default scheduler to a pod and provides a basic template for deploying a simple container in Kubernetes, primarily useful in scenarios where scheduler behavior needs to be explicitly controlled or tested.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod2.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('87b5585d-fae4-515a-a444-4760cc52dc0c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format, specifying the deployment of an Nginx container with persistent storage integration. The Pod named \"test\" contains a single container also called \"test\" that uses the Nginx image. The container mounts a shared volume called \"config\" at two locations: one for the website data (`/usr/share/nginx/html`) with a subPath of \"html,\" and another for the Nginx configuration file (`/etc/nginx/nginx.conf`) with a subPath of \"nginx.conf.\" \\n\\nThe \"config\" volume is linked to a PersistentVolumeClaim named \"test-nfs-claim,\" which enables the container to persist and access external storage, likely via an NFS server. This setup allows for dynamic content serving and configuration management, making it suitable for environments requiring stable, persistent storage with manageable configuration and website data separation.\\n\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: test\\nspec:\\n  containers:\\n    - name: test\\n      image: nginx\\n      volumeMounts:\\n        # a mount for site-data\\n        - name: config\\n          mountPath: /usr/share/nginx/html\\n          subPath: html\\n        # another mount for nginx config\\n        - name: config\\n          mountPath: /etc/nginx/nginx.conf\\n          subPath: nginx.conf\\n  volumes:\\n    - name: config\\n      persistentVolumeClaim:\\n        claimName: test-nfs-claim\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-duplicate.yaml', 'summary': 'This content defines a Kubernetes Pod configuration in YAML format, specifying the deployment of an Nginx container with persistent storage integration. The Pod named \"test\" contains a single container also called \"test\" that uses the Nginx image. The container mounts a shared volume called \"config\" at two locations: one for the website data (`/usr/share/nginx/html`) with a subPath of \"html,\" and another for the Nginx configuration file (`/etc/nginx/nginx.conf`) with a subPath of \"nginx.conf.\" \\n\\nThe \"config\" volume is linked to a PersistentVolumeClaim named \"test-nfs-claim,\" which enables the container to persist and access external storage, likely via an NFS server. This setup allows for dynamic content serving and configuration management, making it suitable for environments requiring stable, persistent storage with manageable configuration and website data separation.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('888d85c2-451e-58e1-ad31-1297d8d74fa6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes configuration defining a DaemonSet named \"fluentd-elasticsearch\" in the \"kube-system\" namespace. The purpose of this DaemonSet is to deploy Fluentd instances on each node to collect and forward log data to Elasticsearch, facilitating centralized logging. It uses a rolling update strategy to ensure minimal downtime during updates and includes tolerations to allow deployment on control plane nodes if necessary. The DaemonSet specifies a container running the Fluentd image, mounted with host paths to access log files stored in \"/var/log\" and Docker container logs in \"/var/lib/docker/containers\", enabling comprehensive log collection from all nodes. The configuration ensures the logging agents are resilient and integrated with the host’s filesystem for efficient log processing across the cluster.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluentd-elasticsearch\\n  namespace: kube-system\\n  labels:\\n    k8s-app: fluentd-logging\\nspec:\\n  selector:\\n    matchLabels:\\n      name: fluentd-elasticsearch\\n  updateStrategy:\\n    type: RollingUpdate\\n    rollingUpdate:\\n      maxUnavailable: 1\\n  template:\\n    metadata:\\n      labels:\\n        name: fluentd-elasticsearch\\n    spec:\\n      tolerations:\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator: Exists\\n        effect: NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      containers:\\n      - name: fluentd-elasticsearch\\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        volumeMounts:\\n        - name: varlog\\n          mountPath: /var/log\\n        - name: varlibdockercontainers\\n          mountPath: /var/lib/docker/containers\\n          readOnly: true\\n      terminationGracePeriodSeconds: 30\\n      volumes:\\n      - name: varlog\\n        hostPath:\\n          path: /var/log\\n      - name: varlibdockercontainers\\n        hostPath:\\n          path: /var/lib/docker/containers\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes configuration defining a DaemonSet named \"fluentd-elasticsearch\" in the \"kube-system\" namespace. The purpose of this DaemonSet is to deploy Fluentd instances on each node to collect and forward log data to Elasticsearch, facilitating centralized logging. It uses a rolling update strategy to ensure minimal downtime during updates and includes tolerations to allow deployment on control plane nodes if necessary. The DaemonSet specifies a container running the Fluentd image, mounted with host paths to access log files stored in \"/var/log\" and Docker container logs in \"/var/lib/docker/containers\", enabling comprehensive log collection from all nodes. The configuration ensures the logging agents are resilient and integrated with the host’s filesystem for efficient log processing across the cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('89ba8bd7-ede6-50e6-be78-beb984553879'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The content contains Kubernetes configuration YAML defining RBAC (Role-Based Access Control) resources and a ServiceAccount. The first part creates a ClusterRoleBinding named \"system:konnectivity-server,\" which grants the \"system:auth-delegator\" ClusterRole to the user \"system:konnectivity-server.\" This setup authorizes the user to perform authentication delegation tasks at the cluster level, essential for secure communication between components. The second part defines a ServiceAccount named \"konnectivity-agent\" within the \"kube-system\" namespace, with labels indicating its role in the cluster\\'s internal service management. Overall, these resources facilitate secure and controlled access for the \"konnectivity\" component, which is responsible for secure network connectivity in a Kubernetes cluster.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: system:konnectivity-server\\n  labels:\\n    kubernetes.io/cluster-service: \"true\"\\n    addonmanager.kubernetes.io/mode: Reconcile\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: system:auth-delegator\\nsubjects:\\n  - apiGroup: rbac.authorization.k8s.io\\n    kind: User\\n    name: system:konnectivity-server\\n---\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: konnectivity-agent\\n  namespace: kube-system\\n  labels:\\n    kubernetes.io/cluster-service: \"true\"\\n    addonmanager.kubernetes.io/mode: Reconcile\\n', 'subchunk': '1/1', 'summary': 'The content contains Kubernetes configuration YAML defining RBAC (Role-Based Access Control) resources and a ServiceAccount. The first part creates a ClusterRoleBinding named \"system:konnectivity-server,\" which grants the \"system:auth-delegator\" ClusterRole to the user \"system:konnectivity-server.\" This setup authorizes the user to perform authentication delegation tasks at the cluster level, essential for secure communication between components. The second part defines a ServiceAccount named \"konnectivity-agent\" within the \"kube-system\" namespace, with labels indicating its role in the cluster\\'s internal service management. Overall, these resources facilitate secure and controlled access for the \"konnectivity\" component, which is responsible for secure network connectivity in a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-rbac.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8c6e449e-ca39-5e6b-b6c7-0e5b84b5fcd0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Secret manifest configuring a bootstrap token, which is essential for securely adding new nodes to a Kubernetes cluster. The secret is of type `bootstrap.kubernetes.io/token` and resides in the `kube-system` namespace, indicating its role in cluster bootstrapping. It contains key data such as a token ID (`5emitj`) and a token secret (`kq4gihvszzgn1p0r`), along with metadata about its expiration date. The secret also specifies that it can be used for both bootstrap authentication and signing, which facilitates secure node onboarding and certificate signing during cluster initialization or expansion. This configuration is fundamental in automating and securing the process of integrating new nodes into a Kubernetes environment.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  # Note how the Secret is named\\n  name: bootstrap-token-5emitj\\n  # A bootstrap token Secret usually resides in the kube-system namespace\\n  namespace: kube-system\\ntype: bootstrap.kubernetes.io/token\\nstringData:\\n  auth-extra-groups: \"system:bootstrappers:kubeadm:default-node-token\"\\n  expiration: \"2020-09-13T04:39:10Z\"\\n  # This token ID is used in the name\\n  token-id: \"5emitj\"\\n  token-secret: \"kq4gihvszzgn1p0r\"\\n  # This token can be used for authentication\\n  usage-bootstrap-authentication: \"true\"\\n  # and it can be used for signing\\n  usage-bootstrap-signing: \"true\"', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\bootstrap-token-secret-literal.yaml', 'summary': 'The provided content is a Kubernetes Secret manifest configuring a bootstrap token, which is essential for securely adding new nodes to a Kubernetes cluster. The secret is of type `bootstrap.kubernetes.io/token` and resides in the `kube-system` namespace, indicating its role in cluster bootstrapping. It contains key data such as a token ID (`5emitj`) and a token secret (`kq4gihvszzgn1p0r`), along with metadata about its expiration date. The secret also specifies that it can be used for both bootstrap authentication and signing, which facilitates secure node onboarding and certificate signing during cluster initialization or expansion. This configuration is fundamental in automating and securing the process of integrating new nodes into a Kubernetes environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8d466db4-f369-58dd-9c01-87c7375fcf4e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod Security Policy named \"restricted,\" which enforces strict security rules to safeguard cluster resources. The policy disables privileged containers, prevents privilege escalation, and drops all capabilities to minimize potential security risks. It allows only certain volume types such as ConfigMaps, empty directories, secrets, CSI drivers, and persistent volume claims, assuming these are safe for use. The policy enforces non-root operation by requiring containers to run as non-root users and restricts network, IPC, and PID access to the host, further isolating containers. It also specifies security context rules for SELinux, supplement groups, and file system groups, explicitly forbidding the use of the root group and emphasizing security best practices by defaulting to AppArmor profiles, assuming SELinux is not in use. The purpose of this policy is to establish a secure, least-privilege environment for running pods in Kubernetes.\\napiVersion: policy/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: restricted\\n  annotations:\\n    # docker/default identifies a profile for seccomp, but it is not particularly tied to the Docker runtime\\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: \\'docker/default,runtime/default\\'\\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: \\'runtime/default\\'\\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  \\'runtime/default\\'\\nspec:\\n  privileged: false\\n  # Required to prevent escalations to root.\\n  allowPrivilegeEscalation: false\\n  requiredDropCapabilities:\\n    - ALL\\n  # Allow core volume types.\\n  volumes:\\n    - \\'configMap\\'\\n    - \\'emptyDir\\'\\n    - \\'projected\\'\\n    - \\'secret\\'\\n    - \\'downwardAPI\\'\\n    # Assume that ephemeral CSI drivers & persistentVolumes set up by the cluster admin are safe to use.\\n    - \\'csi\\'\\n    - \\'persistentVolumeClaim\\'\\n    - \\'ephemeral\\'\\n  hostNetwork: false\\n  hostIPC: false\\n  hostPID: false\\n  runAsUser:\\n    # Require the container to run without root privileges.\\n    rule: \\'MustRunAsNonRoot\\'\\n  seLinux:\\n    # This policy assumes the nodes are using AppArmor rather than SELinux.\\n    rule: \\'RunAsAny\\'\\n  supplementalGroups:\\n    rule: \\'MustRunAs\\'\\n    ranges:\\n      # Forbid adding the root group.\\n      - min: 1\\n        max: 65535\\n  fsGroup:\\n    rule: \\'MustRunAs\\'\\n    ranges:\\n      # Forbid adding the root group.\\n      - min: 1\\n        max: 65535\\n  readOnlyRootFilesystem: false\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Pod Security Policy named \"restricted,\" which enforces strict security rules to safeguard cluster resources. The policy disables privileged containers, prevents privilege escalation, and drops all capabilities to minimize potential security risks. It allows only certain volume types such as ConfigMaps, empty directories, secrets, CSI drivers, and persistent volume claims, assuming these are safe for use. The policy enforces non-root operation by requiring containers to run as non-root users and restricts network, IPC, and PID access to the host, further isolating containers. It also specifies security context rules for SELinux, supplement groups, and file system groups, explicitly forbidding the use of the root group and emphasizing security best practices by defaulting to AppArmor profiles, assuming SELinux is not in use. The purpose of this policy is to establish a secure, least-privilege environment for running pods in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\restricted-psp.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8d6289f5-7b02-5749-91da-98ba9c66b3c9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ConfigMap, which is a way to store configuration data for applications. The ConfigMap includes a key-value pair that specifies the company name as \"ACME, Inc.\" The `immutable: true` setting ensures that once created, this ConfigMap cannot be modified, providing stability for configuration data. The metadata assigns a specific name, \"company-name-20150801,\" to uniquely identify this ConfigMap within the Kubernetes cluster. This setup is useful for managing and injecting configuration data into applications in a predictable and controlled manner.\\napiVersion: v1\\ndata:\\n  company_name: \"ACME, Inc.\" # existing fictional company name\\nkind: ConfigMap\\nimmutable: true\\nmetadata:\\n  name: company-name-20150801', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ConfigMap, which is a way to store configuration data for applications. The ConfigMap includes a key-value pair that specifies the company name as \"ACME, Inc.\" The `immutable: true` setting ensures that once created, this ConfigMap cannot be modified, providing stability for configuration data. The metadata assigns a specific name, \"company-name-20150801,\" to uniquely identify this ConfigMap within the Kubernetes cluster. This setup is useful for managing and injecting configuration data into applications in a predictable and controlled manner.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\immutable-configmap.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8e80f338-5772-571f-a5f2-6f1793c39c0d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration in YAML format, designed to deploy an Nginx container. The configuration specifies that the pod should be scheduled only on nodes that meet certain criteria, using node affinity. Specifically, it requires nodes to have a label with the key \"disktype\" and with the value \"ssd\", ensuring that the pod runs on nodes equipped with SSD storage. The container defined within the pod uses the official Nginx image, with an image pull policy set to \"IfNotPresent\", meaning it will only fetch the image if it isn\\'t already available locally. This setup helps optimize resource placement based on node attributes and manages container image pulling efficiently.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:\\n        - matchExpressions:\\n          - key: disktype\\n            operator: In\\n            values:\\n            - ssd            \\n  containers:\\n  - name: nginx\\n    image: nginx\\n    imagePullPolicy: IfNotPresent\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration in YAML format, designed to deploy an Nginx container. The configuration specifies that the pod should be scheduled only on nodes that meet certain criteria, using node affinity. Specifically, it requires nodes to have a label with the key \"disktype\" and with the value \"ssd\", ensuring that the pod runs on nodes equipped with SSD storage. The container defined within the pod uses the official Nginx image, with an image pull policy set to \"IfNotPresent\", meaning it will only fetch the image if it isn\\'t already available locally. This setup helps optimize resource placement based on node attributes and manages container image pulling efficiently.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-required-affinity.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('8e99a545-de96-52f5-8666-4144b6fc2168'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided configuration is a Kubernetes Pod definition in YAML format. It specifies a pod with the name `constraints-cpu-demo-4`, which contains a single container named `constraints-cpu-demo-4-ctr` using the Docker image `vish/stress`. This setup likely aims to run the stress testing tool within the container to simulate CPU load or performance constraints.\\n\\nThe code focuses on deploying a container that can be used for CPU or resource testing, which is useful in understanding how Kubernetes handles resource constraints or for performance benchmarking. It is a basic example showing how to define a pod with a specific image, but it does not include resource limit configurations or commands, which could be added to extend its functionality for resource management experiments.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo-4\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-4-ctr\\n    image: vish/stress\\n', 'chunk': '1/1', 'summary': 'The provided configuration is a Kubernetes Pod definition in YAML format. It specifies a pod with the name `constraints-cpu-demo-4`, which contains a single container named `constraints-cpu-demo-4-ctr` using the Docker image `vish/stress`. This setup likely aims to run the stress testing tool within the container to simulate CPU load or performance constraints.\\n\\nThe code focuses on deploying a container that can be used for CPU or resource testing, which is useful in understanding how Kubernetes handles resource constraints or for performance benchmarking. It is a basic example showing how to define a pod with a specific image, but it does not include resource limit configurations or commands, which could be added to extend its functionality for resource management experiments.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-4.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9006b109-1f41-51cc-bb20-21b3b1305f7e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Deployment configuration that orchestrates a set of pods running an nginx server with a shared volume and an init container. The deployment creates three replicas of the pod, each consisting of an nginx container and an init container named \"alpine.\" The init container continuously writes the current date and a preferred color (fetched from a ConfigMap named \"color\") into an HTML file located in a shared volume, which nginx then serves via its default document root. \\n\\nThe configuration includes two volumes: a shared empty directory (`shared-data`) used for communication between containers, and a ConfigMap volume (`config-volume`) that provides configuration data (the preferred color). The init container mounts both volumes, periodically updating an HTML file with the latest data, while the nginx container mounts the shared directory to serve this content dynamically. This setup demonstrates the use of init containers for initialization tasks, shared volumes for data exchange, and ConfigMaps for configuration management in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-sidecar-container\\n  labels:\\n    app.kubernetes.io/name: configmap-sidecar-container\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-sidecar-container\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-sidecar-container\\n    spec:\\n      volumes:\\n        - name: shared-data\\n          emptyDir: {}\\n        - name: config-volume\\n          configMap:\\n            name: color\\n      containers:\\n        - name: nginx\\n          image: nginx\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /usr/share/nginx/html\\n      initContainers:\\n        - name: alpine\\n          image: alpine:3\\n          restartPolicy: Always\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /pod-data\\n            - name: config-volume\\n              mountPath: /etc/config\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) My preferred color is $(cat /etc/config/color)\" > /pod-data/index.html;\\n              sleep 10; done;\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Deployment configuration that orchestrates a set of pods running an nginx server with a shared volume and an init container. The deployment creates three replicas of the pod, each consisting of an nginx container and an init container named \"alpine.\" The init container continuously writes the current date and a preferred color (fetched from a ConfigMap named \"color\") into an HTML file located in a shared volume, which nginx then serves via its default document root. \\n\\nThe configuration includes two volumes: a shared empty directory (`shared-data`) used for communication between containers, and a ConfigMap volume (`config-volume`) that provides configuration data (the preferred color). The init container mounts both volumes, periodically updating an HTML file with the latest data, while the nginx container mounts the shared directory to serve this content dynamically. This setup demonstrates the use of init containers for initialization tasks, shared volumes for data exchange, and ConfigMaps for configuration management in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-and-sidecar-container.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('904eaa4d-1499-5db4-a0ef-d70bb0fc1a88'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod resource with the API version v1. The Pod is named \"nginx\" and is configured to run on a specific node labeled \"foo-node\" through the \"nodeName\" field. It contains a single container that runs the \"nginx\" image, with the pull policy set to \"IfNotPresent,\" meaning the image will only be pulled from the registry if it is not already available locally. This configuration helps in deploying and managing an nginx web server on a designated node within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  nodeName: foo-node # schedule pod to specific node\\n  containers:\\n  - name: nginx\\n    image: nginx\\n    imagePullPolicy: IfNotPresent\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod resource with the API version v1. The Pod is named \"nginx\" and is configured to run on a specific node labeled \"foo-node\" through the \"nodeName\" field. It contains a single container that runs the \"nginx\" image, with the pull policy set to \"IfNotPresent,\" meaning the image will only be pulled from the registry if it is not already available locally. This configuration helps in deploying and managing an nginx web server on a designated node within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-specific-node.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('90763127-5d0f-5213-b5c5-0816522f60ad'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes ResourceQuota configuration, which is used to limit resource consumption within a namespace. It specifies that the namespace can request up to 1 CPU and 1 GB of memory, and have limits set to a maximum of 2 CPUs and 2 GB of memory. This helps in managing and controlling resource usage to prevent any single namespace from over-consuming cluster resources, ensuring fair distribution and stability of the environment.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: mem-cpu-demo\\nspec:\\n  hard:\\n    requests.cpu: \"1\"\\n    requests.memory: 1Gi\\n    limits.cpu: \"2\"\\n    limits.memory: 2Gi\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes ResourceQuota configuration, which is used to limit resource consumption within a namespace. It specifies that the namespace can request up to 1 CPU and 1 GB of memory, and have limits set to a maximum of 2 CPUs and 2 GB of memory. This helps in managing and controlling resource usage to prevent any single namespace from over-consuming cluster resources, ensuring fair distribution and stability of the environment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('91095ca0-739a-5bec-be14-c6e0b3cb3395'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format, defining a pod named `quota-mem-cpu-demo-2` with a single container. The container runs the Redis image and has specified resource constraints, including resource requests and limits. The requests indicate the minimum resources the container is guaranteed, set to 700MiB of memory and 400 millicores of CPU, while the limits cap the maximum resources the container can consume, set to 1GiB of memory and 800 millicores of CPU. This configuration helps manage resource allocation and ensures efficient utilization and isolation within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: quota-mem-cpu-demo-2\\nspec:\\n  containers:\\n  - name: quota-mem-cpu-demo-2-ctr\\n    image: redis\\n    resources:\\n      limits:\\n        memory: \"1Gi\"\\n        cpu: \"800m\"\\n      requests:\\n        memory: \"700Mi\"\\n        cpu: \"400m\"\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format, defining a pod named `quota-mem-cpu-demo-2` with a single container. The container runs the Redis image and has specified resource constraints, including resource requests and limits. The requests indicate the minimum resources the container is guaranteed, set to 700MiB of memory and 400 millicores of CPU, while the limits cap the maximum resources the container can consume, set to 1GiB of memory and 800 millicores of CPU. This configuration helps manage resource allocation and ensures efficient utilization and isolation within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('915c1d49-f9ec-5be2-b552-a8454b3bf945'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Ingress resource, which manages external access to services within the cluster. The Ingress is named \"test-ingress\" and specifies a default backend service called \"test\" listening on port 80. This means that any traffic directed to the Ingress that does not match specific rules (none are defined here) will be forwarded to the \"test\" service on port 80. This setup is useful for routing external requests to internal services, enabling simplified and flexible traffic management in Kubernetes clusters.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: test-ingress\\nspec:\\n  defaultBackend:\\n    service:\\n      name: test\\n      port:\\n        number: 80\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Ingress resource, which manages external access to services within the cluster. The Ingress is named \"test-ingress\" and specifies a default backend service called \"test\" listening on port 80. This means that any traffic directed to the Ingress that does not match specific rules (none are defined here) will be forwarded to the \"test\" service on port 80. This setup is useful for routing external requests to internal services, enabling simplified and flexible traffic management in Kubernetes clusters.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\test-ingress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9371b36e-ac02-5121-b6fb-fd747d0fe08e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# The provided content is a Kubernetes DaemonSet resource configuration written in YAML. A DaemonSet ensures that a specified pod runs on all or selected nodes of a cluster, typically for system-level tasks like logging or monitoring. In this case, the DaemonSet deploys Fluentd, a popular log collector, configured to run on all nodes within the `kube-system` namespace. The DaemonSet includes tolerations to allow it to run on control plane nodes, which are usually tainted to prevent regular pods from scheduling on them. It specifies a container using the Fluentd image `quay.io/fluentd_elasticsearch/fluentd:v2.5.2`, with resource requests and limits to optimize resource usage. The container mounts the host's `/var/log` directory, enabling Fluentd to collect logs directly from the host filesystem. The configuration also mentions optional priority class settings to ensure this logging pod gets higher scheduling priority if desired. Overall, this YAML sets up a robust logging agent across Kubernetes nodes, enabling centralized log collection for easier monitoring and troubleshooting.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluentd-elasticsearch\\n  namespace: kube-system\\n  labels:\\n    k8s-app: fluentd-logging\\nspec:\\n  selector:\\n    matchLabels:\\n      name: fluentd-elasticsearch\\n  template:\\n    metadata:\\n      labels:\\n        name: fluentd-elasticsearch\\n    spec:\\n      tolerations:\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator: Exists\\n        effect: NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      containers:\\n      - name: fluentd-elasticsearch\\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        resources:\\n          limits:\\n            memory: 200Mi\\n          requests:\\n            cpu: 100m\\n            memory: 200Mi\\n        volumeMounts:\\n        - name: varlog\\n          mountPath: /var/log\\n      # it may be desirable to set a high priority class to ensure that a DaemonSet Pod\\n      # preempts running Pods\\n      # priorityClassName: important\\n      terminationGracePeriodSeconds: 30\\n      volumes:\\n      - name: varlog\\n        hostPath:\\n          path: /var/log\\n\", 'subchunk': '1/1', 'summary': \"The provided content is a Kubernetes DaemonSet resource configuration written in YAML. A DaemonSet ensures that a specified pod runs on all or selected nodes of a cluster, typically for system-level tasks like logging or monitoring. In this case, the DaemonSet deploys Fluentd, a popular log collector, configured to run on all nodes within the `kube-system` namespace. The DaemonSet includes tolerations to allow it to run on control plane nodes, which are usually tainted to prevent regular pods from scheduling on them. It specifies a container using the Fluentd image `quay.io/fluentd_elasticsearch/fluentd:v2.5.2`, with resource requests and limits to optimize resource usage. The container mounts the host's `/var/log` directory, enabling Fluentd to collect logs directly from the host filesystem. The configuration also mentions optional priority class settings to ensure this logging pod gets higher scheduling priority if desired. Overall, this YAML sets up a robust logging agent across Kubernetes nodes, enabling centralized log collection for easier monitoring and troubleshooting.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\daemonset.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('93ae89d0-d731-5668-8f02-f954b16ceb78'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"envfrom-secret\" that runs an Nginx container. The key feature of this configuration is using the `envFrom` attribute to load environment variables from a secret named \"test-secret\" into the container. This approach allows secure management of sensitive data, such as passwords or API keys, by injecting them as environment variables without hardcoding them into the container image. This setup simplifies the process of securely passing secrets to containers, making deployments more secure and manageable.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: envfrom-secret\\nspec:\\n  containers:\\n  - name: envars-test-container\\n    image: nginx\\n    envFrom:\\n    - secretRef:\\n        name: test-secret\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"envfrom-secret\" that runs an Nginx container. The key feature of this configuration is using the `envFrom` attribute to load environment variables from a secret named \"test-secret\" into the container. This approach allows secure management of sensitive data, such as passwords or API keys, by injecting them as environment variables without hardcoding them into the container image. This setup simplifies the process of securely passing secrets to containers, making deployments more secure and manageable.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-secret-envFrom.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9539aeff-5cf9-57f4-819d-5309a8c22fd2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Job configuration in YAML format, designed to run multiple parallel pods with a specific success policy. The job specifies 10 parallel pods (`parallelism: 10`) and aims to complete all 10 tasks (`completions: 10`), with indexing support enabled (`completionMode: Indexed`). The success policy indicates that the job is considered successful if at least one pod with index 0 or within indices 2-3 succeeds.\\n\\nThe pod template runs a single container based on the Python image, executing a Python script. The script checks an environment variable `JOB_COMPLETION_INDEX` to determine its index. If the index is \"2\", the script exits successfully (`sys.exit(0)`), otherwise, it exits with failure (`sys.exit(1)`).\\n\\nThis setup demonstrates how to control job completion criteria based on specific pod indices and their success or failure, allowing for more flexible and targeted job success policies in Kubernetes.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-success\\nspec:\\n  parallelism: 10\\n  completions: 10\\n  completionMode: Indexed # Required for the success policy\\n  successPolicy:\\n    rules:\\n      - succeededIndexes: 0,2-3\\n        succeededCount: 1\\n  template:\\n    spec:\\n      containers:\\n      - name: main\\n        image: python\\n        command:          # Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded,\\n                          # the overall Job is a success.\\n          - python3\\n          - -c\\n          - |\\n            import os, sys\\n            if os.environ.get(\"JOB_COMPLETION_INDEX\") == \"2\":\\n              sys.exit(0)\\n            else:\\n              sys.exit(1)\\n      restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Job configuration in YAML format, designed to run multiple parallel pods with a specific success policy. The job specifies 10 parallel pods (`parallelism: 10`) and aims to complete all 10 tasks (`completions: 10`), with indexing support enabled (`completionMode: Indexed`). The success policy indicates that the job is considered successful if at least one pod with index 0 or within indices 2-3 succeeds.\\n\\nThe pod template runs a single container based on the Python image, executing a Python script. The script checks an environment variable `JOB_COMPLETION_INDEX` to determine its index. If the index is \"2\", the script exits successfully (`sys.exit(0)`), otherwise, it exits with failure (`sys.exit(1)`).\\n\\nThis setup demonstrates how to control job completion criteria based on specific pod indices and their success or failure, allowing for more flexible and targeted job success policies in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-success-policy.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9549147c-fded-5641-ba20-615174e42674'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a comprehensive YAML configuration for deploying a Kubernetes DNS autoscaler, including the necessary RBAC (Role-Based Access Control) setup and deployment specifications. The configuration begins with creating a ServiceAccount named `kube-dns-autoscaler` in the `kube-system` namespace, which is used by the autoscaler pod for interaction with cluster resources. It then defines a ClusterRole (`system:kube-dns-autoscaler`) with specific permissions to list nodes, get and update deployment and replicaset scales, and access configmaps (noting that the configmaps rule is temporary pending a fix).\\n\\nA ClusterRoleBinding associates this role with the `kube-dns-autoscaler` ServiceAccount, enabling the pod to perform its designated actions across the cluster. The Deployment configuration specifies how to run the autoscaler pod, including resource requests, security context, and node selectors to ensure it runs on Linux nodes with appropriate security settings. Within the container, it uses the `cluster-proportional-autoscaler` image to manage DNS replica counts dynamically based on the cluster size.\\n\\nThe container\\'s command-line arguments set parameters such as the namespace, configuration map name, and target resources to scale (`<SCALE_TARGET>` placeholder). It also defines scaling logic preferences through `default-params`, like `coresPerReplica` and `nodesPerReplica`, which help determine when and how autoscaling should trigger depending on the node hardware and cluster configuration. The configuration ensures the autoscaler operates efficiently and securely within the Kubernetes environment, adjusting DNS pods based on cluster demand.\\nkind: ServiceAccount\\napiVersion: v1\\nmetadata:\\n  name: kube-dns-autoscaler\\n  namespace: kube-system\\n---\\nkind: ClusterRole\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: system:kube-dns-autoscaler\\nrules:\\n  - apiGroups: [\"\"]\\n    resources: [\"nodes\"]\\n    verbs: [\"list\", \"watch\"]\\n  - apiGroups: [\"\"]\\n    resources: [\"replicationcontrollers/scale\"]\\n    verbs: [\"get\", \"update\"]\\n  - apiGroups: [\"apps\"]\\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\\n    verbs: [\"get\", \"update\"]\\n# Remove the configmaps rule once below issue is fixed:\\n# kubernetes-incubator/cluster-proportional-autoscaler#16\\n  - apiGroups: [\"\"]\\n    resources: [\"configmaps\"]\\n    verbs: [\"get\", \"create\"]\\n---\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: system:kube-dns-autoscaler\\nsubjects:\\n  - kind: ServiceAccount\\n    name: kube-dns-autoscaler\\n    namespace: kube-system\\nroleRef:\\n  kind: ClusterRole\\n  name: system:kube-dns-autoscaler\\n  apiGroup: rbac.authorization.k8s.io\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: kube-dns-autoscaler\\n  namespace: kube-system\\n  labels:\\n    k8s-app: kube-dns-autoscaler\\n    kubernetes.io/cluster-service: \"true\"\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: kube-dns-autoscaler\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: kube-dns-autoscaler\\n    spec:\\n      priorityClassName: system-cluster-critical\\n      securityContext:\\n        seccompProfile:\\n          type: RuntimeDefault\\n        supplementalGroups: [ 65534 ]\\n        fsGroup: 65534\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      containers:\\n      - name: autoscaler\\n        image: registry.k8s.io/cpa/cluster-proportional-autoscaler:1.8.4\\n        resources:\\n            requests:\\n                cpu: \"20m\"\\n                memory: \"10Mi\"\\n        command:\\n          - /cluster-proportional-autoscaler\\n          - --namespace=kube-system\\n          - --configmap=kube-dns-autoscaler\\n          # Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base\\n          - --target=<SCALE_TARGET>\\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\\n          # If using small nodes, \"nodesPerReplica\" should dominate.\\n          - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true,\"includeUnschedulableNodes\":true}}\\n          - --logtostderr=true\\n          - --v=2\\n      tolerations:\\n      - key: \"CriticalAddonsOnly\"\\n        operator: \"Exists\"\\n      serviceAccountName: kube-dns-autoscaler\\n', 'subchunk': '1/1', 'summary': \"This content provides a comprehensive YAML configuration for deploying a Kubernetes DNS autoscaler, including the necessary RBAC (Role-Based Access Control) setup and deployment specifications. The configuration begins with creating a ServiceAccount named `kube-dns-autoscaler` in the `kube-system` namespace, which is used by the autoscaler pod for interaction with cluster resources. It then defines a ClusterRole (`system:kube-dns-autoscaler`) with specific permissions to list nodes, get and update deployment and replicaset scales, and access configmaps (noting that the configmaps rule is temporary pending a fix).\\n\\nA ClusterRoleBinding associates this role with the `kube-dns-autoscaler` ServiceAccount, enabling the pod to perform its designated actions across the cluster. The Deployment configuration specifies how to run the autoscaler pod, including resource requests, security context, and node selectors to ensure it runs on Linux nodes with appropriate security settings. Within the container, it uses the `cluster-proportional-autoscaler` image to manage DNS replica counts dynamically based on the cluster size.\\n\\nThe container's command-line arguments set parameters such as the namespace, configuration map name, and target resources to scale (`<SCALE_TARGET>` placeholder). It also defines scaling logic preferences through `default-params`, like `coresPerReplica` and `nodesPerReplica`, which help determine when and how autoscaling should trigger depending on the node hardware and cluster configuration. The configuration ensures the autoscaler operates efficiently and securely within the Kubernetes environment, adjusting DNS pods based on cluster demand.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dns-horizontal-autoscaler.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('96448328-5de9-5c51-8070-8c7431b489f8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Job configuration that defines how the job should behave in case of pod failures. It specifies that the job should run 8 completions with a maximum of 2 pods executing in parallel. The job uses a container with a deliberately non-existent image, which will likely cause pod failures. The restart policy for pods is set to \\'Never\\', meaning failed pods won\\'t be automatically restarted. Additionally, the configuration includes a `podFailurePolicy` with a rule to fail the entire job if a pod encounters a condition of type `ConfigIssue`. This setup illustrates how to control job execution and failure handling policies in Kubernetes, particularly emphasizing conditions under which the job should be terminated due to specific pod issues.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-pod-failure-policy-config-issue\\nspec:\\n  completions: 8\\n  parallelism: 2\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      containers:\\n      - name: main\\n        image: \"non-existing-repo/non-existing-image:example\"\\n  backoffLimit: 6\\n  podFailurePolicy:\\n    rules:\\n    - action: FailJob\\n      onPodConditions:\\n      - type: ConfigIssue\\n', 'subchunk': '1/1', 'summary': \"The provided content is a Kubernetes Job configuration that defines how the job should behave in case of pod failures. It specifies that the job should run 8 completions with a maximum of 2 pods executing in parallel. The job uses a container with a deliberately non-existent image, which will likely cause pod failures. The restart policy for pods is set to 'Never', meaning failed pods won't be automatically restarted. Additionally, the configuration includes a `podFailurePolicy` with a rule to fail the entire job if a pod encounters a condition of type `ConfigIssue`. This setup illustrates how to control job execution and failure handling policies in Kubernetes, particularly emphasizing conditions under which the job should be terminated due to specific pod issues.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-config-issue.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('967ce630-5a22-5308-acf5-3ff17be98ed1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ReplicaSet configuration in YAML format. A ReplicaSet ensures a specified number of pod replicas are running at all times. In this case, it is named \"frontend\" and is labeled with \"app: guestbook\" and \"tier: frontend\". The ReplicaSet is set to maintain 3 replicas, matching pods with the label \"tier: frontend\". The pod template specifies a container named \"php-redis\" that uses a particular Docker image hosted on Google\\'s container registry, which likely contains the application code needed for the frontend service. Overall, this configuration automates the deployment and scaling of multiple instances of a frontend application in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: ReplicaSet\\nmetadata:\\n  name: frontend\\n  labels:\\n    app: guestbook\\n    tier: frontend\\nspec:\\n  # modify replicas according to your case\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      tier: frontend\\n  template:\\n    metadata:\\n      labels:\\n        tier: frontend\\n    spec:\\n      containers:\\n      - name: php-redis\\n        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\frontend.yaml', 'summary': 'This content defines a Kubernetes ReplicaSet configuration in YAML format. A ReplicaSet ensures a specified number of pod replicas are running at all times. In this case, it is named \"frontend\" and is labeled with \"app: guestbook\" and \"tier: frontend\". The ReplicaSet is set to maintain 3 replicas, matching pods with the label \"tier: frontend\". The pod template specifies a container named \"php-redis\" that uses a particular Docker image hosted on Google\\'s container registry, which likely contains the application code needed for the frontend service. Overall, this configuration automates the deployment and scaling of multiple instances of a frontend application in a Kubernetes cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('96cd291b-03e9-5773-b47c-0ef707af790f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"memory-demo-3\" within the \"mem-example\" namespace, containing a single container. The container uses the \"polinux/stress\" Docker image and is configured with specific memory requests and limits set to 1000 GiB, ensuring it can utilize up to that amount of memory. The command executed inside the container is \"stress\" with arguments to simulate memory load: it starts one virtual memory worker (\"--vm 1\") that allocates 150 MB (\"--vm-bytes 150M\") and hangs for a short period (\"--vm-hang 1\"). This configuration is typically used to test memory stress scenarios, resource limits, or performance under heavy memory usage in a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: memory-demo-3\\n  namespace: mem-example\\nspec:\\n  containers:\\n  - name: memory-demo-3-ctr\\n    image: polinux/stress\\n    resources:\\n      requests:\\n        memory: \"1000Gi\"\\n      limits:\\n        memory: \"1000Gi\"\\n    command: [\"stress\"]\\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"]\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"memory-demo-3\" within the \"mem-example\" namespace, containing a single container. The container uses the \"polinux/stress\" Docker image and is configured with specific memory requests and limits set to 1000 GiB, ensuring it can utilize up to that amount of memory. The command executed inside the container is \"stress\" with arguments to simulate memory load: it starts one virtual memory worker (\"--vm 1\") that allocates 150 MB (\"--vm-bytes 150M\") and hangs for a short period (\"--vm-hang 1\"). This configuration is typically used to test memory stress scenarios, resource limits, or performance under heavy memory usage in a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('973e9bab-024b-5b66-9ad3-ff8ea15922a8'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code is a Kubernetes configuration that demonstrates how to create and use a Secret to manage sensitive data within a pod. It comprises two main parts: a Secret resource and a Pod resource. The Secret named \"dotfile-secret\" stores a base64-encoded file under the key \".secret-file\". Subsequently, a pod called \"secret-dotfiles-pod\" is configured with a volume that references this Secret, mounted read-only into the container at \"/etc/secret-volume\". The container runs a BusyBox image and executes the command `ls -l /etc/secret-volume`, which lists the contents of the mounted secret directory, allowing verification that the secret data is correctly accessible within the container. This setup showcases a secure method to inject sensitive data into pods without exposing it in plain text.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: dotfile-secret\\ndata:\\n  .secret-file: dmFsdWUtMg0KDQo=\\n---\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: secret-dotfiles-pod\\nspec:\\n  volumes:\\n    - name: secret-volume\\n      secret:\\n        secretName: dotfile-secret\\n  containers:\\n    - name: dotfile-test-container\\n      image: registry.k8s.io/busybox\\n      command:\\n        - ls\\n        - \"-l\"\\n        - \"/etc/secret-volume\"\\n      volumeMounts:\\n        - name: secret-volume\\n          readOnly: true\\n          mountPath: \"/etc/secret-volume\"', 'subchunk': '1/1', 'summary': 'The provided code is a Kubernetes configuration that demonstrates how to create and use a Secret to manage sensitive data within a pod. It comprises two main parts: a Secret resource and a Pod resource. The Secret named \"dotfile-secret\" stores a base64-encoded file under the key \".secret-file\". Subsequently, a pod called \"secret-dotfiles-pod\" is configured with a volume that references this Secret, mounted read-only into the container at \"/etc/secret-volume\". The container runs a BusyBox image and executes the command `ls -l /etc/secret-volume`, which lists the contents of the mounted secret directory, allowing verification that the secret data is correctly accessible within the container. This setup showcases a secure method to inject sensitive data into pods without exposing it in plain text.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\dotfile-secret.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('97ac8b22-28e4-57ba-acfc-8446ecbc4977'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes manifest defines a Deployment resource named \"patch-demo\" with two replicas of an Nginx container. The deployment specifies a label selector to manage pods with the label \"app: nginx,\" and the pod template also includes this label. The container uses the official Nginx image and is named \"patch-demo-ctr.\" Additionally, the pod incorporates tolerations that allow it to be scheduled on nodes with a \"dedicated\" key set to \"test-team\" and effect \"NoSchedule,\" meaning the pod can be scheduled on nodes with matching taints. Overall, this configuration ensures that two Nginx pods are running and can be deployed on tainted nodes with matching tolerations.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: patch-demo\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: patch-demo-ctr\\n        image: nginx\\n      tolerations:\\n      - effect: NoSchedule\\n        key: dedicated\\n        value: test-team\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes manifest defines a Deployment resource named \"patch-demo\" with two replicas of an Nginx container. The deployment specifies a label selector to manage pods with the label \"app: nginx,\" and the pod template also includes this label. The container uses the official Nginx image and is named \"patch-demo-ctr.\" Additionally, the pod incorporates tolerations that allow it to be scheduled on nodes with a \"dedicated\" key set to \"test-team\" and effect \"NoSchedule,\" meaning the pod can be scheduled on nodes with matching taints. Overall, this configuration ensures that two Nginx pods are running and can be deployed on tainted nodes with matching tolerations.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-patch.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('991b7f38-adfa-5495-9b19-cc1a2b0430fa'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"dapi-test-pod.\" The Pod hosts a single container using the BusyBox image, a lightweight Linux environment commonly used for testing and troubleshooting. The container executes a command to display environment variables (`/bin/sh -c env`) upon start. \\n\\nA key feature of this setup is the injection of an environment variable named `SPECIAL_LEVEL_KEY`. Instead of hardcoding its value, the variable\\'s value is retrieved dynamically from a ConfigMap called `special-config`, specifically from its key `special.how`. This approach demonstrates how to decouple configuration data from application code, enabling flexible and manageable configuration management in Kubernetes. The Pod\\'s restart policy is set to \"Never,\" indicating that it should not automatically restart after completion or failure.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\\n      env:\\n        # Define the environment variable\\n        - name: SPECIAL_LEVEL_KEY\\n          valueFrom:\\n            configMapKeyRef:\\n              # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY\\n              name: special-config\\n              # Specify the key associated with the value\\n              key: special.how\\n  restartPolicy: Never\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"dapi-test-pod.\" The Pod hosts a single container using the BusyBox image, a lightweight Linux environment commonly used for testing and troubleshooting. The container executes a command to display environment variables (`/bin/sh -c env`) upon start. \\n\\nA key feature of this setup is the injection of an environment variable named `SPECIAL_LEVEL_KEY`. Instead of hardcoding its value, the variable\\'s value is retrieved dynamically from a ConfigMap called `special-config`, specifically from its key `special.how`. This approach demonstrates how to decouple configuration data from application code, enabling flexible and manageable configuration management in Kubernetes. The Pod\\'s restart policy is set to \"Never,\" indicating that it should not automatically restart after completion or failure.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-single-configmap-env-variable.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('996d256a-4e05-540b-9b4f-b536ff881038'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"busybox\" in the default namespace. The Pod runs a single container based on the BusyBox image version 1.28. The container executes a command to sleep for 3600 seconds (one hour), keeping the container active during this period. The image pull policy specifies that the image should only be pulled if it is not already present on the node. The Pod has a restart policy set to \"Always,\" which means it will automatically restart if it crashes or is terminated. Overall, this configuration sets up a simple, always-running BusyBox container primarily used for testing or lightweight operations within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: busybox\\n  namespace: default\\nspec:\\n  containers:\\n  - name: busybox\\n    image: busybox:1.28\\n    command:\\n      - sleep\\n      - \"3600\"\\n    imagePullPolicy: IfNotPresent\\n  restartPolicy: Always\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"busybox\" in the default namespace. The Pod runs a single container based on the BusyBox image version 1.28. The container executes a command to sleep for 3600 seconds (one hour), keeping the container active during this period. The image pull policy specifies that the image should only be pulled if it is not already present on the node. The Pod has a restart policy set to \"Always,\" which means it will automatically restart if it crashes or is terminated. Overall, this configuration sets up a simple, always-running BusyBox container primarily used for testing or lightweight operations within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\busybox.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9bfc00ee-e869-5044-8c18-166b606a34ef'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# The provided content is a Kubernetes ConfigMap definition that stores Redis configuration settings. The ConfigMap, named `example-redis-config`, contains a key `redis-config` with Redis configuration directives. These directives specify that Redis should use a maximum memory of 2 megabytes (`maxmemory 2mb`) and employ the `allkeys-lru` policy (`maxmemory-policy allkeys-lru`) to evict the least recently used keys when the memory limit is reached. This setup enables dynamic configuration of Redis through Kubernetes, allowing seamless adjustments and management of Redis's memory behavior in a containerized environment.\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: example-redis-config\\ndata:\\n  redis-config: |\\n    maxmemory 2mb\\n    maxmemory-policy allkeys-lru\\n\", 'chunk': '1/1', 'summary': \"The provided content is a Kubernetes ConfigMap definition that stores Redis configuration settings. The ConfigMap, named `example-redis-config`, contains a key `redis-config` with Redis configuration directives. These directives specify that Redis should use a maximum memory of 2 megabytes (`maxmemory 2mb`) and employ the `allkeys-lru` policy (`maxmemory-policy allkeys-lru`) to evict the least recently used keys when the memory limit is reached. This setup enables dynamic configuration of Redis through Kubernetes, allowing seamless adjustments and management of Redis's memory behavior in a containerized environment.\", 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\example-redis-config.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9c0831f4-4db6-5a88-aa36-fc8769373978'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The content defines a Kubernetes ValidatingAdmissionPolicy using the admissionregistration.k8s.io/v1 API version. This policy is used to validate Kubernetes resource objects during creation or update. The key component shown here is the `failurePolicy`, which is set to \"Ignore,\" meaning that if the validation fails, the request will proceed without blocking the operation—a behavior different from the default \"Fail\" setting that would reject invalid requests. The `validations` section contains an expression that verifies whether the `xyz` field in the object\\'s spec matches a parameter `x`. This ensures that custom validation logic can be applied, providing controlled enforcement of resource specifications within Kubernetes clusters.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nspec:\\n...\\nfailurePolicy: Ignore # The default is \"Fail\"\\nvalidations:\\n- expression: \"object.spec.xyz == params.x\"  ', 'subchunk': '1/1', 'summary': 'The content defines a Kubernetes ValidatingAdmissionPolicy using the admissionregistration.k8s.io/v1 API version. This policy is used to validate Kubernetes resource objects during creation or update. The key component shown here is the `failurePolicy`, which is set to \"Ignore,\" meaning that if the validation fails, the request will proceed without blocking the operation—a behavior different from the default \"Fail\" setting that would reject invalid requests. The `validations` section contains an expression that verifies whether the `xyz` field in the object\\'s spec matches a parameter `x`. This ensures that custom validation logic can be applied, providing controlled enforcement of resource specifications within Kubernetes clusters.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\failure-policy-ignore.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9c15e35f-150f-5026-acc8-95e2032595e5'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod with two containers, sharing data through a volume. It specifies a Pod with a \"shared-data\" volume of type `emptyDir`, which creates an ephemeral shared storage space accessible by both containers. The first container runs an Nginx server and mounts the shared volume at `/usr/share/nginx/html`, allowing it to serve static files. The second container runs Debian and mounts the same volume at `/pod-data`. It executes a shell command that writes a simple message, \"Hello from the debian container,\" into an `index.html` file within the shared volume. This setup enables the Debian container to create content dynamically, which the Nginx container then serves, demonstrating shared storage and inter-container communication within a Kubernetes Pod.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: two-containers\\nspec:\\n\\n  restartPolicy: Never\\n\\n  volumes:\\n  - name: shared-data\\n    emptyDir: {}\\n\\n  containers:\\n\\n  - name: nginx-container\\n    image: nginx\\n    volumeMounts:\\n    - name: shared-data\\n      mountPath: /usr/share/nginx/html\\n\\n  - name: debian-container\\n    image: debian\\n    volumeMounts:\\n    - name: shared-data\\n      mountPath: /pod-data\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"echo Hello from the debian container > /pod-data/index.html\"]\\n', 'chunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod with two containers, sharing data through a volume. It specifies a Pod with a \"shared-data\" volume of type `emptyDir`, which creates an ephemeral shared storage space accessible by both containers. The first container runs an Nginx server and mounts the shared volume at `/usr/share/nginx/html`, allowing it to serve static files. The second container runs Debian and mounts the same volume at `/pod-data`. It executes a shell command that writes a simple message, \"Hello from the debian container,\" into an `index.html` file within the shared volume. This setup enables the Debian container to create content dynamically, which the Nginx container then serves, demonstrating shared storage and inter-container communication within a Kubernetes Pod.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\two-container-pod.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9c7e86d2-942f-56f7-a74a-4632996db0fa'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format. It specifies a pod named \"default-cpu-demo-3\" containing a single container that uses the nginx image. The container is allocated a CPU request of 0.75 cores, indicating the amount of CPU resources the container is guaranteed to have. This configuration helps manage resource allocation in a Kubernetes cluster by reserving specific CPU resources for the container to ensure it operates reliably.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-cpu-demo-3\\nspec:\\n  containers:\\n  - name: default-cpu-demo-3-ctr\\n    image: nginx\\n    resources:\\n      requests:\\n        cpu: \"0.75\"\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Pod configuration in YAML format. It specifies a pod named \"default-cpu-demo-3\" containing a single container that uses the nginx image. The container is allocated a CPU request of 0.75 cores, indicating the amount of CPU resources the container is guaranteed to have. This configuration helps manage resource allocation in a Kubernetes cluster by reserving specific CPU resources for the container to ensure it operates reliably.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9d543baa-a6b0-5cdb-92ad-ef732a83958e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod manifest that demonstrates configuring user identities within containers running on Windows nodes. The manifest defines a pod called \"run-as-username-container-demo\" with specific security settings, including a `securityContext` at the pod level that sets the Windows user to \"ContainerUser.\" Additionally, the container named \"run-as-username-demo\" uses the `windowsOptions` to run as \"ContainerAdministrator,\" overriding the pod-level setting. The container runs a Windows Server Core image and continuously pings localhost to keep the container active. The `nodeSelector` ensures that the pod is scheduled on Windows nodes, as indicated by the label `kubernetes.io/os: windows`. Overall, this configuration showcases how to specify different user contexts within the same pod for Windows containers.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: run-as-username-container-demo\\nspec:\\n  securityContext:\\n    windowsOptions:\\n      runAsUserName: \"ContainerUser\"\\n  containers:\\n  - name: run-as-username-demo\\n    image: mcr.microsoft.com/windows/servercore:ltsc2019\\n    command: [\"ping\", \"-t\", \"localhost\"]\\n    securityContext:\\n        windowsOptions:\\n            runAsUserName: \"ContainerAdministrator\"\\n  nodeSelector:\\n    kubernetes.io/os: windows\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Pod manifest that demonstrates configuring user identities within containers running on Windows nodes. The manifest defines a pod called \"run-as-username-container-demo\" with specific security settings, including a `securityContext` at the pod level that sets the Windows user to \"ContainerUser.\" Additionally, the container named \"run-as-username-demo\" uses the `windowsOptions` to run as \"ContainerAdministrator,\" overriding the pod-level setting. The container runs a Windows Server Core image and continuously pings localhost to keep the container active. The `nodeSelector` ensures that the pod is scheduled on Windows nodes, as indicated by the label `kubernetes.io/os: windows`. Overall, this configuration showcases how to specify different user contexts within the same pod for Windows containers.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\run-as-username-container.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9d6f7978-3a4c-5f6a-8ef7-41203ee1ae47'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"termination-demo\" with a single container. The container uses the Debian image and runs a shell command that sleeps for 10 seconds before writing \"Sleep expired\" to the termination log. The code demonstrates how to set up a simple pod that performs a delay and logs a message upon completion, useful for testing pod termination behavior or delay handling in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: termination-demo\\nspec:\\n  containers:\\n  - name: termination-demo-container\\n    image: debian\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"sleep 10 && echo Sleep expired > /dev/termination-log\"]\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"termination-demo\" with a single container. The container uses the Debian image and runs a shell command that sleeps for 10 seconds before writing \"Sleep expired\" to the termination log. The code demonstrates how to set up a simple pod that performs a delay and logs a message upon completion, useful for testing pod termination behavior or delay handling in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\termination.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9d90d5b5-cd3a-5a86-91e9-b44aaf2080cd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This code is a Kubernetes ReplicationController configuration, which manages the deployment and scaling of a set number of identical pods. It specifies that 5 replicas of an nginx container should run simultaneously. Each pod will run the nginx:1.14.2 image and expose port 80. The ReplicationController ensures that the specified number of nginx pods are maintained, automatically creating or deleting pods to match the desired replica count, providing high availability and load balancing for the application.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:\\n  name: my-nginx\\nspec:\\n  replicas: 5\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'This code is a Kubernetes ReplicationController configuration, which manages the deployment and scaling of a set number of identical pods. It specifies that 5 replicas of an nginx container should run simultaneously. Each pod will run the nginx:1.14.2 image and expose port 80. The ReplicationController ensures that the specified number of nginx pods are maintained, automatically creating or deleting pods to match the desired replica count, providing high availability and load balancing for the application.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.14.2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('9f074ed8-071d-5637-833e-e73d2fc9e6c1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes configuration defining a set of resources for deploying a WordPress application with a MySQL database. It includes a Service, a PersistentVolumeClaim, and a Deployment. The Service exposes the MySQL database on port 3306 without a specific cluster IP (headless service), enabling direct access from other pods. The PersistentVolumeClaim requests 20Gi of storage for persistent data storage, ensuring data durability across pod restarts. The Deployment manages the MySQL pod, specifying environment variables for database credentials, which are securely referenced from a Secret. It uses the MySQL 8.0 Docker image and mounts the persistent storage volume at `/var/lib/mysql` to retain database data. Overall, this configuration automates the deployment, persistent storage, and networking setup for a MySQL database in a Kubernetes cluster, serving as a backend for a WordPress application.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: wordpress-mysql\\n  labels:\\n    app: wordpress\\nspec:\\n  ports:\\n    - port: 3306\\n  selector:\\n    app: wordpress\\n    tier: mysql\\n  clusterIP: None\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mysql-pv-claim\\n  labels:\\n    app: wordpress\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 20Gi\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: wordpress-mysql\\n  labels:\\n    app: wordpress\\nspec:\\n  selector:\\n    matchLabels:\\n      app: wordpress\\n      tier: mysql\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      labels:\\n        app: wordpress\\n        tier: mysql\\n    spec:\\n      containers:\\n      - image: mysql:8.0\\n        name: mysql\\n        env:\\n        - name: MYSQL_ROOT_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: mysql-pass\\n              key: password\\n        - name: MYSQL_DATABASE\\n          value: wordpress\\n        - name: MYSQL_USER\\n          value: wordpress\\n        - name: MYSQL_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: mysql-pass\\n              key: password\\n        ports:\\n        - containerPort: 3306\\n          name: mysql\\n        volumeMounts:\\n        - name: mysql-persistent-storage\\n          mountPath: /var/lib/mysql\\n      volumes:\\n      - name: mysql-persistent-storage\\n        persistentVolumeClaim:\\n          claimName: mysql-pv-claim\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes configuration defining a set of resources for deploying a WordPress application with a MySQL database. It includes a Service, a PersistentVolumeClaim, and a Deployment. The Service exposes the MySQL database on port 3306 without a specific cluster IP (headless service), enabling direct access from other pods. The PersistentVolumeClaim requests 20Gi of storage for persistent data storage, ensuring data durability across pod restarts. The Deployment manages the MySQL pod, specifying environment variables for database credentials, which are securely referenced from a Secret. It uses the MySQL 8.0 Docker image and mounts the persistent storage volume at `/var/lib/mysql` to retain database data. Overall, this configuration automates the deployment, persistent storage, and networking setup for a MySQL database in a Kubernetes cluster, serving as a backend for a WordPress application.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\mysql-deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a03a09db-5e24-508d-8924-0e3eaf1792d1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"nginx.\" It specifies a container that uses the official nginx image and mounts a secret token into the container at the directory \"/var/run/secrets/tokens.\" The Pod is configured to use a service account called \"build-robot,\" which facilitates secure access to resources. Additionally, a volume named \"vault-token\" is created using the projected volume source, which pulls the service account token with a set expiration time of 7200 seconds (2 hours) and an audience of \"vault.\" This setup enables the container to authenticate securely, often used for secret management or authenticated access to external services like HashiCorp Vault.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  containers:\\n  - image: nginx\\n    name: nginx\\n    volumeMounts:\\n    - mountPath: /var/run/secrets/tokens\\n      name: vault-token\\n  serviceAccountName: build-robot\\n  volumes:\\n  - name: vault-token\\n    projected:\\n      sources:\\n      - serviceAccountToken:\\n          path: vault-token\\n          expirationSeconds: 7200\\n          audience: vault\\n', 'chunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"nginx.\" It specifies a container that uses the official nginx image and mounts a secret token into the container at the directory \"/var/run/secrets/tokens.\" The Pod is configured to use a service account called \"build-robot,\" which facilitates secure access to resources. Additionally, a volume named \"vault-token\" is created using the projected volume source, which pulls the service account token with a set expiration time of 7200 seconds (2 hours) and an audience of \"vault.\" This setup enables the container to authenticate securely, often used for secret management or authenticated access to external services like HashiCorp Vault.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-projected-svc-token.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a0a8abb8-f9ce-52b0-9f5c-5764ff073dc4'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes `FlowSchema` resource named \"health-for-strangers.\" The FlowSchema is used to control request flow within the Kubernetes API server, determining how requests are prioritized and routed.\\n\\nThe FlowSchema matches requests targeting specific non-resource URLs: \"/healthz\", \"/livez\", and \"/readyz,\" which are typical health check endpoints. These URLs are accessible to unauthenticated subjects, specifically members of the \"system:unauthenticated\" group, and all HTTP verbs (\"*\") are permitted for these endpoints. The configuration assigns a high matching precedence (1000) and associates it with the \"exempt\" priority level, likely indicating these health check requests are designed to bypass certain restrictions or throttling policies. Overall, the setup ensures that health check endpoints are accessible to unauthenticated users, facilitating system monitoring without authorization barriers.\\napiVersion: flowcontrol.apiserver.k8s.io/v1\\nkind: FlowSchema\\nmetadata:\\n  name: health-for-strangers\\nspec:\\n  matchingPrecedence: 1000\\n  priorityLevelConfiguration:\\n    name: exempt\\n  rules:\\n    - nonResourceRules:\\n      - nonResourceURLs:\\n          - \"/healthz\"\\n          - \"/livez\"\\n          - \"/readyz\"\\n        verbs:\\n          - \"*\"\\n      subjects:\\n        - kind: Group\\n          group:\\n            name: \"system:unauthenticated\"\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes `FlowSchema` resource named \"health-for-strangers.\" The FlowSchema is used to control request flow within the Kubernetes API server, determining how requests are prioritized and routed.\\n\\nThe FlowSchema matches requests targeting specific non-resource URLs: \"/healthz\", \"/livez\", and \"/readyz,\" which are typical health check endpoints. These URLs are accessible to unauthenticated subjects, specifically members of the \"system:unauthenticated\" group, and all HTTP verbs (\"*\") are permitted for these endpoints. The configuration assigns a high matching precedence (1000) and associates it with the \"exempt\" priority level, likely indicating these health check requests are designed to bypass certain restrictions or throttling policies. Overall, the setup ensures that health check endpoints are accessible to unauthenticated users, facilitating system monitoring without authorization barriers.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priority-and-fairness\\\\health-for-strangers.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a13571a3-f8bc-5a8f-a898-699db6acc3d2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes DaemonSet configuration in YAML format. A DaemonSet ensures that a specific pod runs on all (or selected) nodes in a cluster. The configuration specifies the DaemonSet named \"ssd-driver,\" which targets nodes labeled with `ssd: \"true\"` using a nodeSelector. It includes a single container named \"example-container\" that runs from the \"example-image\" image. The purpose of this setup is to deploy a specific pod (presumably a driver or service related to SSDs) on all nodes equipped with SSDs, aiding in managing hardware-specific workloads across the cluster.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: ssd-driver\\n  labels:\\n    app: nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: ssd-driver-pod\\n  template:\\n    metadata:\\n      labels:\\n        app: ssd-driver-pod\\n    spec:\\n      nodeSelector:\\n        ssd: \"true\"\\n      containers:\\n        - name: example-container\\n          image: example-image', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes DaemonSet configuration in YAML format. A DaemonSet ensures that a specific pod runs on all (or selected) nodes in a cluster. The configuration specifies the DaemonSet named \"ssd-driver,\" which targets nodes labeled with `ssd: \"true\"` using a nodeSelector. It includes a single container named \"example-container\" that runs from the \"example-image\" image. The purpose of this setup is to deploy a specific pod (presumably a driver or service related to SSDs) on all nodes equipped with SSDs, aiding in managing hardware-specific workloads across the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\daemonset-label-selector.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a2263a01-b60e-5199-ab84-82b0e38812c3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Horizontal Pod Autoscaler (HPA) for a Kubernetes deployment, specifically targeting a ReplicaSet named \"frontend.\" The HPA automatically adjusts the number of pod replicas between 3 and 10, based on CPU utilization. It aims to maintain CPU usage at around 50%, scaling out or in to ensure the application remains responsive while efficiently using resources. This setup helps improve application scalability and reliability by dynamically managing pod counts according to workload demand.\\napiVersion: autoscaling/v1\\nkind: HorizontalPodAutoscaler\\nmetadata:\\n  name: frontend-scaler\\nspec:\\n  scaleTargetRef:\\n    kind: ReplicaSet\\n    name: frontend\\n  minReplicas: 3\\n  maxReplicas: 10\\n  targetCPUUtilizationPercentage: 50\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Horizontal Pod Autoscaler (HPA) for a Kubernetes deployment, specifically targeting a ReplicaSet named \"frontend.\" The HPA automatically adjusts the number of pod replicas between 3 and 10, based on CPU utilization. It aims to maintain CPU usage at around 50%, scaling out or in to ensure the application remains responsive while efficiently using resources. This setup helps improve application scalability and reliability by dynamically managing pod counts according to workload demand.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\hpa-rs.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a2954ce7-b1b8-5db9-b99a-206ee32df779'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code is a Kubernetes Pod configuration written in YAML, used to define security settings at both the Pod and container levels. It sets a security context for the entire Pod with `runAsUser: 1000`, meaning all processes inside the Pod initially run as user ID 1000 unless overridden. Within the container, a specific security context overrides this with `runAsUser: 2000`, so processes inside the container run as user ID 2000. Additionally, it disables privilege escalation with `allowPrivilegeEscalation: false`, which enhances security by preventing the container from gaining higher privileges than initially assigned. Overall, this configuration demonstrates how to control user privileges and security permissions for a Pod and its container in Kubernetes, promoting a secure container deployment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo-2\\nspec:\\n  securityContext:\\n    runAsUser: 1000\\n  containers:\\n  - name: sec-ctx-demo-2\\n    image: gcr.io/google-samples/hello-app:2.0\\n    securityContext:\\n      runAsUser: 2000\\n      allowPrivilegeEscalation: false\\n', 'chunk': '1/1', 'summary': 'The provided code is a Kubernetes Pod configuration written in YAML, used to define security settings at both the Pod and container levels. It sets a security context for the entire Pod with `runAsUser: 1000`, meaning all processes inside the Pod initially run as user ID 1000 unless overridden. Within the container, a specific security context overrides this with `runAsUser: 2000`, so processes inside the container run as user ID 2000. Additionally, it disables privilege escalation with `allowPrivilegeEscalation: false`, which enhances security by preventing the container from gaining higher privileges than initially assigned. Overall, this configuration demonstrates how to control user privileges and security permissions for a Pod and its container in Kubernetes, promoting a secure container deployment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a2b303b1-6bd4-55e1-8622-0b7ab52c71af'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# The provided content is a Kubernetes DaemonSet configuration, which ensures that a specific pod runs on all (or selected) nodes in a cluster. This particular DaemonSet deploys Fluentd, a logging agent, configured to forward logs to Elasticsearch. It is set in the `kube-system` namespace and labeled appropriately for management. The update strategy employs rolling updates with a maximum of one unavailable pod during updates, ensuring minimal disruption.\\n\\nThe pod template specifies tolerations allowing it to run on control plane nodes by matching specific taints, which is useful for centralized logging. The container uses a Fluentd image (`quay.io/fluentd_elasticsearch/fluentd:v2.5.2`) with resource limits and requests to optimize resource utilization. It mounts host paths (`/var/log` and `/var/lib/docker/containers`) into the container, enabling Fluentd to collect logs directly from the node's log files and Docker container logs. The configuration thus ensures that logs from all nodes are collected and forwarded for centralized analysis with minimal impact on host resources.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluentd-elasticsearch\\n  namespace: kube-system\\n  labels:\\n    k8s-app: fluentd-logging\\nspec:\\n  selector:\\n    matchLabels:\\n      name: fluentd-elasticsearch\\n  updateStrategy:\\n    type: RollingUpdate\\n    rollingUpdate:\\n      maxUnavailable: 1\\n  template:\\n    metadata:\\n      labels:\\n        name: fluentd-elasticsearch\\n    spec:\\n      tolerations:\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator: Exists\\n        effect: NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      containers:\\n      - name: fluentd-elasticsearch\\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        resources:\\n          limits:\\n            memory: 200Mi\\n          requests:\\n            cpu: 100m\\n            memory: 200Mi\\n        volumeMounts:\\n        - name: varlog\\n          mountPath: /var/log\\n        - name: varlibdockercontainers\\n          mountPath: /var/lib/docker/containers\\n          readOnly: true\\n      terminationGracePeriodSeconds: 30\\n      volumes:\\n      - name: varlog\\n        hostPath:\\n          path: /var/log\\n      - name: varlibdockercontainers\\n        hostPath:\\n          path: /var/lib/docker/containers\\n\", 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset-update.yaml', 'summary': \"The provided content is a Kubernetes DaemonSet configuration, which ensures that a specific pod runs on all (or selected) nodes in a cluster. This particular DaemonSet deploys Fluentd, a logging agent, configured to forward logs to Elasticsearch. It is set in the `kube-system` namespace and labeled appropriately for management. The update strategy employs rolling updates with a maximum of one unavailable pod during updates, ensuring minimal disruption.\\n\\nThe pod template specifies tolerations allowing it to run on control plane nodes by matching specific taints, which is useful for centralized logging. The container uses a Fluentd image (`quay.io/fluentd_elasticsearch/fluentd:v2.5.2`) with resource limits and requests to optimize resource utilization. It mounts host paths (`/var/log` and `/var/lib/docker/containers`) into the container, enabling Fluentd to collect logs directly from the node's log files and Docker container logs. The configuration thus ensures that logs from all nodes are collected and forwarded for centralized analysis with minimal impact on host resources.\", 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a326b217-2253-5004-9928-4427613359cd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"retainkeys-demo\" that manages an nginx application. The deployment uses a rolling update strategy with a maximum surge of 30%, allowing new pods to be created ahead of old ones being terminated, thereby minimizing downtime. The template specifies a container named \"retainkeys-demo-ctr\" running the nginx image. Overall, this configuration automates the deployment and upgrade process of an nginx-based application using Kubernetes, ensuring smooth updates with minimal service disruption.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: retainkeys-demo\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 30%\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: retainkeys-demo-ctr\\n        image: nginx\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"retainkeys-demo\" that manages an nginx application. The deployment uses a rolling update strategy with a maximum surge of 30%, allowing new pods to be created ahead of old ones being terminated, thereby minimizing downtime. The template specifies a container named \"retainkeys-demo-ctr\" running the nginx image. Overall, this configuration automates the deployment and upgrade process of an nginx-based application using Kubernetes, ensuring smooth updates with minimal service disruption.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-retainkeys.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a35bd6ea-24a2-5f41-ac2b-bffa18c9f8dd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes audit policy configuration, which defines rules for logging various API server requests to monitor cluster activity. The policy specifies that audit events should not be generated during the \"RequestReceived\" stage for efficiency, and sets different logging levels—such as `RequestResponse`, `Metadata`, or `None`—based on resource types, user identity, or request paths.\\n\\nThe policy provides detailed rules for logging specific resources like pods, configmaps, secrets, and logs, with certain exceptions like ignoring requests for a \"controller-leader\" configmap, watch requests by the \"system:kube-proxy\" user, or authenticated requests to non-resource URLs (e.g., `/api*`, `/version`). Some rules focus on capturing detailed request bodies, such as for configmaps in the \"kube-system\" namespace, while more general rules log resource metadata or all other requests at a broader level. This structured approach helps administrators monitor critical activities while filtering out less important or routine requests to reduce noise in audit logs.\\napiVersion: audit.k8s.io/v1 # This is required.\\nkind: Policy\\n# Don\\'t generate audit events for all requests in RequestReceived stage.\\nomitStages:\\n  - \"RequestReceived\"\\nrules:\\n  # Log pod changes at RequestResponse level\\n  - level: RequestResponse\\n    resources:\\n    - group: \"\"\\n      # Resource \"pods\" doesn\\'t match requests to any subresource of pods,\\n      # which is consistent with the RBAC policy.\\n      resources: [\"pods\"]\\n  # Log \"pods/log\", \"pods/status\" at Metadata level\\n  - level: Metadata\\n    resources:\\n    - group: \"\"\\n      resources: [\"pods/log\", \"pods/status\"]\\n\\n  # Don\\'t log requests to a configmap called \"controller-leader\"\\n  - level: None\\n    resources:\\n    - group: \"\"\\n      resources: [\"configmaps\"]\\n      resourceNames: [\"controller-leader\"]\\n\\n  # Don\\'t log watch requests by the \"system:kube-proxy\" on endpoints or services\\n  - level: None\\n    users: [\"system:kube-proxy\"]\\n    verbs: [\"watch\"]\\n    resources:\\n    - group: \"\" # core API group\\n      resources: [\"endpoints\", \"services\"]\\n\\n  # Don\\'t log authenticated requests to certain non-resource URL paths.\\n  - level: None\\n    userGroups: [\"system:authenticated\"]\\n    nonResourceURLs:\\n    - \"/api*\" # Wildcard matching.\\n    - \"/version\"\\n\\n  # Log the request body of configmap changes in kube-system.\\n  - level: Request\\n    resources:\\n    - group: \"\" # core API group\\n      resources: [\"configmaps\"]\\n    # This rule only applies to resources in the \"kube-system\" namespace.\\n    # The empty string \"\" can be used to select non-namespaced resources.\\n    namespaces: [\"kube-system\"]\\n\\n  # Log configmap and secret changes in all other namespaces at the Metadata level.\\n  - level: Metadata\\n    resources:\\n    - group: \"\" # core API group\\n      resources: [\"secrets\", \"configmaps\"]\\n\\n  # Log all other resources in core and extensions at the Request level.\\n  - level: Request\\n    resources:\\n    - group: \"\" # core API group\\n    - group: \"extensions\" # Version of group should NOT be included.\\n\\n  # A catch-all rule to log all other requests at the Metadata level.\\n  - level: Metadata\\n    # Long-running requests like watches that fall under this rule will not\\n    # generate an audit event in RequestReceived.\\n    omitStages:\\n      - \"RequestReceived\"\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes audit policy configuration, which defines rules for logging various API server requests to monitor cluster activity. The policy specifies that audit events should not be generated during the \"RequestReceived\" stage for efficiency, and sets different logging levels—such as `RequestResponse`, `Metadata`, or `None`—based on resource types, user identity, or request paths.\\n\\nThe policy provides detailed rules for logging specific resources like pods, configmaps, secrets, and logs, with certain exceptions like ignoring requests for a \"controller-leader\" configmap, watch requests by the \"system:kube-proxy\" user, or authenticated requests to non-resource URLs (e.g., `/api*`, `/version`). Some rules focus on capturing detailed request bodies, such as for configmaps in the \"kube-system\" namespace, while more general rules log resource metadata or all other requests at a broader level. This structured approach helps administrators monitor critical activities while filtering out less important or routine requests to reduce noise in audit logs.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\audit\\\\audit-policy.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a38e93f4-3955-5d2b-943c-a6192c784e58'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes IngressClass resource, which specifies how ingress traffic should be managed in a cluster. The resource is named \"external-lb\" and uses a custom ingress controller identified by \"example.com/ingress-controller\". It also references additional ingress parameters through a specified API group (\"k8s.example.com\") and kind (\"IngressParameters\"), with the parameters named \"external-lb\". Essentially, this configuration allows users to set up and customize ingress behavior by associating specific ingress controllers with particular parameters, enabling flexible traffic management in Kubernetes applications.\\napiVersion: networking.k8s.io/v1\\nkind: IngressClass\\nmetadata:\\n  name: external-lb\\nspec:\\n  controller: example.com/ingress-controller\\n  parameters:\\n    apiGroup: k8s.example.com\\n    kind: IngressParameters\\n    name: external-lb\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes IngressClass resource, which specifies how ingress traffic should be managed in a cluster. The resource is named \"external-lb\" and uses a custom ingress controller identified by \"example.com/ingress-controller\". It also references additional ingress parameters through a specified API group (\"k8s.example.com\") and kind (\"IngressParameters\"), with the parameters named \"external-lb\". Essentially, this configuration allows users to set up and customize ingress behavior by associating specific ingress controllers with particular parameters, enabling flexible traffic management in Kubernetes applications.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\external-lb.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a6a9486e-3abe-5fb5-ba76-706a6d829352'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration in YAML format. It defines a pod named `sa-ctb-name-test` that runs a single container using the `busybox` image, executing a sleep command to keep the container alive for an hour. The container mounts a volume at `/root-certificates`, which is designed to project trusted root certificates from the cluster. The volume `token-vol` is configured using a projected source that includes two cluster trust bundles: one explicitly named `example`, and another identified by a signer name with an optional path, label selector, and optional flag. This setup enables the container to access multiple cluster trust bundles, which are used for SSL/TLS verification, adhering to best practices for secure communication within Kubernetes clusters.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: sa-ctb-name-test\\nspec:\\n  containers:\\n  - name: container-test\\n    image: busybox\\n    command: [\"sleep\", \"3600\"]\\n    volumeMounts:\\n    - name: token-vol\\n      mountPath: \"/root-certificates\"\\n      readOnly: true\\n  serviceAccountName: default\\n  volumes:\\n  - name: token-vol\\n    projected:\\n      sources:\\n      - clusterTrustBundle:\\n          name: example\\n          path: example-roots.pem\\n      - clusterTrustBundle:\\n          signerName: \"example.com/mysigner\"\\n          labelSelector:\\n            matchLabels:\\n              version: live\\n          path: mysigner-roots.pem\\n          optional: true\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration in YAML format. It defines a pod named `sa-ctb-name-test` that runs a single container using the `busybox` image, executing a sleep command to keep the container alive for an hour. The container mounts a volume at `/root-certificates`, which is designed to project trusted root certificates from the cluster. The volume `token-vol` is configured using a projected source that includes two cluster trust bundles: one explicitly named `example`, and another identified by a signer name with an optional path, label selector, and optional flag. This setup enables the container to access multiple cluster trust bundles, which are used for SSL/TLS verification, adhering to best practices for secure communication within Kubernetes clusters.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-clustertrustbundle.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a7aef51a-d320-5170-9874-4063d731c237'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes ConfigMap that configures Fluentd, a data collector used for log management. This configuration defines two log sources, both of type \"tail,\" which monitor specific log files (`/var/log/1.log` and `/var/log/2.log`) without applying any particular parsing format. Each source uses a position file to keep track of log reading progress. Incoming logs from these sources are tagged differently (`count.format1` and `count.format2`) for identification. All logs captured by Fluentd are then forwarded to Google Cloud Logging using a match rule that applies to all tags (\"**\"). This setup enables Fluentd to aggregate log data from specified files and send it to Google Cloud for centralized storage and analysis.\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: fluentd-config\\ndata:\\n  fluentd.conf: |\\n    <source>\\n      type tail\\n      format none\\n      path /var/log/1.log\\n      pos_file /var/log/1.log.pos\\n      tag count.format1\\n    </source>\\n\\n    <source>\\n      type tail\\n      format none\\n      path /var/log/2.log\\n      pos_file /var/log/2.log.pos\\n      tag count.format2\\n    </source>\\n\\n    <match **>\\n      type google_cloud\\n    </match>\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\fluentd-sidecar-config.yaml', 'summary': 'The provided content is a Kubernetes ConfigMap that configures Fluentd, a data collector used for log management. This configuration defines two log sources, both of type \"tail,\" which monitor specific log files (`/var/log/1.log` and `/var/log/2.log`) without applying any particular parsing format. Each source uses a position file to keep track of log reading progress. Incoming logs from these sources are tagged differently (`count.format1` and `count.format2`) for identification. All logs captured by Fluentd are then forwarded to Google Cloud Logging using a match rule that applies to all tags (\"**\"). This setup enables Fluentd to aggregate log data from specified files and send it to Google Cloud for centralized storage and analysis.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a85b3375-6795-50be-ac77-5acb373c643b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content defines a Kubernetes deployment and service for running an NGINX web server. The service configuration creates a LoadBalancer type service named \"my-nginx-svc\" that exposes port 80 and selects pods with the label \"app: nginx,\" allowing external access to the application. The deployment configuration named \"my-nginx\" specifies three replicas of the NGINX container running the version 1.14.2 image, each listening on port 80, and ensures that pods are created with the label \"app: nginx\" to be matched by the service. This setup enables load-balanced, scalable deployment of the NGINX web server in a Kubernetes cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-nginx-svc\\n  labels:\\n    app: nginx\\nspec:\\n  type: LoadBalancer\\n  ports:\\n  - port: 80\\n  selector:\\n    app: nginx\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'The provided content defines a Kubernetes deployment and service for running an NGINX web server. The service configuration creates a LoadBalancer type service named \"my-nginx-svc\" that exposes port 80 and selects pods with the label \"app: nginx,\" allowing external access to the application. The deployment configuration named \"my-nginx\" specifies three replicas of the NGINX container running the version 1.14.2 image, each listening on port 80, and ensures that pods are created with the label \"app: nginx\" to be matched by the service. This setup enables load-balanced, scalable deployment of the NGINX web server in a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-app.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('a9f528d2-e363-52bc-ba4e-3a3b659c86be'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Service named \"nginx-service.\" The service uses a label selector (\"app: nginx\") to identify the pods it should route traffic to. It exposes port 80 using TCP protocol, where the service port (80) forwards traffic to port 80 on the selected pods. Essentially, this setup creates a stable access point for clients to communicate with any nginx pods running in the cluster, enabling load balancing and simplified service discovery.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx-service\\nspec:\\n  selector:\\n    app: nginx\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n      targetPort: 80', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Service named \"nginx-service.\" The service uses a label selector (\"app: nginx\") to identify the pods it should route traffic to. It exposes port 80 using TCP protocol, where the service port (80) forwards traffic to port 80 on the selected pods. Essentially, this setup creates a stable access point for clients to communicate with any nginx pods running in the cluster, enabling load balancing and simplified service discovery.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\explore-graceful-termination-nginx.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('aa3645bf-0055-58fa-89bb-a5812c415fc2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration file written in YAML, designed to specify security settings with AppArmor profiles. The key aspect is the use of the `securityContext` to assign a specific AppArmor profile (`k8s-apparmor-example-deny-write`) to the Pod, which is of the \\'Localhost\\' type, indicating the profile exists locally on the node. The container within the Pod uses the `busybox:1.28` image and runs a simple command that outputs \"Hello AppArmor!\" and then sleeps for an hour. This setup demonstrates how to enforce AppArmor security policies on containers in Kubernetes, helping to enhance security by restricting container actions according to the specified profile.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: hello-apparmor\\nspec:\\n  securityContext:\\n    appArmorProfile:\\n      type: Localhost\\n      localhostProfile: k8s-apparmor-example-deny-write\\n  containers:\\n  - name: hello\\n    image: busybox:1.28\\n    command: [ \"sh\", \"-c\", \"echo \\'Hello AppArmor!\\' && sleep 1h\" ]\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration file written in YAML, designed to specify security settings with AppArmor profiles. The key aspect is the use of the `securityContext` to assign a specific AppArmor profile (`k8s-apparmor-example-deny-write`) to the Pod, which is of the \\'Localhost\\' type, indicating the profile exists locally on the node. The container within the Pod uses the `busybox:1.28` image and runs a simple command that outputs \"Hello AppArmor!\" and then sleeps for an hour. This setup demonstrates how to enforce AppArmor security policies on containers in Kubernetes, helping to enhance security by restricting container actions according to the specified profile.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\hello-apparmor.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('aa4e016f-7b01-5f9e-a0ae-53c861154b3d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Pod named \"qos-demo\" within the \"qos-example\" namespace. The Pod contains a single container running the Nginx image. It specifies resource requests and limits for CPU and memory, both set to 200Mi of memory and 700m of CPU. These settings help Kubernetes manage the Pod’s Quality of Service (QoS) by indicating the minimum required resources (requests) and the maximum allowed (limits). This ensures proper resource allocation and prioritization among multiple Pods in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: qos-demo\\n  namespace: qos-example\\nspec:\\n  containers:\\n  - name: qos-demo-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"700m\"\\n      requests:\\n        memory: \"200Mi\"\\n        cpu: \"700m\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod.yaml', 'summary': 'This Kubernetes configuration defines a Pod named \"qos-demo\" within the \"qos-example\" namespace. The Pod contains a single container running the Nginx image. It specifies resource requests and limits for CPU and memory, both set to 200Mi of memory and 700m of CPU. These settings help Kubernetes manage the Pod’s Quality of Service (QoS) by indicating the minimum required resources (requests) and the maximum allowed (limits). This ensures proper resource allocation and prioritization among multiple Pods in the cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('aaa77092-821e-51c2-9eef-f773fce4e7c0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a Kubernetes deployment configuration for an Nginx server. It specifies creating a deployment named \"nginx-deployment\" with a single replica, which uses the latest Nginx image and exposes port 80. The deployment includes a lifecycle hook for the container\\'s preStop event, where it executes a command to sleep for 180 seconds before shutdown. This simulates a graceful termination period, allowing ongoing requests to complete while the container is shutting down, even though the specified termination grace period is only 120 seconds. The configuration ensures the container will hang for the sleep duration during termination, exemplifying how to manage graceful shutdowns in Kubernetes deployments.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      terminationGracePeriodSeconds: 120 # extra long grace period\\n      containers:\\n      - name: nginx\\n        image: nginx:latest\\n        ports:\\n        - containerPort: 80\\n        lifecycle:\\n          preStop:\\n            exec:\\n              # Real life termination may take any time up to terminationGracePeriodSeconds.\\n              # In this example - just hang around for at least the duration of terminationGracePeriodSeconds,\\n              # at 120 seconds container will be forcibly terminated.\\n              # Note, all this time nginx will keep processing requests.\\n              command: [\\n                \"/bin/sh\", \"-c\", \"sleep 180\"\\n              ]\\n', 'chunk': '1/1', 'summary': 'This content provides a Kubernetes deployment configuration for an Nginx server. It specifies creating a deployment named \"nginx-deployment\" with a single replica, which uses the latest Nginx image and exposes port 80. The deployment includes a lifecycle hook for the container\\'s preStop event, where it executes a command to sleep for 180 seconds before shutdown. This simulates a graceful termination period, allowing ongoing requests to complete while the container is shutting down, even though the specified termination grace period is only 120 seconds. The configuration ensures the container will hang for the sleep duration during termination, exemplifying how to manage graceful shutdowns in Kubernetes deployments.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\pod-with-graceful-termination.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('abc7d502-e261-5f88-93fe-c61ea06c70bb'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"sa-token-test\" that runs a single container using the \"busybox:1.28\" image. The container executes a long sleep command to keep it running. A volume named \"token-vol\" is mounted inside the container at \"/service-account\" as a read-only volume, which is projected from a service account token source. This token is configured with a specific audience (\"api\") and expiration time (3600 seconds). The purpose of this setup is to generate and mount a short-lived service account token inside the container, facilitating secure access to Kubernetes API or other services. This configuration is useful for testing or managing service account credentials within a pod.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: sa-token-test\\nspec:\\n  containers:\\n  - name: container-test\\n    image: busybox:1.28\\n    command: [\"sleep\", \"3600\"]\\n    volumeMounts:\\n    - name: token-vol\\n      mountPath: \"/service-account\"\\n      readOnly: true\\n  serviceAccountName: default\\n  volumes:\\n  - name: token-vol\\n    projected:\\n      sources:\\n      - serviceAccountToken:\\n          audience: api\\n          expirationSeconds: 3600\\n          path: token\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-service-account-token.yaml', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"sa-token-test\" that runs a single container using the \"busybox:1.28\" image. The container executes a long sleep command to keep it running. A volume named \"token-vol\" is mounted inside the container at \"/service-account\" as a read-only volume, which is projected from a service account token source. This token is configured with a specific audience (\"api\") and expiration time (3600 seconds). The purpose of this setup is to generate and mount a short-lived service account token inside the container, facilitating secure access to Kubernetes API or other services. This configuration is useful for testing or managing service account credentials within a pod.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('aca3279e-1344-5c1f-b469-161c742d752c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Secret object, specifically of type `kubernetes.io/dockercfg`. It is used to store Docker registry credentials securely within a Kubernetes cluster. The Secret has the name `secret-dockercfg`, and contains encoded data under the `.dockercfg` key. This data is a Base64-encoded string, which, when decoded, reveals Docker credentials necessary for authenticating against a Docker registry, such as for pulling container images. This setup allows Kubernetes to authenticate seamlessly with private Docker registries during image deployment.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret-dockercfg\\ntype: kubernetes.io/dockercfg\\ndata:\\n  .dockercfg: |\\n    eyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes Secret object, specifically of type `kubernetes.io/dockercfg`. It is used to store Docker registry credentials securely within a Kubernetes cluster. The Secret has the name `secret-dockercfg`, and contains encoded data under the `.dockercfg` key. This data is a Base64-encoded string, which, when decoded, reveals Docker credentials necessary for authenticating against a Docker registry, such as for pulling container images. This setup allows Kubernetes to authenticate seamlessly with private Docker registries during image deployment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\dockercfg-secret.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ad2a398b-bc25-5837-8338-f49c6069710a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"capacity-reservation\" with a single replica of a pod. The deployment uses labels and annotations for identification and descriptive purposes. The pod template specifies a container named \"pause\" that runs the \"registry.k8s.io/pause:3.6\" image, which is commonly used as a placeholder or to reserve capacity in Kubernetes. Resource requests and limits are set to ensure the container requests 50 millicores of CPU and 512 MiB of memory, with limits also at 512 MiB. The deployment incorporates affinity rules, specifically a pod anti-affinity configuration, which aims to distribute the overhead Pods across different nodes by avoiding placing similar pods on the same hostname, thus enhancing fault tolerance and resource distribution. This configuration facilitates capacity reservation and optimal placement strategies within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: capacity-reservation\\n  # You should decide what namespace to deploy this into\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: capacity-placeholder\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: capacity-placeholder\\n      annotations:\\n        kubernetes.io/description: \"Capacity reservation\"\\n    spec:\\n      priorityClassName: placeholder\\n      affinity: # Try to place these overhead Pods on different nodes\\n                # if possible\\n        podAntiAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                app: placeholder\\n            topologyKey: \"kubernetes.io/hostname\"\\n      containers:\\n      - name: pause\\n        image: registry.k8s.io/pause:3.6\\n        resources:\\n          requests:\\n            cpu: \"50m\"\\n            memory: \"512Mi\"\\n          limits:\\n            memory: \"512Mi\"\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"capacity-reservation\" with a single replica of a pod. The deployment uses labels and annotations for identification and descriptive purposes. The pod template specifies a container named \"pause\" that runs the \"registry.k8s.io/pause:3.6\" image, which is commonly used as a placeholder or to reserve capacity in Kubernetes. Resource requests and limits are set to ensure the container requests 50 millicores of CPU and 512 MiB of memory, with limits also at 512 MiB. The deployment incorporates affinity rules, specifically a pod anti-affinity configuration, which aims to distribute the overhead Pods across different nodes by avoiding placing similar pods on the same hostname, thus enhancing fault tolerance and resource distribution. This configuration facilitates capacity reservation and optimal placement strategies within a Kubernetes cluster.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-capacity-reservation.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ad735445-4f41-5b83-b4c0-405e2de48adf'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Namespace called \"my-privileged-namespace.\" It includes metadata labels that enforce security policies, specifically setting the pod security level to \"privileged\" and applying the latest security policy version. This configuration ensures that pods created within this namespace will have privileged access, granting them extensive permissions, which can be useful for certain high-trust applications but requires careful security considerations.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: my-privileged-namespace\\n  labels:\\n    pod-security.kubernetes.io/enforce: privileged\\n    pod-security.kubernetes.io/enforce-version: latest', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Namespace called \"my-privileged-namespace.\" It includes metadata labels that enforce security policies, specifically setting the pod security level to \"privileged\" and applying the latest security policy version. This configuration ensures that pods created within this namespace will have privileged access, granting them extensive permissions, which can be useful for certain high-trust applications but requires careful security considerations.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-privileged.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('addecbcf-b943-50dc-9edd-e133d227ca3f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages two replicas (pods) running an Nginx server. The deployment uses labels to manage the pods and specifies the container image \"nginx:1.14.2\", exposing port 80 for web traffic. This configuration automates the deployment, scaling, and management of multiple Nginx instances within a Kubernetes cluster, ensuring high availability and load balancing.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 2 # tells deployment to run 2 pods matching the template\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages two replicas (pods) running an Nginx server. The deployment uses labels to manage the pods and specifies the container image \"nginx:1.14.2\", exposing port 80 for web traffic. This configuration automates the deployment, scaling, and management of multiple Nginx instances within a Kubernetes cluster, ensuring high availability and load balancing.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('aea27f3f-d4d3-5a21-9efa-d283ed5e15af'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ValidatingAdmissionPolicy resource that enforces custom validation rules during the creation or update of deployment resources. The policy, named \"replicalimit-policy.example.com,\" uses a custom parameter kind \"ReplicaLimit\" to impose replica count constraints. It operates with a failure policy set to Fail, meaning invalid requests will be rejected. The policy targets \"deployments\" within the \"apps\" API group and \"v1\" version, specifically during CREATE and UPDATE operations. \\n\\nThe validation rule checks whether the number of replicas specified in a deployment (\"object.spec.replicas\") is less than or equal to a maximum threshold defined by the custom parameter. If the condition fails, the request is invalid, and an appropriate reason \"Invalid\" is provided. This policy helps enforce resource limits and maintain stability by preventing excessive replica deployments.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"replicalimit-policy.example.com\"\\nspec:\\n  failurePolicy: Fail\\n  paramKind:\\n    apiVersion: rules.example.com/v1\\n    kind: ReplicaLimit\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  validations:\\n    - expression: \"object.spec.replicas <= params.maxReplicas\"\\n      reason: Invalid', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ValidatingAdmissionPolicy resource that enforces custom validation rules during the creation or update of deployment resources. The policy, named \"replicalimit-policy.example.com,\" uses a custom parameter kind \"ReplicaLimit\" to impose replica count constraints. It operates with a failure policy set to Fail, meaning invalid requests will be rejected. The policy targets \"deployments\" within the \"apps\" API group and \"v1\" version, specifically during CREATE and UPDATE operations. \\n\\nThe validation rule checks whether the number of replicas specified in a deployment (\"object.spec.replicas\") is less than or equal to a maximum threshold defined by the custom parameter. If the condition fails, the request is invalid, and an appropriate reason \"Invalid\" is provided. This policy helps enforce resource limits and maintain stability by preventing excessive replica deployments.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\policy-with-param.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('aee41855-0166-56e6-b62c-e5a64e99d78a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"fine-pod\" with specific security settings and a single container. The pod\\'s securityContext includes a seccompProfile of type \"Localhost,\" referencing a custom profile located at \"profiles/fine-grained.json,\" which enhances container security by restricting system calls. The container uses the \"hashicorp/http-echo:1.0\" image and is set to display the message \"just made some syscalls!\" when run, with privilege escalation disabled under the securityContext. This setup demonstrates how to enforce fine-grained security policies in a Kubernetes environment through seccomp profiles.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fine-pod\\n  labels:\\n    app: fine-pod\\nspec:\\n  securityContext:\\n    seccompProfile:\\n      type: Localhost\\n      localhostProfile: profiles/fine-grained.json\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:1.0\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"fine-pod\" with specific security settings and a single container. The pod\\'s securityContext includes a seccompProfile of type \"Localhost,\" referencing a custom profile located at \"profiles/fine-grained.json,\" which enhances container security by restricting system calls. The container uses the \"hashicorp/http-echo:1.0\" image and is set to display the message \"just made some syscalls!\" when run, with privilege escalation disabled under the securityContext. This setup demonstrates how to enforce fine-grained security policies in a Kubernetes environment through seccomp profiles.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\fine-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b058d62f-a873-526f-8d70-bc976d1fef70'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided configuration defines a Kubernetes setup with a headless Service and a StatefulSet for deploying NGINX web servers. The Service, named \"nginx,\" exposes port 80 and uses a label selector to associate with the pods, enabling network communication within the cluster without a ClusterIP. The StatefulSet named \"web\" manages two replicas of an NGINX container based on the specified image, ensuring each pod has a stable network identity and persistent storage. Each pod mounts a PersistentVolumeClaim (PVC) named \"www,\" requesting 1Gi of storage with read-write once access mode, to serve or store web content. The overall design ensures scalable, stable, and data-persistent deployment of NGINX instances in a Kubernetes environment.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx\\n  labels:\\n    app: nginx\\nspec:\\n  ports:\\n  - port: 80\\n    name: web\\n  clusterIP: None\\n  selector:\\n    app: nginx\\n---\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: web\\nspec:\\n  serviceName: \"nginx\"\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: registry.k8s.io/nginx-slim:0.21\\n        ports:\\n        - containerPort: 80\\n          name: web\\n        volumeMounts:\\n        - name: www\\n          mountPath: /usr/share/nginx/html\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: www\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      resources:\\n        requests:\\n          storage: 1Gi\\n\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web.yaml', 'summary': 'The provided configuration defines a Kubernetes setup with a headless Service and a StatefulSet for deploying NGINX web servers. The Service, named \"nginx,\" exposes port 80 and uses a label selector to associate with the pods, enabling network communication within the cluster without a ClusterIP. The StatefulSet named \"web\" manages two replicas of an NGINX container based on the specified image, ensuring each pod has a stable network identity and persistent storage. Each pod mounts a PersistentVolumeClaim (PVC) named \"www,\" requesting 1Gi of storage with read-write once access mode, to serve or store web content. The overall design ensures scalable, stable, and data-persistent deployment of NGINX instances in a Kubernetes environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b0fc5bc0-985e-5ed5-a54d-51897b8ddadf'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"counter-err\" with a single container. The container uses the \"busybox:1.28\" image and runs a shell script as its command. This script initializes a counter \"i\" at zero and runs an infinite loop that, every second, outputs the current count and timestamp to standard output, while simultaneously sending an error message with the same counter to standard error. This setup is useful for testing logging, stdout and stderr handling, or monitoring setups within Kubernetes environments.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: counter-err\\nspec:\\n  containers:\\n  - name: count\\n    image: busybox:1.28\\n    args: [/bin/sh, -c,\\n            \\'i=0; while true; do echo \"$i: $(date)\"; echo \"$i: err\" >&2 ; i=$((i+1)); sleep 1; done\\']\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"counter-err\" with a single container. The container uses the \"busybox:1.28\" image and runs a shell script as its command. This script initializes a counter \"i\" at zero and runs an infinite loop that, every second, outputs the current count and timestamp to standard output, while simultaneously sending an error message with the same counter to standard error. This setup is useful for testing logging, stdout and stderr handling, or monitoring setups within Kubernetes environments.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\counter-pod-err.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b116c406-63f7-5baa-bf38-32b90d45e73d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Deployment configuration with two containers sharing a volume and using a ConfigMap. The deployment, named \"configmap-two-containers,\" creates three replicas of a pod, each containing an nginx server and an alpine container. The nginx container mounts a shared emptyDir volume at \"/usr/share/nginx/html\" to serve static content. The alpine container, running a shell, mounts the same shared volume at \"/pod-data\" for writing output, and a ConfigMap named \"color\" at \"/etc/config\" to access configuration data. The alpine container executes a continuous loop that writes the current date and a message indicating the preferred color (from the ConfigMap) into the file \"/pod-data/index.html\" every 10 seconds. This setup enables dynamic content updates and data sharing between containers within the pod.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-two-containers\\n  labels:\\n    app.kubernetes.io/name: configmap-two-containers\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-two-containers\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-two-containers\\n    spec:\\n      volumes:\\n        - name: shared-data\\n          emptyDir: {}\\n        - name: config-volume\\n          configMap:\\n            name: color\\n      containers:\\n        - name: nginx\\n          image: nginx\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /usr/share/nginx/html\\n        - name: alpine\\n          image: alpine:3\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /pod-data\\n            - name: config-volume\\n              mountPath: /etc/config\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) My preferred color is $(cat /etc/config/color)\" > /pod-data/index.html;\\n              sleep 10; done;\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Deployment configuration with two containers sharing a volume and using a ConfigMap. The deployment, named \"configmap-two-containers,\" creates three replicas of a pod, each containing an nginx server and an alpine container. The nginx container mounts a shared emptyDir volume at \"/usr/share/nginx/html\" to serve static content. The alpine container, running a shell, mounts the same shared volume at \"/pod-data\" for writing output, and a ConfigMap named \"color\" at \"/etc/config\" to access configuration data. The alpine container executes a continuous loop that writes the current date and a message indicating the preferred color (from the ConfigMap) into the file \"/pod-data/index.html\" every 10 seconds. This setup enables dynamic content updates and data sharing between containers within the pod.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-two-containers.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b1aee545-b122-5332-a2b2-b8930d5a71d4'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes configuration defining a headless Service and a StatefulSet for deploying nginx web servers. The Service exposes port 80 and is labeled for app identification, with clusterIP set to None to create a headless service that enables direct Pod communication and stable network identities. The StatefulSet manages two nginx Pods with parallel deployment, each linked to the Service and configured to run a specific nginx image. Each Pod mounts a persistent volume for storing web content, ensuring data persistence across Pod restarts. The volume claim template allocates 1 GiB of storage per Pod, enabling reliable and scalable web server deployment with stable network identities, ideal for stateful applications like web servers or databases.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx\\n  labels:\\n    app: nginx\\nspec:\\n  ports:\\n  - port: 80\\n    name: web\\n  clusterIP: None\\n  selector:\\n    app: nginx\\n---\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: web\\nspec:\\n  serviceName: \"nginx\"\\n  podManagementPolicy: \"Parallel\"\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: registry.k8s.io/nginx-slim:0.24\\n        ports:\\n        - containerPort: 80\\n          name: web\\n        volumeMounts:\\n        - name: www\\n          mountPath: /usr/share/nginx/html\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: www\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      resources:\\n        requests:\\n          storage: 1Gi\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web-parallel.yaml', 'summary': 'The provided content is a Kubernetes configuration defining a headless Service and a StatefulSet for deploying nginx web servers. The Service exposes port 80 and is labeled for app identification, with clusterIP set to None to create a headless service that enables direct Pod communication and stable network identities. The StatefulSet manages two nginx Pods with parallel deployment, each linked to the Service and configured to run a specific nginx image. Each Pod mounts a persistent volume for storing web content, ensuring data persistence across Pod restarts. The volume claim template allocates 1 GiB of storage per Pod, enabling reliable and scalable web server deployment with stable network identities, ideal for stateful applications like web servers or databases.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b21bbaac-6b0a-576b-a3c8-6f67c95f5b1d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod specification. It creates a pod named \"example-no-conflict-with-limitrange-cpu\" with a single container named \"demo\" that uses the \"pause:3.8\" image from the Kubernetes container registry. The resource requests and limits for CPU are set to 700 millicores (0.7 CPU). This configuration ensures that the container requests a specific amount of CPU resources and enforces a limit to prevent it from consuming more, promoting efficient resource management and avoiding contention with other pods.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: example-no-conflict-with-limitrange-cpu\\nspec:\\n  containers:\\n  - name: demo\\n    image: registry.k8s.io/pause:3.8\\n    resources:\\n      requests:\\n        cpu: 700m\\n      limits:\\n        cpu: 700m\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-no-conflict-with-limitrange-cpu.yaml', 'summary': 'This YAML configuration defines a Kubernetes Pod specification. It creates a pod named \"example-no-conflict-with-limitrange-cpu\" with a single container named \"demo\" that uses the \"pause:3.8\" image from the Kubernetes container registry. The resource requests and limits for CPU are set to 700 millicores (0.7 CPU). This configuration ensures that the container requests a specific amount of CPU resources and enforces a limit to prevent it from consuming more, promoting efficient resource management and avoiding contention with other pods.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b287e2d9-e365-573b-8316-d64e08e81911'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# This content defines a Kubernetes Pod configuration that includes a Redis container with a TCP socket health check. The configuration specifies a `livenessProbe` which uses a TCP socket probe targeting port 6379 to verify the container's health. The probe is set to start 30 seconds after the container begins and has a 1-second timeout for the response. This setup ensures that Kubernetes can monitor the Redis container’s health effectively by checking if the port is open and responsive, helping to restart the container if it becomes unresponsive or fails.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-tcp-socket-healthcheck\\nspec:\\n  containers:\\n  - name: redis\\n    image: redis\\n    # defines the health checking\\n    livenessProbe:\\n      # a TCP socket probe\\n      tcpSocket:\\n        port: 6379\\n      # length of time to wait for a pod to initialize\\n      # after pod startup, before applying health checking\\n      initialDelaySeconds: 30\\n      timeoutSeconds: 1\\n    ports:\\n    - containerPort: 6379\\n\", 'subchunk': '1/1', 'summary': \"This content defines a Kubernetes Pod configuration that includes a Redis container with a TCP socket health check. The configuration specifies a `livenessProbe` which uses a TCP socket probe targeting port 6379 to verify the container's health. The probe is set to start 30 seconds after the container begins and has a 1-second timeout for the response. This setup ensures that Kubernetes can monitor the Redis container’s health effectively by checking if the port is open and responsive, helping to restart the container if it becomes unresponsive or fails.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\pod-with-tcp-socket-healthcheck.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b438484c-906c-5131-adc8-54a8f38e1a7c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes manifest defines a Deployment named \"configmap-volume\" that manages three replica pods. These pods run Alpine Linux containers with a command that continuously prints the current date along with the value of a configuration parameter called \"sport\" every 10 seconds. The container mounts a ConfigMap named \"sport\" as a volume at the path \"/etc/config.\" This setup enables the container to access configuration data stored in the ConfigMap, which can be dynamically updated without redeploying the pods. The key aspect of this configuration is the use of volumes linked to ConfigMaps, allowing for flexible and centralized configuration management in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-volume\\n  labels:\\n    app.kubernetes.io/name: configmap-volume\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-volume\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-volume\\n    spec:\\n      containers:\\n        - name: alpine\\n          image: alpine:3\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) My preferred sport is $(cat /etc/config/sport)\";\\n              sleep 10; done;\\n          ports:\\n            - containerPort: 80\\n          volumeMounts:\\n            - name: config-volume\\n              mountPath: /etc/config\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: sport', 'subchunk': '1/1', 'summary': 'This Kubernetes manifest defines a Deployment named \"configmap-volume\" that manages three replica pods. These pods run Alpine Linux containers with a command that continuously prints the current date along with the value of a configuration parameter called \"sport\" every 10 seconds. The container mounts a ConfigMap named \"sport\" as a volume at the path \"/etc/config.\" This setup enables the container to access configuration data stored in the ConfigMap, which can be dynamically updated without redeploying the pods. The key aspect of this configuration is the use of volumes linked to ConfigMaps, allowing for flexible and centralized configuration management in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-volume.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b54c6f22-ced3-5a6d-84ba-890c4cdd743f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Pod configuration in YAML format. The Pod is named \"qos-demo-4\" and resides in the \"qos-example\" namespace. It includes two containers: one running Nginx with a specified resource request for 200Mi of memory, and another running Redis without specific resource constraints. This setup demonstrates resource management and Quality of Service (QoS) classes in Kubernetes, where the first container\\'s memory request influences its QoS classification, potentially affecting its priority during resource contention.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: qos-demo-4\\n  namespace: qos-example\\nspec:\\n  containers:\\n\\n  - name: qos-demo-4-ctr-1\\n    image: nginx\\n    resources:\\n      requests:\\n        memory: \"200Mi\"\\n\\n  - name: qos-demo-4-ctr-2\\n    image: redis\\n', 'subchunk': '1/1', 'summary': 'This content describes a Kubernetes Pod configuration in YAML format. The Pod is named \"qos-demo-4\" and resides in the \"qos-example\" namespace. It includes two containers: one running Nginx with a specified resource request for 200Mi of memory, and another running Redis without specific resource constraints. This setup demonstrates resource management and Quality of Service (QoS) classes in Kubernetes, where the first container\\'s memory request influences its QoS classification, potentially affecting its priority during resource contention.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-4.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b818e1fa-5737-574b-9ad9-2861d00d3ff2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content demonstrates how to use a Kubernetes ConfigMap to manage configuration data for a pod. The ConfigMap, named \"example-config,\" stores key-value pairs (\"example.property.1\" as \"hello\" and \"example.property.2\" as \"world\"). The pod, named \"configmap-pod,\" runs a Redis container with an image specified for Windows nanoserver. The environment variables within the container are set dynamically by referencing the ConfigMap keys, allowing the container to access configuration data in a decoupled manner, which enhances flexibility and maintainability. The pod is scheduled on Windows nodes using the node selector. Overall, this setup illustrates best practices for externalizing configuration in Kubernetes and injecting it into containers at runtime.\\nkind: ConfigMap\\napiVersion: v1\\nmetadata:\\n  name: example-config\\ndata:\\n  example.property.1: hello\\n  example.property.2: world\\n\\n---\\n\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: configmap-pod\\nspec:\\n  containers:\\n  - name: configmap-redis\\n    image: redis:3.0-nanoserver\\n    env:\\n      - name: EXAMPLE_PROPERTY_1\\n        valueFrom:\\n          configMapKeyRef:\\n            name: example-config\\n            key: example.property.1\\n      - name: EXAMPLE_PROPERTY_2\\n        valueFrom:\\n          configMapKeyRef:\\n            name: example-config\\n            key: example.property.2\\n  nodeSelector:\\n    kubernetes.io/os: windows', 'chunk': '1/1', 'summary': 'This content demonstrates how to use a Kubernetes ConfigMap to manage configuration data for a pod. The ConfigMap, named \"example-config,\" stores key-value pairs (\"example.property.1\" as \"hello\" and \"example.property.2\" as \"world\"). The pod, named \"configmap-pod,\" runs a Redis container with an image specified for Windows nanoserver. The environment variables within the container are set dynamically by referencing the ConfigMap keys, allowing the container to access configuration data in a decoupled manner, which enhances flexibility and maintainability. The pod is scheduled on Windows nodes using the node selector. Overall, this setup illustrates best practices for externalizing configuration in Kubernetes and injecting it into containers at runtime.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\configmap-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b8e9bc47-90cd-5559-8c10-5cee012a7314'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Ingress resource configuration written in YAML. It defines an Ingress named \"minimal-ingress\" that manages external access to services within a Kubernetes cluster. The configuration specifies use of the nginx ingress controller (via the `ingressClassName`) and includes an annotation to rewrite request URLs to the root path. The core of the spec is the rule that directs HTTP requests with the path \"/testpath\" to the backend service named \"test\" on port 80. Essentially, this setup enables external traffic reaching \"/testpath\" on the cluster\\'s ingress point to be forwarded to the \"test\" service internally, with URL rewriting applied to simplify routing.\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: minimal-ingress\\n  annotations:\\n    nginx.ingress.kubernetes.io/rewrite-target: /\\nspec:\\n  ingressClassName: nginx-example\\n  rules:\\n  - http:\\n      paths:\\n      - path: /testpath\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: test\\n            port:\\n              number: 80\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Ingress resource configuration written in YAML. It defines an Ingress named \"minimal-ingress\" that manages external access to services within a Kubernetes cluster. The configuration specifies use of the nginx ingress controller (via the `ingressClassName`) and includes an annotation to rewrite request URLs to the root path. The core of the spec is the rule that directs HTTP requests with the path \"/testpath\" to the backend service named \"test\" on port 80. Essentially, this setup enables external traffic reaching \"/testpath\" on the cluster\\'s ingress point to be forwarded to the \"test\" service internally, with URL rewriting applied to simplify routing.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\minimal-ingress.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('b90956af-f8b4-5b1a-b680-a5924bc763a7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with the API version v1. The Pod is named \"default-cpu-demo\" and contains a single container named \"default-cpu-demo-ctr\" that runs the Nginx web server image. The configuration is straightforward, specifying how to deploy a simple Nginx container within a Pod in a Kubernetes environment. It serves as a fundamental example of creating a Pod with an embedded container for hosting web services.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-cpu-demo\\nspec:\\n  containers:\\n  - name: default-cpu-demo-ctr\\n    image: nginx\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod with the API version v1. The Pod is named \"default-cpu-demo\" and contains a single container named \"default-cpu-demo-ctr\" that runs the Nginx web server image. The configuration is straightforward, specifying how to deploy a simple Nginx container within a Pod in a Kubernetes environment. It serves as a fundamental example of creating a Pod with an embedded container for hosting web services.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ba7b7661-a3af-5c8f-8c6b-1c486a438e5b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes RoleBinding resource that grants specific permissions to a user within a particular namespace. It links the user \"dave\" to a ClusterRole named \"secret-reader,\" allowing him to read secrets only in the \"development\" namespace. The RoleBinding is scoped to the namespace \"development,\" meaning the permissions are limited to that environment. This setup is useful for fine-grained access control, ensuring users can only perform actions permitted by the associated ClusterRole in specified namespaces. The overall purpose is to securely delegate secret reading privileges to individual users within a controlled scope.\\napiVersion: rbac.authorization.k8s.io/v1\\n# This role binding allows \"dave\" to read secrets in the \"development\" namespace.\\n# You need to already have a ClusterRole named \"secret-reader\".\\nkind: RoleBinding\\nmetadata:\\n  name: read-secrets\\n  #\\n  # The namespace of the RoleBinding determines where the permissions are granted.\\n  # This only grants permissions within the \"development\" namespace.\\n  namespace: development\\nsubjects:\\n- kind: User\\n  name: dave # Name is case sensitive\\n  apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  kind: ClusterRole\\n  name: secret-reader\\n  apiGroup: rbac.authorization.k8s.io\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes RoleBinding resource that grants specific permissions to a user within a particular namespace. It links the user \"dave\" to a ClusterRole named \"secret-reader,\" allowing him to read secrets only in the \"development\" namespace. The RoleBinding is scoped to the namespace \"development,\" meaning the permissions are limited to that environment. This setup is useful for fine-grained access control, ensuring users can only perform actions permitted by the associated ClusterRole in specified namespaces. The overall purpose is to securely delegate secret reading privileges to individual users within a controlled scope.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-rolebinding-with-clusterrole.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ba9e74f2-3680-5c05-95ee-8842ee304779'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes YAML configuration defining a simple Pod resource. The Pod is named \"no-annotation\" and includes labels for identification. It specifies a single container that uses the \"pause\" image from the Kubernetes registry, which is typically used as a placeholder or for testing purposes. The configuration does not include annotations, focusing solely on basic metadata and container setup.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: no-annotation\\n  labels:\\n    name: multischeduler-example\\nspec:\\n  containers:\\n  - name: pod-with-no-annotation-container\\n    image: registry.k8s.io/pause:3.8', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes YAML configuration defining a simple Pod resource. The Pod is named \"no-annotation\" and includes labels for identification. It specifies a single container that uses the \"pause\" image from the Kubernetes registry, which is typically used as a placeholder or for testing purposes. The configuration does not include annotations, focusing solely on basic metadata and container setup.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod1.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bac02487-aa9d-55d0-b73e-4a1ea6a09bb3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content comprises two Kubernetes resource definitions: a Secret and a Pod. The Secret, named \"mysecret,\" is of type \"Opaque\" and securely stores encoded data for a username and password. The data fields contain base64-encoded strings that represent the credentials. The Pod configuration specifies a Windows-based container running the \"microsoft/windowsservercore:1709\" image. It defines environment variables \"USERNAME\" and \"PASSWORD,\" which are populated by referencing the keys from the \"mysecret\" Secret resource, ensuring sensitive data remains protected and not hard-coded. The Pod is scheduled on a Windows node, as indicated by the node selector. This setup demonstrates how to securely inject secret data into containers using Kubernetes Secrets and environment variable references.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: mysecret\\ntype: Opaque\\ndata:\\n  username: YWRtaW4=\\n  password: MWYyZDFlMmU2N2Rm\\n\\n---\\n\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: my-secret-pod\\nspec:\\n  containers:\\n  - name: my-secret-pod\\n    image: microsoft/windowsservercore:1709\\n    env:\\n      - name: USERNAME\\n        valueFrom:\\n          secretKeyRef:\\n            name: mysecret\\n            key: username\\n      - name: PASSWORD\\n        valueFrom:\\n          secretKeyRef:\\n            name: mysecret\\n            key: password\\n  nodeSelector:\\n    kubernetes.io/os: windows\\n', 'subchunk': '1/1', 'summary': 'The provided content comprises two Kubernetes resource definitions: a Secret and a Pod. The Secret, named \"mysecret,\" is of type \"Opaque\" and securely stores encoded data for a username and password. The data fields contain base64-encoded strings that represent the credentials. The Pod configuration specifies a Windows-based container running the \"microsoft/windowsservercore:1709\" image. It defines environment variables \"USERNAME\" and \"PASSWORD,\" which are populated by referencing the keys from the \"mysecret\" Secret resource, ensuring sensitive data remains protected and not hard-coded. The Pod is scheduled on a Windows node, as indicated by the node selector. This setup demonstrates how to securely inject secret data into containers using Kubernetes Secrets and environment variable references.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\secret-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bac3178f-660e-5799-9a7e-bef9355f1891'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment object named \"backend\" that manages three replicas of a containerized application. The deployment uses a label selector to identify its pods, which are labeled with \"app: hello\", \"tier: backend\", and \"track: stable\". The pod template specifies the container details, including the container name \"hello\" and the image sourced from Google Container Registry (\"gcr.io/google-samples/hello-go-gke:1.0\"). The container exposes port 80 for HTTP traffic. Overall, this configuration automates the deployment and scaling of a simple backend service in a Kubernetes cluster.\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: backend\\nspec:\\n  selector:\\n    matchLabels:\\n      app: hello\\n      tier: backend\\n      track: stable\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: hello\\n        tier: backend\\n        track: stable\\n    spec:\\n      containers:\\n        - name: hello\\n          image: \"gcr.io/google-samples/hello-go-gke:1.0\"\\n          ports:\\n            - name: http\\n              containerPort: 80\\n...', 'chunk': '1/1', 'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment object named \"backend\" that manages three replicas of a containerized application. The deployment uses a label selector to identify its pods, which are labeled with \"app: hello\", \"tier: backend\", and \"track: stable\". The pod template specifies the container details, including the container name \"hello\" and the image sourced from Google Container Registry (\"gcr.io/google-samples/hello-go-gke:1.0\"). The container exposes port 80 for HTTP traffic. Overall, this configuration automates the deployment and scaling of a simple backend service in a Kubernetes cluster.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-deployment.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bafc400e-2bdb-59ec-a7f6-3693a72f6db7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This code defines a Kubernetes Pod configuration using YAML. It specifies the creation of a Pod named \"qos-demo-5\" within the \"qos-example\" namespace, containing a single container running the \"nginx\" image. The resources section sets both resource requests and limits for CPU and memory, with each set to 200Mi (memory) and 700m (CPU). This configuration illustrates how to specify resource reservations (requests) and limits to manage the Quality of Service (QoS) for the container, ensuring predictable performance and resource allocation within the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: qos-demo-5\\n  namespace: qos-example\\nspec:\\n  containers:\\n  - name: qos-demo-ctr-5\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"700m\"\\n      requests:\\n        memory: \"200Mi\"\\n        cpu: \"700m\"\\n', 'chunk': '1/1', 'summary': 'This code defines a Kubernetes Pod configuration using YAML. It specifies the creation of a Pod named \"qos-demo-5\" within the \"qos-example\" namespace, containing a single container running the \"nginx\" image. The resources section sets both resource requests and limits for CPU and memory, with each set to 200Mi (memory) and 700m (CPU). This configuration illustrates how to specify resource reservations (requests) and limits to manage the Quality of Service (QoS) for the container, ensuring predictable performance and resource allocation within the cluster.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-5.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bb2008d5-e704-5bc0-944d-a6b06117427c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration creates a Kubernetes Pod that demonstrates how to utilize ConfigMaps for managing environment variables and configuration files. The Pod contains a single container running an Alpine image, which executes a sleep command to keep it active. Environment variables within the container are populated by referencing specific keys from a ConfigMap named \"game-demo,\" effectively decoupling configuration data from container images. Additionally, a volume is mounted at \"/config,\" sourcing its data from the same ConfigMap, and selectively exposes certain keys (\"game.properties\" and \"user-interface.properties\") as files within the container. This setup exemplifies best practices for injecting configuration data into containers, enabling more flexible and manageable application deployments.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: configmap-demo-pod\\nspec:\\n  containers:\\n    - name: demo\\n      image: alpine\\n      command: [\"sleep\", \"3600\"]\\n      env:\\n        # Define the environment variable\\n        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here\\n                                     # from the key name in the ConfigMap.\\n          valueFrom:\\n            configMapKeyRef:\\n              name: game-demo           # The ConfigMap this value comes from.\\n              key: player_initial_lives # The key to fetch.\\n        - name: UI_PROPERTIES_FILE_NAME\\n          valueFrom:\\n            configMapKeyRef:\\n              name: game-demo\\n              key: ui_properties_file_name\\n      volumeMounts:\\n      - name: config\\n        mountPath: \"/config\"\\n        readOnly: true\\n  volumes:\\n  # You set volumes at the Pod level, then mount them into containers inside that Pod\\n  - name: config\\n    configMap:\\n      # Provide the name of the ConfigMap you want to mount.\\n      name: game-demo\\n      # An array of keys from the ConfigMap to create as files\\n      items:\\n      - key: \"game.properties\"\\n        path: \"game.properties\"\\n      - key: \"user-interface.properties\"\\n        path: \"user-interface.properties\"\\n        ', 'subchunk': '1/1', 'summary': 'The provided YAML configuration creates a Kubernetes Pod that demonstrates how to utilize ConfigMaps for managing environment variables and configuration files. The Pod contains a single container running an Alpine image, which executes a sleep command to keep it active. Environment variables within the container are populated by referencing specific keys from a ConfigMap named \"game-demo,\" effectively decoupling configuration data from container images. Additionally, a volume is mounted at \"/config,\" sourcing its data from the same ConfigMap, and selectively exposes certain keys (\"game.properties\" and \"user-interface.properties\") as files within the container. This setup exemplifies best practices for injecting configuration data into containers, enabling more flexible and manageable application deployments.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configure-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bb211f9d-2a02-5a9d-86cf-b6125d61b8e2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with the name \"nginx.\" The Pod contains a single container that runs the nginx web server, using version 1.14.2 of the nginx image from a container registry. The container listens on port 80, which is the default port for HTTP traffic. This simple setup is used to deploy a basic web server inside a Kubernetes cluster, allowing it to serve web pages or act as a reverse proxy.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  containers:\\n  - name: nginx\\n    image: nginx:1.14.2\\n    ports:\\n    - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod with the name \"nginx.\" The Pod contains a single container that runs the nginx web server, using version 1.14.2 of the nginx image from a container registry. The container listens on port 80, which is the default port for HTTP traffic. This simple setup is used to deploy a basic web server inside a Kubernetes cluster, allowing it to serve web pages or act as a reverse proxy.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\simple-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bb72eb18-1da1-5c5d-b413-88eb53c69fc2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration that combines scheduling constraints and affinity rules to control pod placement. It specifies a pod named \"mypod\" with a label \"foo: bar\" and one container running a pause image, typically used for testing or as a placeholder.\\n\\nThe configuration includes **topology spread constraints**, which aim to evenly distribute pods across different zones to prevent clustering. Specifically, it sets a maximum skew of 1, meaning there should be at most one more pod in one zone compared to another, and applies this rule only to zones matching the label \"foo: bar.\" The **whenUnsatisfiable** setting \"DoNotSchedule\" prevents scheduling if the distribution cannot be achieved.\\n\\nAdditionally, **node affinity** ensures the pod is scheduled on nodes outside \"zoneC\" using a requiredDuringScheduling rule with a matchExpressions condition. This enforces placement on preferred zones, avoiding certain zones based on the node labels.\\n\\nOverall, this configuration demonstrates advanced scheduling policies to control pod distribution across zones and nodes, ensuring high availability and fault tolerance.\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: mypod\\n  labels:\\n    foo: bar\\nspec:\\n  topologySpreadConstraints:\\n  - maxSkew: 1\\n    topologyKey: zone\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:\\n        - matchExpressions:\\n          - key: zone\\n            operator: NotIn\\n            values:\\n            - zoneC\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.1', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration that combines scheduling constraints and affinity rules to control pod placement. It specifies a pod named \"mypod\" with a label \"foo: bar\" and one container running a pause image, typically used for testing or as a placeholder.\\n\\nThe configuration includes **topology spread constraints**, which aim to evenly distribute pods across different zones to prevent clustering. Specifically, it sets a maximum skew of 1, meaning there should be at most one more pod in one zone compared to another, and applies this rule only to zones matching the label \"foo: bar.\" The **whenUnsatisfiable** setting \"DoNotSchedule\" prevents scheduling if the distribution cannot be achieved.\\n\\nAdditionally, **node affinity** ensures the pod is scheduled on nodes outside \"zoneC\" using a requiredDuringScheduling rule with a matchExpressions condition. This enforces placement on preferred zones, avoiding certain zones based on the node labels.\\n\\nOverall, this configuration demonstrates advanced scheduling policies to control pod distribution across zones and nodes, ensuring high availability and fault tolerance.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint-with-nodeaffinity.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bcac386a-7c5b-5baa-a16f-c219685394f1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes YAML configuration that defines a Service resource named \"frontend.\" The Service is configured to select Pods with specific labels (\"app: hello\" and \"tier: frontend\") and exposes them through port 80 using TCP protocol. Its type is set to \"LoadBalancer,\" meaning it will provision an external load balancer to distribute incoming traffic to the selected Pods, making the application accessible from outside the cluster. This setup is typically used to expose frontend applications to users in a scalable and reliable manner.\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: frontend\\nspec:\\n  selector:\\n    app: hello\\n    tier: frontend\\n  ports:\\n  - protocol: \"TCP\"\\n    port: 80\\n    targetPort: 80\\n  type: LoadBalancer\\n...', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-service.yaml', 'summary': 'This content is a Kubernetes YAML configuration that defines a Service resource named \"frontend.\" The Service is configured to select Pods with specific labels (\"app: hello\" and \"tier: frontend\") and exposes them through port 80 using TCP protocol. Its type is set to \"LoadBalancer,\" meaning it will provision an external load balancer to distribute incoming traffic to the selected Pods, making the application accessible from outside the cluster. This setup is typically used to expose frontend applications to users in a scalable and reliable manner.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bd45b6bd-34a1-522d-bbeb-f4e2dbaabd29'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content consists of two Kubernetes ConfigMap resource definitions. These ConfigMaps are used to store configuration data as key-value pairs that can be consumed by applications running within the cluster. The first ConfigMap, named \"special-config,\" contains a single key \"special.how\" with the value \"very.\" The second ConfigMap, named \"env-config,\" includes a key \"log_level\" set to \"INFO.\" These ConfigMaps enable decoupling configuration from application code, making it easier to manage and update settings without modifying the container images.\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: special-config\\n  namespace: default\\ndata:\\n  special.how: very\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: env-config\\n  namespace: default\\ndata:\\n  log_level: INFO\\n', 'subchunk': '1/1', 'summary': 'The provided content consists of two Kubernetes ConfigMap resource definitions. These ConfigMaps are used to store configuration data as key-value pairs that can be consumed by applications running within the cluster. The first ConfigMap, named \"special-config,\" contains a single key \"special.how\" with the value \"very.\" The second ConfigMap, named \"env-config,\" includes a key \"log_level\" set to \"INFO.\" These ConfigMaps enable decoupling configuration from application code, making it easier to manage and update settings without modifying the container images.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configmaps.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bd59e110-9454-5db5-9c8b-fc8f159544e7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format, focusing on security contexts. It specifies a Pod named \"security-context-demo\" with a global security context that sets user and group IDs (`runAsUser: 1000`, `runAsGroup: 3000`) and additional groups (`supplementalGroups: [4000]`) with a strict policy. The container within the Pod, \"sec-ctx-demo,\" uses the \"agnhost\" image and runs a simple command to sleep for an hour. The container\\'s security context disallows privilege escalation (`allowPrivilegeEscalation: false`). Overall, the configuration demonstrates the setup of security settings at both Pod and container levels to enforce security best practices in a Kubernetes environment, such as defining user permissions and restricting privilege escalation.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo\\nspec:\\n  securityContext:\\n    runAsUser: 1000\\n    runAsGroup: 3000\\n    supplementalGroups: [4000]\\n    supplementalGroupsPolicy: Strict\\n  containers:\\n  - name: sec-ctx-demo\\n    image: registry.k8s.io/e2e-test-images/agnhost:2.45\\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\\n    securityContext:\\n      allowPrivilegeEscalation: false\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Pod configuration in YAML format, focusing on security contexts. It specifies a Pod named \"security-context-demo\" with a global security context that sets user and group IDs (`runAsUser: 1000`, `runAsGroup: 3000`) and additional groups (`supplementalGroups: [4000]`) with a strict policy. The container within the Pod, \"sec-ctx-demo,\" uses the \"agnhost\" image and runs a simple command to sleep for an hour. The container\\'s security context disallows privilege escalation (`allowPrivilegeEscalation: false`). Overall, the configuration demonstrates the setup of security settings at both Pod and container levels to enforce security best practices in a Kubernetes environment, such as defining user permissions and restricting privilege escalation.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-6.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bdb4d4a6-2f5b-5f59-8397-10a648b0081a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod that demonstrates the use of the Downward API to expose container resource requests and limits as files within a volume. The Pod contains a single container running BusyBox, which executes a shell script in a continuous loop. This script reads and prints resource information—such as CPU and memory requests and limits—from specific files mounted at `/etc/podinfo`. The volume `podinfo` uses the Downward API to expose container resource constraints by creating files for each resource attribute, referencing the container\\'s resource requests and limits, and dividing the values appropriately for human-readable units.\\n\\nThe code\\'s core functionality involves mounting these resource attribute files into the container, then repeatedly reading and displaying their contents every 5 seconds. This allows dynamic introspection of resource configurations at runtime, which is useful for debugging, monitoring, or ensuring containers adhere to specified resource quotas. The configuration effectively links container resource specifications with accessible runtime data, enabling in-container introspection and validation of resource allocations.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubernetes-downwardapi-volume-example-2\\nspec:\\n  containers:\\n    - name: client-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [\"sh\", \"-c\"]\\n      args:\\n      - while true; do\\n          echo -en \\'\\\\n\\';\\n          if [[ -e /etc/podinfo/cpu_limit ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/cpu_limit; fi;\\n          if [[ -e /etc/podinfo/cpu_request ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/cpu_request; fi;\\n          if [[ -e /etc/podinfo/mem_limit ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/mem_limit; fi;\\n          if [[ -e /etc/podinfo/mem_request ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/mem_request; fi;\\n          sleep 5;\\n        done;\\n      resources:\\n        requests:\\n          memory: \"32Mi\"\\n          cpu: \"125m\"\\n        limits:\\n          memory: \"64Mi\"\\n          cpu: \"250m\"\\n      volumeMounts:\\n        - name: podinfo\\n          mountPath: /etc/podinfo\\n  volumes:\\n    - name: podinfo\\n      downwardAPI:\\n        items:\\n          - path: \"cpu_limit\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: limits.cpu\\n              divisor: 1m\\n          - path: \"cpu_request\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: requests.cpu\\n              divisor: 1m\\n          - path: \"mem_limit\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: limits.memory\\n              divisor: 1Mi\\n          - path: \"mem_request\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: requests.memory\\n              divisor: 1Mi\\n\\n', 'subchunk': '1/1', 'summary': \"The provided YAML configuration defines a Kubernetes Pod that demonstrates the use of the Downward API to expose container resource requests and limits as files within a volume. The Pod contains a single container running BusyBox, which executes a shell script in a continuous loop. This script reads and prints resource information—such as CPU and memory requests and limits—from specific files mounted at `/etc/podinfo`. The volume `podinfo` uses the Downward API to expose container resource constraints by creating files for each resource attribute, referencing the container's resource requests and limits, and dividing the values appropriately for human-readable units.\\n\\nThe code's core functionality involves mounting these resource attribute files into the container, then repeatedly reading and displaying their contents every 5 seconds. This allows dynamic introspection of resource configurations at runtime, which is useful for debugging, monitoring, or ensuring containers adhere to specified resource quotas. The configuration effectively links container resource specifications with accessible runtime data, enabling in-container introspection and validation of resource allocations.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume-resources.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bdc00aee-740c-51cb-b9d1-15c981c9f9cf'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML code defines a PersistentVolumeClaim (PVC) in Kubernetes, which is a request for storage by a pod. The PVC named \"pvc-quota-demo-2\" specifies the use of a storage class called \"manual,\" with access mode set to \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node. It requests 4 GiB of storage. This configuration allows pods to dynamically allocate and use persistent storage based on the defined parameters, enabling data persistence and management in Kubernetes environments.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: pvc-quota-demo-2\\nspec:\\n  storageClassName: manual\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 4Gi\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects-pvc-2.yaml', 'summary': 'This YAML code defines a PersistentVolumeClaim (PVC) in Kubernetes, which is a request for storage by a pod. The PVC named \"pvc-quota-demo-2\" specifies the use of a storage class called \"manual,\" with access mode set to \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node. It requests 4 GiB of storage. This configuration allows pods to dynamically allocate and use persistent storage based on the defined parameters, enabling data persistence and management in Kubernetes environments.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bdda992f-9912-524c-b39c-414ce6d466d7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a LimitRange resource in a Kubernetes cluster. It sets constraints on PersistentVolumeClaims (PVCs), specifying that each PVC must have a minimum storage of 1Gi and a maximum storage of 2Gi. This ensures that users or applications requesting storage through PVCs adhere to these size limits, helping manage resource utilization and prevent over-provisioning or under-provisioning of storage. The configuration helps enforce storage policies at the namespace level within the cluster.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: storagelimits\\nspec:\\n  limits:\\n  - type: PersistentVolumeClaim\\n    max:\\n      storage: 2Gi\\n    min:\\n      storage: 1Gi\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\storagelimits.yaml', 'summary': 'This YAML configuration defines a LimitRange resource in a Kubernetes cluster. It sets constraints on PersistentVolumeClaims (PVCs), specifying that each PVC must have a minimum storage of 1Gi and a maximum storage of 2Gi. This ensures that users or applications requesting storage through PVCs adhere to these size limits, helping manage resource utilization and prevent over-provisioning or under-provisioning of storage. The configuration helps enforce storage policies at the namespace level within the cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('be12e8c7-8157-59ff-acd6-2db42e762f29'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes NetworkPolicy configuration, which defines network access rules for pods within a cluster. This policy applies to pods labeled with `role: db` in the default namespace and specifies both ingress and egress rules to control traffic flow. The ingress rules allow connections from specific sources: IP addresses within the CIDR block 172.17.0.0/16 except for 172.17.1.0/24, pods labeled with `role: frontend`, and namespaces labeled with `project: myproject`. Access is restricted to TCP port 6379, typically used by Redis. The egress rules permit the pods to send traffic to IP addresses within 10.0.0.0/24 over TCP port 5978. This policy helps secure the pods by defining explicit inbound and outbound traffic permissions, implementing network segmentation and access control in a Kubernetes environment.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: test-network-policy\\n  namespace: default\\nspec:\\n  podSelector:\\n    matchLabels:\\n      role: db\\n  policyTypes:\\n  - Ingress\\n  - Egress\\n  ingress:\\n  - from:\\n    - ipBlock:\\n        cidr: 172.17.0.0/16\\n        except:\\n        - 172.17.1.0/24\\n    - namespaceSelector:\\n        matchLabels:\\n          project: myproject\\n    - podSelector:\\n        matchLabels:\\n          role: frontend\\n    ports:\\n    - protocol: TCP\\n      port: 6379\\n  egress:\\n  - to:\\n    - ipBlock:\\n        cidr: 10.0.0.0/24\\n    ports:\\n    - protocol: TCP\\n      port: 5978\\n\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes NetworkPolicy configuration, which defines network access rules for pods within a cluster. This policy applies to pods labeled with `role: db` in the default namespace and specifies both ingress and egress rules to control traffic flow. The ingress rules allow connections from specific sources: IP addresses within the CIDR block 172.17.0.0/16 except for 172.17.1.0/24, pods labeled with `role: frontend`, and namespaces labeled with `project: myproject`. Access is restricted to TCP port 6379, typically used by Redis. The egress rules permit the pods to send traffic to IP addresses within 10.0.0.0/24 over TCP port 5978. This policy helps secure the pods by defining explicit inbound and outbound traffic permissions, implementing network segmentation and access control in a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\networkpolicy.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('be85257a-d776-50fb-b976-bb2c3347be99'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration file in YAML format, defining a pod named \"busybox2\" with four containers, all based on the busybox:1.28 image. Each container runs an infinite shell loop, periodically echoing a message specific to its identifier (cnt01, cnt02, cnt03, cnt04) every 10 seconds.\\n\\nThe configuration specifies resource requests and limits for some containers to manage their CPU and memory consumption: for example, cnt01 requests 100Mi of memory and 100m of CPU, with limits set to 200Mi and 500m respectively. Notably, cnt02 lacks resource limits, and cnt04 does not specify resource requests or limits. This setup helps control resource utilization and ensures predictable performance within a shared environment.\\n\\nOverall, this YAML demonstrates a basic use case of defining multiple containers within a single pod, each running simple, continuous tasks with resource constraints applied where specified.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: busybox2\\nspec:\\n  containers:\\n  - name: busybox-cnt01\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt01; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt02\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt02; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n  - name: busybox-cnt03\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt03; sleep 10;done\"]\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt04\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt04; sleep 10;done\"]\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration file in YAML format, defining a pod named \"busybox2\" with four containers, all based on the busybox:1.28 image. Each container runs an infinite shell loop, periodically echoing a message specific to its identifier (cnt01, cnt02, cnt03, cnt04) every 10 seconds.\\n\\nThe configuration specifies resource requests and limits for some containers to manage their CPU and memory consumption: for example, cnt01 requests 100Mi of memory and 100m of CPU, with limits set to 200Mi and 500m respectively. Notably, cnt02 lacks resource limits, and cnt04 does not specify resource requests or limits. This setup helps control resource utilization and ensures predictable performance within a shared environment.\\n\\nOverall, this YAML demonstrates a basic use case of defining multiple containers within a single pod, each running simple, continuous tasks with resource constraints applied where specified.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('bed6164b-aa5a-5422-b154-c0974eaa18f1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code defines a Kubernetes Pod configuration in YAML format. It specifies a single pod named \"env-single-secret\" that runs an Nginx container. The key feature of this configuration is the environment variable \"SECRET_USERNAME\" within the container, which is populated using a secret reference. Specifically, the environment variable retrieves its value from a Kubernetes secret named \"backend-user\" and the key \"backend-username\" within that secret. This approach allows sensitive data, like usernames or passwords, to be securely injected into the container without hardcoding them in the pod specification, adhering to best practices for managing secrets in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: env-single-secret\\nspec:\\n  containers:\\n  - name: envars-test-container\\n    image: nginx\\n    env:\\n    - name: SECRET_USERNAME\\n      valueFrom:\\n        secretKeyRef:\\n          name: backend-user\\n          key: backend-username\\n', 'subchunk': '1/1', 'summary': 'The provided code defines a Kubernetes Pod configuration in YAML format. It specifies a single pod named \"env-single-secret\" that runs an Nginx container. The key feature of this configuration is the environment variable \"SECRET_USERNAME\" within the container, which is populated using a secret reference. Specifically, the environment variable retrieves its value from a Kubernetes secret named \"backend-user\" and the key \"backend-username\" within that secret. This approach allows sensitive data, like usernames or passwords, to be securely injected into the container without hardcoding them in the pod specification, adhering to best practices for managing secrets in Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-single-secret-env-variable.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c1009114-7f14-5213-a87b-2f8e08345e10'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This configuration defines a Kubernetes MutatingAdmissionPolicy, which injects a sidecar proxy into pods during creation. It specifies that the policy applies to pod creation requests and only when the pod does not already contain a sidecar named \"mesh-proxy.\" If the condition is met, the policy applies a JSONPatch mutation to add an initContainer with specific properties, including its name, image, and restart policy. This allows automatic injection of a sidecar component into pods, facilitating the implementation of service meshes or other sidecar-based functionalities in a Kubernetes cluster.\\napiVersion: admissionregistration.k8s.io/v1alpha1\\nkind: MutatingAdmissionPolicy\\nmetadata:\\n  name: \"sidecar-policy.example.com\"\\nspec:\\n  paramKind:\\n    kind: Sidecar\\n    apiVersion: mutations.example.com/v1\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\"]\\n      resources:   [\"pods\"]\\n  matchConditions:\\n    - name: does-not-already-have-sidecar\\n      expression: \"!object.spec.initContainers.exists(ic, ic.name == \\\\\"mesh-proxy\\\\\")\"\\n  failurePolicy: Fail\\n  reinvocationPolicy: IfNeeded\\n  mutations:\\n    - patchType: \"JSONPatch\"\\n      jsonPatch:\\n        expression: >\\n          [\\n            JSONPatch{\\n              op: \"add\", path: \"/spec/initContainers/-\",\\n              value: Object.spec.initContainers{\\n                name: \"mesh-proxy\",\\n                image: \"mesh-proxy/v1.0.0\",\\n                restartPolicy: \"Always\"\\n              }\\n            }\\n          ]\\n', 'subchunk': '1/1', 'summary': 'This configuration defines a Kubernetes MutatingAdmissionPolicy, which injects a sidecar proxy into pods during creation. It specifies that the policy applies to pod creation requests and only when the pod does not already contain a sidecar named \"mesh-proxy.\" If the condition is met, the policy applies a JSONPatch mutation to add an initContainer with specific properties, including its name, image, and restart policy. This allows automatic injection of a sidecar component into pods, facilitating the implementation of service meshes or other sidecar-based functionalities in a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\mutatingadmissionpolicy\\\\json-patch-example.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c1735356-4f6b-58d5-9135-06324315a2fd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration defined in YAML. It specifies a pod named \"default-mem-demo-2\" containing a single container. The container uses the \"nginx\" image and has a resource limit set for memory, restricting its usage to 1 gigabyte (1Gi). This configuration ensures that the container will not consume more than the specified memory limit, aiding in resource management and preventing potential overuse on the node.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-mem-demo-2\\nspec:\\n  containers:\\n  - name: default-mem-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"1Gi\"\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod-2.yaml', 'summary': 'The provided content is a Kubernetes Pod configuration defined in YAML. It specifies a pod named \"default-mem-demo-2\" containing a single container. The container uses the \"nginx\" image and has a resource limit set for memory, restricting its usage to 1 gigabyte (1Gi). This configuration ensures that the container will not consume more than the specified memory limit, aiding in resource management and preventing potential overuse on the node.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c220a331-6eee-5c0b-810d-77f6bfeddb9e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# This content provides a Kubernetes manifest defining resources for deploying an Event Exporter application. It includes a ServiceAccount, a ClusterRoleBinding, and a Deployment. The ServiceAccount creates a dedicated identity for the application to run securely in the cluster. The ClusterRoleBinding grants this ServiceAccount read-only access to cluster resources by binding it to the 'view' ClusterRole. The Deployment specifies a single replica pod running the Event Exporter container, configured with the specified image version, and associates it with the created ServiceAccount to ensure appropriate permissions during operation. The container executes the '/event-exporter' command, and the deployment includes a termination grace period of 30 seconds for smooth shutdowns. Overall, these resources set up a secure and manageable deployment of the Event Exporter in a Kubernetes environment.\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: event-exporter-sa\\n  namespace: default\\n  labels:\\n    app: event-exporter\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: event-exporter-rb\\n  labels:\\n    app: event-exporter\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: view\\nsubjects:\\n- kind: ServiceAccount\\n  name: event-exporter-sa\\n  namespace: default\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: event-exporter-v0.2.3\\n  namespace: default\\n  labels:\\n    app: event-exporter\\nspec:\\n  selector:\\n    matchLabels:\\n      app: event-exporter\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: event-exporter\\n    spec:\\n      serviceAccountName: event-exporter-sa\\n      containers:\\n      - name: event-exporter\\n        image: registry.k8s.io/event-exporter:v0.2.3\\n        command:\\n        - '/event-exporter'\\n      terminationGracePeriodSeconds: 30\\n\", 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\event-exporter.yaml', 'summary': \"This content provides a Kubernetes manifest defining resources for deploying an Event Exporter application. It includes a ServiceAccount, a ClusterRoleBinding, and a Deployment. The ServiceAccount creates a dedicated identity for the application to run securely in the cluster. The ClusterRoleBinding grants this ServiceAccount read-only access to cluster resources by binding it to the 'view' ClusterRole. The Deployment specifies a single replica pod running the Event Exporter container, configured with the specified image version, and associates it with the created ServiceAccount to ensure appropriate permissions during operation. The container executes the '/event-exporter' command, and the deployment includes a termination grace period of 30 seconds for smooth shutdowns. Overall, these resources set up a secure and manageable deployment of the Event Exporter in a Kubernetes environment.\", 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c41bc46d-f2d3-5b89-a9d3-bbe75f405fb1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes MutatingAdmissionPolicy in YAML format, which is used to automatically inject a sidecar container into pods during creation. The policy is named \"sidecar-policy.example.com\" and is configured to apply to CREATE operations on pods within the \"apps\" API group and version \"v1.\" It specifies that it targets pods that do not already contain a \"mesh-proxy\" initContainer, preventing multiple injections. When the policy is triggered, it applies a patch to modify the pod\\'s specification, adding an initContainer named \"mesh-proxy\" with a specified image, arguments, and restart policy. The policy enforces that if the condition matches (the sidecar isn\\'t already present), the sidecar is injected to enable service mesh functionalities, and execution fails if constraints are not met.\\napiVersion: admissionregistration.k8s.io/v1alpha1\\nkind: MutatingAdmissionPolicy\\nmetadata:\\n  name: \"sidecar-policy.example.com\"\\nspec:\\n  paramKind:\\n    kind: Sidecar\\n    apiVersion: mutations.example.com/v1\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\"]\\n      resources:   [\"pods\"]\\n  matchConditions:\\n    - name: does-not-already-have-sidecar\\n      expression: \"!object.spec.initContainers.exists(ic, ic.name == \\\\\"mesh-proxy\\\\\")\"\\n  failurePolicy: Fail\\n  reinvocationPolicy: IfNeeded\\n  mutations:\\n    - patchType: \"ApplyConfiguration\"\\n      applyConfiguration:\\n        expression: >\\n          Object{\\n            spec: Object.spec{\\n              initContainers: [\\n                Object.spec.initContainers{\\n                  name: \"mesh-proxy\",\\n                  image: \"mesh/proxy:v1.0.0\",\\n                  args: [\"proxy\", \"sidecar\"],\\n                  restartPolicy: \"Always\"\\n                }\\n              ]\\n            }\\n          }\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes MutatingAdmissionPolicy in YAML format, which is used to automatically inject a sidecar container into pods during creation. The policy is named \"sidecar-policy.example.com\" and is configured to apply to CREATE operations on pods within the \"apps\" API group and version \"v1.\" It specifies that it targets pods that do not already contain a \"mesh-proxy\" initContainer, preventing multiple injections. When the policy is triggered, it applies a patch to modify the pod\\'s specification, adding an initContainer named \"mesh-proxy\" with a specified image, arguments, and restart policy. The policy enforces that if the condition matches (the sidecar isn\\'t already present), the sidecar is injected to enable service mesh functionalities, and execution fails if constraints are not met.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\mutatingadmissionpolicy\\\\applyconfiguration-example.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c4f4b8dc-505f-5086-b842-b3e4157e6ba4'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# This content describes a basic Kubernetes Pod configuration using YAML. The configuration specifies the API version (`v1`) and the resource kind (`Pod`). It includes metadata with the pod's name, `constraints-mem-demo-4`. The spec section defines a single container within the pod, named `constraints-mem-demo-4-ctr`, which uses the `nginx` image. This setup creates a simple container running an Nginx web server, useful for testing or deploying web applications in a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-mem-demo-4\\nspec:\\n  containers:\\n  - name: constraints-mem-demo-4-ctr\\n    image: nginx\\n\\n\", 'subchunk': '1/1', 'summary': \"This content describes a basic Kubernetes Pod configuration using YAML. The configuration specifies the API version (`v1`) and the resource kind (`Pod`). It includes metadata with the pod's name, `constraints-mem-demo-4`. The spec section defines a single container within the pod, named `constraints-mem-demo-4-ctr`, which uses the `nginx` image. This setup creates a simple container running an Nginx web server, useful for testing or deploying web applications in a Kubernetes environment.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-4.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c57ee4e0-dbc1-5778-828f-4b03e9d8e956'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format that demonstrates how to customize DNS settings for a Pod. It specifies a Pod named \"dns-example\" within the default namespace, containing a single container running the nginx image. The key aspect of this configuration is the overriding of the default DNS policy by setting `dnsPolicy` to \"None,\" which allows explicit configuration through `dnsConfig`. \\n\\nThe `dnsConfig` section defines custom DNS settings, including specific nameservers (e.g., 192.0.2.1), search domains, and DNS options such as `ndots` with a value of \"2\" and enabling EDNS0 support. These configurations impact how DNS resolution is performed within the Pod, providing finer control over DNS behavior for internal or external name resolution. This setup is useful in environments where custom DNS resolution strategies are required.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  namespace: default\\n  name: dns-example\\nspec:\\n  containers:\\n    - name: test\\n      image: nginx\\n  dnsPolicy: \"None\"\\n  dnsConfig:\\n    nameservers:\\n      - 192.0.2.1 # this is an example\\n    searches:\\n      - ns1.svc.cluster-domain.example\\n      - my.dns.search.suffix\\n    options:\\n      - name: ndots\\n        value: \"2\"\\n      - name: edns0\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\custom-dns.yaml', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format that demonstrates how to customize DNS settings for a Pod. It specifies a Pod named \"dns-example\" within the default namespace, containing a single container running the nginx image. The key aspect of this configuration is the overriding of the default DNS policy by setting `dnsPolicy` to \"None,\" which allows explicit configuration through `dnsConfig`. \\n\\nThe `dnsConfig` section defines custom DNS settings, including specific nameservers (e.g., 192.0.2.1), search domains, and DNS options such as `ndots` with a value of \"2\" and enabling EDNS0 support. These configurations impact how DNS resolution is performed within the Pod, providing finer control over DNS behavior for internal or external name resolution. This setup is useful in environments where custom DNS resolution strategies are required.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c5978889-99cd-5dcf-9ddc-6c26eab2363c'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# The provided code is a Kubernetes configuration for deploying a RabbitMQ message broker using a StatefulSet. This configuration specifies a single replica of RabbitMQ with associated labels for identification, a dedicated service name, and label selectors for managing the Pods. The template section defines the container details, including the RabbitMQ Docker image, port configuration, and resource requests and limits to manage CPU and memory usage. It also includes a volume mount to persist data at `/var/lib/rabbitmq`, linked to an empty directory volume named `rabbitmq-data`, which provides ephemeral storage for RabbitMQ data during the Pod's lifecycle. Overall, this manifest facilitates deploying a scalable, manageable RabbitMQ instance on Kubernetes with resource constraints and persistent storage.\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  labels:\\n    component: rabbitmq\\n  name: rabbitmq\\nspec:\\n  replicas: 1\\n  serviceName: rabbitmq-service\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: task-queue\\n      app.kubernetes.io/component: rabbitmq\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: task-queue\\n        app.kubernetes.io/component: rabbitmq\\n    spec:\\n      containers:\\n      - image: rabbitmq\\n        name: rabbitmq\\n        ports:\\n        - containerPort: 5672\\n        resources:\\n          requests:\\n            memory: 16M\\n          limits:\\n            cpu: 250m\\n            memory: 512M\\n        volumeMounts:\\n        - mountPath: /var/lib/rabbitmq\\n          name: rabbitmq-data\\n      volumes:\\n      - name: rabbitmq-data\\n        emptyDir: {}\\n\", 'chunk': '1/1', 'summary': \"The provided code is a Kubernetes configuration for deploying a RabbitMQ message broker using a StatefulSet. This configuration specifies a single replica of RabbitMQ with associated labels for identification, a dedicated service name, and label selectors for managing the Pods. The template section defines the container details, including the RabbitMQ Docker image, port configuration, and resource requests and limits to manage CPU and memory usage. It also includes a volume mount to persist data at `/var/lib/rabbitmq`, linked to an empty directory volume named `rabbitmq-data`, which provides ephemeral storage for RabbitMQ data during the Pod's lifecycle. Overall, this manifest facilitates deploying a scalable, manageable RabbitMQ instance on Kubernetes with resource constraints and persistent storage.\", 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\rabbitmq-statefulset.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c6f539b4-2814-53af-8744-2e6ebc2c3dd9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes IngressClass resource using YAML. It specifies a class named \"nginx-example\" with relevant metadata such as labels and annotations. The annotation indicates that this IngressClass is set as the default class for ingress resources. The spec section points to the ingress controller responsible for managing this ingress class, in this case, the \"k8s.io/ingress-nginx\" controller. This configuration enables Kubernetes to route external traffic to services via the specified nginx ingress controller, establishing a standardized way to manage ingress resources across the cluster.\\napiVersion: networking.k8s.io/v1\\nkind: IngressClass\\nmetadata:\\n  labels:\\n    app.kubernetes.io/component: controller\\n  name: nginx-example\\n  annotations:\\n    ingressclass.kubernetes.io/is-default-class: \"true\"\\nspec:\\n  controller: k8s.io/ingress-nginx\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes IngressClass resource using YAML. It specifies a class named \"nginx-example\" with relevant metadata such as labels and annotations. The annotation indicates that this IngressClass is set as the default class for ingress resources. The spec section points to the ingress controller responsible for managing this ingress class, in this case, the \"k8s.io/ingress-nginx\" controller. This configuration enables Kubernetes to route external traffic to services via the specified nginx ingress controller, establishing a standardized way to manage ingress resources across the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\default-ingressclass.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c77d5262-2e2e-5752-af26-746f89e88da9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes LimitRange resource named `cpu-resource-constraint`, which sets constraints on CPU resource allocation for containers within a namespace. It specifies default CPU requests and limits, with containers requesting and being limited to 500 millicores (`500m`) by default. The configuration also enforces maximum and minimum bounds on CPU usage: a maximum of 1 CPU (`\"1\"`) and a minimum of 100 millicores (`100m`). This helps ensure resource fairness and prevents containers from consuming excessive CPU resources, maintaining cluster stability.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: cpu-resource-constraint\\nspec:\\n  limits:\\n  - default: # this section defines default limits\\n      cpu: 500m\\n    defaultRequest: # this section defines default requests\\n      cpu: 500m\\n    max: # max and min define the limit range\\n      cpu: \"1\"\\n    min:\\n      cpu: 100m\\n    type: Container\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\problematic-limit-range.yaml', 'summary': 'This content defines a Kubernetes LimitRange resource named `cpu-resource-constraint`, which sets constraints on CPU resource allocation for containers within a namespace. It specifies default CPU requests and limits, with containers requesting and being limited to 500 millicores (`500m`) by default. The configuration also enforces maximum and minimum bounds on CPU usage: a maximum of 1 CPU (`\"1\"`) and a minimum of 100 millicores (`100m`). This helps ensure resource fairness and prevents containers from consuming excessive CPU resources, maintaining cluster stability.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c8623f5d-3098-5904-bfc3-fbe1b5bd5cd0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content describes a Kubernetes configuration file for setting up egress traffic control via an EgressSelectorConfiguration resource. This configuration specifies how the API server communicates outbound traffic to external endpoints, focusing on a selection named \"cluster.\" It defines connection parameters including the protocol (using gRPC in this case) and the transport method, which can be either Unix Domain Sockets (UDS) or TCP. \\n\\nThe detailed code configures the API server to connect to the Konnectivity server using the gRPC protocol, with the transport set to UDS, referring to a socket file located at \"/etc/kubernetes/konnectivity-server/konnectivity-server.socket.\" The setup ensures secure and efficient communication between the API server and the Konnectivity server, which manages egress traffic, with options to adapt the transport method based on deployment needs.\\napiVersion: apiserver.k8s.io/v1beta1\\nkind: EgressSelectorConfiguration\\negressSelections:\\n# Since we want to control the egress traffic to the cluster, we use the\\n# \"cluster\" as the name. Other supported values are \"etcd\", and \"controlplane\".\\n- name: cluster\\n  connection:\\n    # This controls the protocol between the API Server and the Konnectivity\\n    # server. Supported values are \"GRPC\" and \"HTTPConnect\". There is no\\n    # end user visible difference between the two modes. You need to set the\\n    # Konnectivity server to work in the same mode.\\n    proxyProtocol: GRPC\\n    transport:\\n      # This controls what transport the API Server uses to communicate with the\\n      # Konnectivity server. UDS is recommended if the Konnectivity server\\n      # locates on the same machine as the API Server. You need to configure the\\n      # Konnectivity server to listen on the same UDS socket.\\n      # The other supported transport is \"tcp\". You will need to set up TLS \\n      # config to secure the TCP transport.\\n      uds:\\n        udsName: /etc/kubernetes/konnectivity-server/konnectivity-server.socket\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\egress-selector-configuration.yaml', 'summary': 'The provided content describes a Kubernetes configuration file for setting up egress traffic control via an EgressSelectorConfiguration resource. This configuration specifies how the API server communicates outbound traffic to external endpoints, focusing on a selection named \"cluster.\" It defines connection parameters including the protocol (using gRPC in this case) and the transport method, which can be either Unix Domain Sockets (UDS) or TCP. \\n\\nThe detailed code configures the API server to connect to the Konnectivity server using the gRPC protocol, with the transport set to UDS, referring to a socket file located at \"/etc/kubernetes/konnectivity-server/konnectivity-server.socket.\" The setup ensures secure and efficient communication between the API server and the Konnectivity server, which manages egress traffic, with options to adapt the transport method based on deployment needs.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c87ce97a-e308-51e5-b786-00bf0553f106'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Secret resource of type `kubernetes.io/tls`, used to store TLS certificate and key data securely. The secret named `secret-tls` contains two data fields: `tls.crt` and `tls.key`, both base64 encoded. The `tls.crt` field holds a TLS certificate, although in this example it is a placeholder, and the `tls.key` contains a placeholder private key data. These fields are used for encrypting traffic between clients and services hosted on Kubernetes, enabling secure communication. The example emphasizes that base64 encoding obscures the data but does not provide real confidentiality. The overall structure defines how to store TLS credentials as a Kubernetes secret for later use in ingress or other TLS-requiring components.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret-tls\\ntype: kubernetes.io/tls\\ndata:\\n  # values are base64 encoded, which obscures them but does NOT provide\\n  # any useful level of confidentiality\\n  tls.crt: |\\n    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNVakNDQWJzQ0FnMytNQTBHQ1NxR1NJYjNE\\n    UUVCQlFVQU1JR2JNUXN3Q1FZRFZRUUdFd0pLVURFT01Bd0cKQTFVRUNCTUZWRzlyZVc4eEVEQU9C\\n    Z05WQkFjVEIwTm9kVzh0YTNVeEVUQVBCZ05WQkFvVENFWnlZVzVyTkVSRQpNUmd3RmdZRFZRUUxF\\n    dzlYWldKRFpYSjBJRk4xY0hCdmNuUXhHREFXQmdOVkJBTVREMFp5WVc1ck5FUkVJRmRsCllpQkRR\\n    VEVqTUNFR0NTcUdTSWIzRFFFSkFSWVVjM1Z3Y0c5eWRFQm1jbUZ1YXpSa1pDNWpiMjB3SGhjTk1U\\n    TXcKTVRFeE1EUTFNVE01V2hjTk1UZ3dNVEV3TURRMU1UTTVXakJMTVFzd0NRWURWUVFHREFKS1VE\\n    RVBNQTBHQTFVRQpDQXdHWEZSdmEzbHZNUkV3RHdZRFZRUUtEQWhHY21GdWF6UkVSREVZTUJZR0Ex\\n    VUVBd3dQZDNkM0xtVjRZVzF3CmJHVXVZMjl0TUlHYU1BMEdDU3FHU0liM0RRRUJBUVVBQTRHSUFE\\n    Q0JoQUo5WThFaUhmeHhNL25PbjJTbkkxWHgKRHdPdEJEVDFKRjBReTliMVlKanV2YjdjaTEwZjVN\\n    Vm1UQllqMUZTVWZNOU1vejJDVVFZdW4yRFljV29IcFA4ZQpqSG1BUFVrNVd5cDJRN1ArMjh1bklI\\n    QkphVGZlQ09PekZSUFY2MEdTWWUzNmFScG04L3dVVm16eGFLOGtCOWVaCmhPN3F1TjdtSWQxL2pW\\n    cTNKODhDQXdFQUFUQU5CZ2txaGtpRzl3MEJBUVVGQUFPQmdRQU1meTQzeE15OHh3QTUKVjF2T2NS\\n    OEtyNWNaSXdtbFhCUU8xeFEzazlxSGtyNFlUY1JxTVQ5WjVKTm1rWHYxK2VSaGcwTi9WMW5NUTRZ\\n    RgpnWXcxbnlESnBnOTduZUV4VzQyeXVlMFlHSDYyV1hYUUhyOVNVREgrRlowVnQvRGZsdklVTWRj\\n    UUFEZjM4aU9zCjlQbG1kb3YrcE0vNCs5a1h5aDhSUEkzZXZ6OS9NQT09Ci0tLS0tRU5EIENFUlRJ\\n    RklDQVRFLS0tLS0K\\n  # In this example, the key data is not a real PEM-encoded private key\\n  tls.key: |\\n    RXhhbXBsZSBkYXRhIGZvciB0aGUgVExTIGNydCBmaWVsZA==', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Secret resource of type `kubernetes.io/tls`, used to store TLS certificate and key data securely. The secret named `secret-tls` contains two data fields: `tls.crt` and `tls.key`, both base64 encoded. The `tls.crt` field holds a TLS certificate, although in this example it is a placeholder, and the `tls.key` contains a placeholder private key data. These fields are used for encrypting traffic between clients and services hosted on Kubernetes, enabling secure communication. The example emphasizes that base64 encoding obscures the data but does not provide real confidentiality. The overall structure defines how to store TLS credentials as a Kubernetes secret for later use in ingress or other TLS-requiring components.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\tls-auth-secret.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('c8f19a98-d4d4-52b7-a732-1a1505409c43'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes PersistentVolume (PV), which is a piece of storage in the cluster that can be used by pods. It specifies metadata including the PV\\'s name (\"task-pv-volume\") and a label (\"type: local\"). The specification details the storage class as \"manual,\" a capacity of 10Gi, and access mode \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node. The storage is backed by a hostPath, pointing to \"/mnt/data\" on the host machine, which implies the volume is tied to specific node storage rather than cloud or network storage options. This setup is useful for local storage needs where data persistence on a specific node is required.\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: task-pv-volume\\n  labels:\\n    type: local\\nspec:\\n  storageClassName: manual\\n  capacity:\\n    storage: 10Gi\\n  accessModes:\\n    - ReadWriteOnce\\n  hostPath:\\n    path: \"/mnt/data\"\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes PersistentVolume (PV), which is a piece of storage in the cluster that can be used by pods. It specifies metadata including the PV\\'s name (\"task-pv-volume\") and a label (\"type: local\"). The specification details the storage class as \"manual,\" a capacity of 10Gi, and access mode \"ReadWriteOnce,\" meaning the volume can be mounted as read-write by a single node. The storage is backed by a hostPath, pointing to \"/mnt/data\" on the host machine, which implies the volume is tied to specific node storage rather than cloud or network storage options. This setup is useful for local storage needs where data persistence on a specific node is required.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-volume.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('cb4e2d2f-0917-5be3-a504-03e8a5734a0f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': \"# This Kubernetes manifest defines a FlowSchema resource using the `flowcontrol.apiserver.k8s.io/v1` API version. The FlowSchema named `list-events-default-service-account` specifies network flow control rules for API requests, primarily focusing on permissions associated with listing events in the default namespace. \\n\\nThe configuration uses `ByUser` as the distinguisher method, allowing the flow control to distinguish requests based on the user making the API call. The `matchingPrecedence` value of 8000 determines the order in which this rule is evaluated relative to other rules. It references a priority level named `catch-all`, which manages the overall request prioritization.\\n\\nThe rule within the FlowSchema grants permission to list (`verbs: list`) resources of type `events` across all API groups (`'*'`) but limited to the `default` namespace. This permission is restricted to requests made by the default ServiceAccount in the default namespace, ensuring that only the default service account can list events under these constraints.\\n\\nOverall, this manifest sets up a flow control rule to authorize the default ServiceAccount in the default namespace to list events, with request differentiation based on the user, and directs how these rules are prioritized within the API server's flow control framework.\\napiVersion: flowcontrol.apiserver.k8s.io/v1\\nkind: FlowSchema\\nmetadata:\\n  name: list-events-default-service-account\\nspec:\\n  distinguisherMethod:\\n    type: ByUser\\n  matchingPrecedence: 8000\\n  priorityLevelConfiguration:\\n    name: catch-all\\n  rules:\\n    - resourceRules:\\n      - apiGroups:\\n          - '*'\\n        namespaces:\\n          - default\\n        resources:\\n          - events\\n        verbs:\\n          - list\\n      subjects:\\n        - kind: ServiceAccount\\n          serviceAccount:\\n            name: default\\n            namespace: default\", 'chunk': '1/1', 'summary': \"This Kubernetes manifest defines a FlowSchema resource using the `flowcontrol.apiserver.k8s.io/v1` API version. The FlowSchema named `list-events-default-service-account` specifies network flow control rules for API requests, primarily focusing on permissions associated with listing events in the default namespace. \\n\\nThe configuration uses `ByUser` as the distinguisher method, allowing the flow control to distinguish requests based on the user making the API call. The `matchingPrecedence` value of 8000 determines the order in which this rule is evaluated relative to other rules. It references a priority level named `catch-all`, which manages the overall request prioritization.\\n\\nThe rule within the FlowSchema grants permission to list (`verbs: list`) resources of type `events` across all API groups (`'*'`) but limited to the `default` namespace. This permission is restricted to requests made by the default ServiceAccount in the default namespace, ensuring that only the default service account can list events under these constraints.\\n\\nOverall, this manifest sets up a flow control rule to authorize the default ServiceAccount in the default namespace to list events, with request differentiation based on the user, and directs how these rules are prioritized within the API server's flow control framework.\", 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priority-and-fairness\\\\list-events-default-service-account.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('cd12d5fa-cc93-556d-b6da-1eaa3db211ac'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a Kubernetes Pod configuration that demonstrates the use of a projected volume. The Pod named \"volume-test\" runs a single container based on the Busybox image, executing a long sleep command. The container mounts a volume called \"all-in-one\" at the path \"/projected-volume\" in read-only mode. \\n\\nThe \"all-in-one\" volume is defined as a projected volume, which consolidates multiple data sources into a single volume. In this case, it projects two secrets: \"mysecret\" and \"mysecret2.\" From \"mysecret,\" it extracts the \"username\" key into the path \"my-group/my-username,\" and from \"mysecret2,\" it extracts the \"password\" key into \"my-group/my-password,\" with specific file permissions set to mode 511 (octal), which controls access permissions. Overall, this configuration showcases how Kubernetes can combine secret data into a unified volume accessible to containers, useful for managing sensitive information securely.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: volume-test\\nspec:\\n  containers:\\n  - name: container-test\\n    image: busybox:1.28\\n    command: [\"sleep\", \"3600\"]\\n    volumeMounts:\\n    - name: all-in-one\\n      mountPath: \"/projected-volume\"\\n      readOnly: true\\n  volumes:\\n  - name: all-in-one\\n    projected:\\n      sources:\\n      - secret:\\n          name: mysecret\\n          items:\\n            - key: username\\n              path: my-group/my-username\\n      - secret:\\n          name: mysecret2\\n          items:\\n            - key: password\\n              path: my-group/my-password\\n              mode: 511\\n', 'chunk': '1/1', 'summary': 'This content provides a Kubernetes Pod configuration that demonstrates the use of a projected volume. The Pod named \"volume-test\" runs a single container based on the Busybox image, executing a long sleep command. The container mounts a volume called \"all-in-one\" at the path \"/projected-volume\" in read-only mode. \\n\\nThe \"all-in-one\" volume is defined as a projected volume, which consolidates multiple data sources into a single volume. In this case, it projects two secrets: \"mysecret\" and \"mysecret2.\" From \"mysecret,\" it extracts the \"username\" key into the path \"my-group/my-username,\" and from \"mysecret2,\" it extracts the \"password\" key into \"my-group/my-password,\" with specific file permissions set to mode 511 (octal), which controls access permissions. Overall, this configuration showcases how Kubernetes can combine secret data into a unified volume accessible to containers, useful for managing sensitive information securely.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-secrets-nondefault-permission-mode.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('cd1a5dfa-6e24-50ad-96f6-3f0740dea430'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes pod configuration written in YAML. It defines a pod named \"redis\" with a single container that uses the official Redis image. The configuration includes a volume mount, where a volume named \"redis-storage\" is mounted at the path \"/data/redis\" inside the container. The volume \"redis-storage\" is specified as an `emptyDir`, which creates an ephemeral storage that persists only for the lifetime of the pod. This setup is typically used to run a Redis server with temporary storage that is initialized upon pod creation.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: redis\\nspec:\\n  containers:\\n  - name: redis\\n    image: redis\\n    volumeMounts:\\n    - name: redis-storage\\n      mountPath: /data/redis\\n  volumes:\\n  - name: redis-storage\\n    emptyDir: {}\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes pod configuration written in YAML. It defines a pod named \"redis\" with a single container that uses the official Redis image. The configuration includes a volume mount, where a volume named \"redis-storage\" is mounted at the path \"/data/redis\" inside the container. The volume \"redis-storage\" is specified as an `emptyDir`, which creates an ephemeral storage that persists only for the lifetime of the pod. This setup is typically used to run a Redis server with temporary storage that is initialized upon pod creation.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\redis.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ce20c792-ee7a-51ad-bf1f-b6801859dda1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"counter\" with two containers, illustrating a typical setup for log generation and collection in a containerized environment. The first container, \"count,\" uses the busybox image to run an infinite loop that increments a counter every second, logging timestamped messages to two log files located at /var/log/1.log and /var/log/2.log. These logs simulate ongoing data generation for monitoring or analysis purposes. The second container, \"count-agent,\" runs Fluentd (via the registry.k8s.io/fluentd-gcp:1.30 image), a popular logging agent, configured with environment variables and mounted volumes to enable log collection and forwarding. It shares the volume \"/var/log\" with the first container to access the logs and is configured via a ConfigMap named \"fluentd-config.\" The volume definitions include an emptyDir volume for temporary storage and a ConfigMap volume for Fluentd configuration, illustrating how Kubernetes manages shared storage and configuration for log processing pipelines.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: counter\\nspec:\\n  containers:\\n  - name: count\\n    image: busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  - name: count-agent\\n    image: registry.k8s.io/fluentd-gcp:1.30\\n    env:\\n    - name: FLUENTD_ARGS\\n      value: -c /etc/fluentd-config/fluentd.conf\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n    - name: config-volume\\n      mountPath: /etc/fluentd-config\\n  volumes:\\n  - name: varlog\\n    emptyDir: {}\\n  - name: config-volume\\n    configMap:\\n      name: fluentd-config\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"counter\" with two containers, illustrating a typical setup for log generation and collection in a containerized environment. The first container, \"count,\" uses the busybox image to run an infinite loop that increments a counter every second, logging timestamped messages to two log files located at /var/log/1.log and /var/log/2.log. These logs simulate ongoing data generation for monitoring or analysis purposes. The second container, \"count-agent,\" runs Fluentd (via the registry.k8s.io/fluentd-gcp:1.30 image), a popular logging agent, configured with environment variables and mounted volumes to enable log collection and forwarding. It shares the volume \"/var/log\" with the first container to access the logs and is configured via a ConfigMap named \"fluentd-config.\" The volume definitions include an emptyDir volume for temporary storage and a ConfigMap volume for Fluentd configuration, illustrating how Kubernetes manages shared storage and configuration for log processing pipelines.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-agent-sidecar.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ce3f2d7f-3f53-5ab7-8c88-d7a8b473ef6a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Service configuration in YAML format. It defines a service named \"my-service\" that exposes a network endpoint for a specific application labeled \"MyApp\". The service is configured to use IPv6 addresses, indicated by the `ipFamilies: - IPv6`, and it routes TCP traffic arriving on port 80 to the associated application pods identified by the label `app.kubernetes.io/name: MyApp`. This setup enables network communication to the application within the cluster or externally if properly exposed.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\n  labels:\\n    app.kubernetes.io/name: MyApp\\nspec:\\n  ipFamilies:\\n  - IPv6\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Service configuration in YAML format. It defines a service named \"my-service\" that exposes a network endpoint for a specific application labeled \"MyApp\". The service is configured to use IPv6 addresses, indicated by the `ipFamilies: - IPv6`, and it routes TCP traffic arriving on port 80 to the associated application pods identified by the label `app.kubernetes.io/name: MyApp`. This setup enables network communication to the application within the cluster or externally if properly exposed.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-ipfamilies-ipv6.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ceb62179-1742-5163-a34f-87edf4ce676b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod resource. The Pod is named \"default-mem-demo-3\" and contains a single container named \"default-mem-demo-3-ctr\" that runs the Nginx web server. The container specifies a resource request for memory, reserving 128Mi (128 Megabytes) for its operation. This ensures that the container is allocated sufficient memory when scheduled on a node, helping with resource management and preventing overcommitment in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-mem-demo-3\\nspec:\\n  containers:\\n  - name: default-mem-demo-3-ctr\\n    image: nginx\\n    resources:\\n      requests:\\n        memory: \"128Mi\"\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod resource. The Pod is named \"default-mem-demo-3\" and contains a single container named \"default-mem-demo-3-ctr\" that runs the Nginx web server. The container specifies a resource request for memory, reserving 128Mi (128 Megabytes) for its operation. This ensures that the container is allocated sufficient memory when scheduled on a node, helping with resource management and preventing overcommitment in the cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod-3.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('cf81d60d-51b9-5e5a-bf29-337ee21e21bf'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes StorageClass named \"example-nfs\" for dynamic storage provisioning. It specifies the use of an external NFS provisioner, identified by \"example.com/external-nfs,\" and provides parameters for connecting to the NFS server, including the server address (\"nfs-server.example.com\") and the shared directory path (\"/share\"). The \"readOnly\" parameter is set to \"false,\" indicating that the storage should be mounted with read-write access, allowing applications to both read from and write to the NFS share. This StorageClass enables Kubernetes to automatically create and manage persistent volumes backed by the specified NFS server, facilitating scalable and persistent storage solutions for containerized workloads.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: example-nfs\\nprovisioner: example.com/external-nfs\\nparameters:\\n  server: nfs-server.example.com\\n  path: /share\\n  readOnly: \"false\"\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes StorageClass named \"example-nfs\" for dynamic storage provisioning. It specifies the use of an external NFS provisioner, identified by \"example.com/external-nfs,\" and provides parameters for connecting to the NFS server, including the server address (\"nfs-server.example.com\") and the shared directory path (\"/share\"). The \"readOnly\" parameter is set to \"false,\" indicating that the storage should be mounted with read-write access, allowing applications to both read from and write to the NFS share. This StorageClass enables Kubernetes to automatically create and manage persistent volumes backed by the specified NFS server, facilitating scalable and persistent storage solutions for containerized workloads.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-nfs.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d03c0909-9733-5a13-ab6f-49fc18557aa0'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes PersistentVolumeClaim (PVC) named \"pvc-limit-lower.\" It requests a volume with a storage capacity of 500MiB that can be mounted with ReadWriteOnce access mode, meaning only a single node can read and write to the volume at a time. This PVC allows pods to dynamically allocate storage for persistent data needs within a Kubernetes cluster.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: pvc-limit-lower\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 500Mi\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes PersistentVolumeClaim (PVC) named \"pvc-limit-lower.\" It requests a volume with a storage capacity of 500MiB that can be mounted with ReadWriteOnce access mode, meaning only a single node can read and write to the volume at a time. This PVC allows pods to dynamically allocate storage for persistent data needs within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\pvc-limit-lower.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d0657429-84f0-5428-8e2e-29c8951b031b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format. It specifies a pod named \"audit-pod\" with a label for identification. The pod\\'s security context utilizes a seccomp profile of type \"Localhost,\" referencing a specific profile file (\"profiles/audit.json\") to enforce syscall filtering at the kernel level. Inside the pod, there is a single container named \"test-container\" that uses the Hashicorp \"http-echo\" image, which simply replies with the message \"just made some syscalls!\" when accessed. The container\\'s security context explicitly disables privilege escalation, enhancing security by preventing the container from gaining elevated permissions. Overall, this configuration emphasizes security and syscall auditing, likely for monitoring or analyzing system calls made within the container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: audit-pod\\n  labels:\\n    app: audit-pod\\nspec:\\n  securityContext:\\n    seccompProfile:\\n      type: Localhost\\n      localhostProfile: profiles/audit.json\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:1.0\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\audit-pod.yaml', 'summary': 'This content defines a Kubernetes Pod configuration in YAML format. It specifies a pod named \"audit-pod\" with a label for identification. The pod\\'s security context utilizes a seccomp profile of type \"Localhost,\" referencing a specific profile file (\"profiles/audit.json\") to enforce syscall filtering at the kernel level. Inside the pod, there is a single container named \"test-container\" that uses the Hashicorp \"http-echo\" image, which simply replies with the message \"just made some syscalls!\" when accessed. The container\\'s security context explicitly disables privilege escalation, enhancing security by preventing the container from gaining elevated permissions. Overall, this configuration emphasizes security and syscall auditing, likely for monitoring or analyzing system calls made within the container.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d136c4ec-a5a5-5e6a-a23a-9a2b7b21307d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Pod named \"extended-resource-demo\" with a single container running the nginx image. The key aspect of this setup is the specification of extended resources through the \"resources\" field. It requests and limits the custom resource \"example.com/dongle\" to a quantity of 3 units. This demonstrates how to allocate extended or custom resources in a container, which can be used to manage hardware or software components beyond standard CPU and memory, such as specialized devices or custom features. The configuration ensures that the container both requests and is limited to the specified amount of this extended resource.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: extended-resource-demo\\nspec:\\n  containers:\\n  - name: extended-resource-demo-ctr\\n    image: nginx\\n    resources:\\n      requests:\\n        example.com/dongle: 3\\n      limits:\\n        example.com/dongle: 3\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod.yaml', 'summary': 'This Kubernetes configuration defines a Pod named \"extended-resource-demo\" with a single container running the nginx image. The key aspect of this setup is the specification of extended resources through the \"resources\" field. It requests and limits the custom resource \"example.com/dongle\" to a quantity of 3 units. This demonstrates how to allocate extended or custom resources in a container, which can be used to manage hardware or software components beyond standard CPU and memory, such as specialized devices or custom features. The configuration ensures that the container both requests and is limited to the specified amount of this extended resource.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d1ea92ad-3b19-5a4c-ac82-ed75c8b15034'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format, which is primarily used for deploying and managing containerized applications. The Pod is named \"liveness-http\" and includes metadata labels for identification. It contains a single container using the \"agnhost:2.40\" image from the registry, and it is configured with a liveness probe. The liveness probe periodically performs an HTTP GET request to the \"/healthz\" endpoint on port 8080, including a custom header (\"Custom-Header: Awesome\") to check if the container is healthy. The probe has an initial delay of 3 seconds before starting and repeats every 3 seconds afterward. This setup ensures Kubernetes can determine when the container is unresponsive or unhealthy and take appropriate actions, such as restarting the container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  labels:\\n    test: liveness\\n  name: liveness-http\\nspec:\\n  containers:\\n  - name: liveness\\n    image: registry.k8s.io/e2e-test-images/agnhost:2.40\\n    args:\\n    - liveness\\n    livenessProbe:\\n      httpGet:\\n        path: /healthz\\n        port: 8080\\n        httpHeaders:\\n        - name: Custom-Header\\n          value: Awesome\\n      initialDelaySeconds: 3\\n      periodSeconds: 3\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\http-liveness.yaml', 'summary': 'This content defines a Kubernetes Pod configuration in YAML format, which is primarily used for deploying and managing containerized applications. The Pod is named \"liveness-http\" and includes metadata labels for identification. It contains a single container using the \"agnhost:2.40\" image from the registry, and it is configured with a liveness probe. The liveness probe periodically performs an HTTP GET request to the \"/healthz\" endpoint on port 8080, including a custom header (\"Custom-Header: Awesome\") to check if the container is healthy. The probe has an initial delay of 3 seconds before starting and repeats every 3 seconds afterward. This setup ensures Kubernetes can determine when the container is unresponsive or unhealthy and take appropriate actions, such as restarting the container.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d2d06bca-9d75-5093-93dc-b8ab0cd325d3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ConfigMap named \"mysql\" that stores configuration data for MySQL instances, specifying different settings for primary and replica servers. The ConfigMap contains two configuration files: \"primary.cnf\" and \"replica.cnf\". The primary configuration enables binary logging with \"log-bin,\" which is essential for replication and backups. The replica configuration sets the server to \"super-read-only,\" ensuring the replica can only perform read operations, maintaining data consistency during replication. Overall, this setup helps in managing MySQL high-availability and replication by clearly separating configurations for primary and secondary nodes, facilitating easier deployment and management within Kubernetes environments.\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: mysql\\n  labels:\\n    app: mysql\\n    app.kubernetes.io/name: mysql\\ndata:\\n  primary.cnf: |\\n    # Apply this config only on the primary.\\n    [mysqld]\\n    log-bin\\n  replica.cnf: |\\n    # Apply this config only on replicas.\\n    [mysqld]\\n    super-read-only\\n\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes ConfigMap named \"mysql\" that stores configuration data for MySQL instances, specifying different settings for primary and replica servers. The ConfigMap contains two configuration files: \"primary.cnf\" and \"replica.cnf\". The primary configuration enables binary logging with \"log-bin,\" which is essential for replication and backups. The replica configuration sets the server to \"super-read-only,\" ensuring the replica can only perform read operations, maintaining data consistency during replication. Overall, this setup helps in managing MySQL high-availability and replication by clearly separating configurations for primary and secondary nodes, facilitating easier deployment and management within Kubernetes environments.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-configmap.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d5c574d6-7263-56de-ba16-e5823986aef9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML content defines multiple Kubernetes ResourceQuotas, each targeted at pods with different priority classes: high, medium, and low. Each ResourceQuota specifies resource limits for CPU, memory, and the number of pods, and uses scope selectors to associate quotas with specific PriorityClasses. The high priority quota allows the most resources, while the low priority quota limits resources for lower-priority pods. This setup helps manage and enforce resource distribution based on pod importance, ensuring critical workloads have sufficient resources while preventing resource exhaustion by lower-priority pods.\\napiVersion: v1\\nkind: List\\nitems:\\n- apiVersion: v1\\n  kind: ResourceQuota\\n  metadata:\\n    name: pods-high\\n  spec:\\n    hard:\\n      cpu: \"1000\"\\n      memory: \"200Gi\"\\n      pods: \"10\"\\n    scopeSelector:\\n      matchExpressions:\\n      - operator: In\\n        scopeName: PriorityClass\\n        values: [\"high\"]\\n- apiVersion: v1\\n  kind: ResourceQuota\\n  metadata:\\n    name: pods-medium\\n  spec:\\n    hard:\\n      cpu: \"10\"\\n      memory: \"20Gi\"\\n      pods: \"10\"\\n    scopeSelector:\\n      matchExpressions:\\n      - operator: In\\n        scopeName: PriorityClass\\n        values: [\"medium\"]\\n- apiVersion: v1\\n  kind: ResourceQuota\\n  metadata:\\n    name: pods-low\\n  spec:\\n    hard:\\n      cpu: \"5\"\\n      memory: \"10Gi\"\\n      pods: \"10\"\\n    scopeSelector:\\n      matchExpressions:\\n      - operator: In\\n        scopeName: PriorityClass\\n        values: [\"low\"]\\n', 'subchunk': '1/1', 'summary': 'This YAML content defines multiple Kubernetes ResourceQuotas, each targeted at pods with different priority classes: high, medium, and low. Each ResourceQuota specifies resource limits for CPU, memory, and the number of pods, and uses scope selectors to associate quotas with specific PriorityClasses. The high priority quota allows the most resources, while the low priority quota limits resources for lower-priority pods. This setup helps manage and enforce resource distribution based on pod importance, ensuring critical workloads have sufficient resources while preventing resource exhaustion by lower-priority pods.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\quota.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d6cb2531-8b21-5a0a-9da1-92924d48a7a7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML code defines a PersistentVolumeClaim (PVC) in Kubernetes, which requests storage resources for a pod. The PVC named \"task-pv-claim\" specifies a storage class called \"manual,\" indicating a specific or custom storage provisioner. It permits ReadWriteOnce access mode, meaning the volume can be mounted as read/write by a single node at a time. The request is for 3 gigabytes of storage. This configuration enables Kubernetes to allocate and manage persistent storage for applications, ensuring data persistence beyond pod lifecycle.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: task-pv-claim\\nspec:\\n  storageClassName: manual\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 3Gi\\n', 'subchunk': '1/1', 'summary': 'This YAML code defines a PersistentVolumeClaim (PVC) in Kubernetes, which requests storage resources for a pod. The PVC named \"task-pv-claim\" specifies a storage class called \"manual,\" indicating a specific or custom storage provisioner. It permits ReadWriteOnce access mode, meaning the volume can be mounted as read/write by a single node at a time. The request is for 3 gigabytes of storage. This configuration enables Kubernetes to allocate and manage persistent storage for applications, ensuring data persistence beyond pod lifecycle.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-claim.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d751b1a1-8d94-5c4c-9dbc-966a8f8a7bbe'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Job configuration written in YAML. The job runs a container using the Perl 5.34.0 image to calculate the value of Pi to 2000 digits using the BigNum library. The command executed within the container invokes Perl with specific modules and options to perform the calculation. The configuration ensures that the job does not restart upon completion or failure, with a maximum retry limit set to four attempts. Overall, this setup automates the process of performing a complex mathematical calculation within a containerized environment.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: pi\\nspec:\\n  template:\\n    spec:\\n      containers:\\n      - name: pi\\n        image: perl:5.34.0\\n        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\\n      restartPolicy: Never\\n  backoffLimit: 4\\n', 'chunk': '1/1', 'summary': 'This content describes a Kubernetes Job configuration written in YAML. The job runs a container using the Perl 5.34.0 image to calculate the value of Pi to 2000 digits using the BigNum library. The command executed within the container invokes Perl with specific modules and options to perform the calculation. The configuration ensures that the job does not restart upon completion or failure, with a maximum retry limit set to four attempts. Overall, this setup automates the process of performing a complex mathematical calculation within a containerized environment.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d760ffa2-d3c0-5ac5-b9f9-c1034c37773d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes replication controller configuration written in YAML. It specifies a resource of type `ReplicationController` that manages five identical pods running the Nginx server. The controller is named `my-nginx-v4`, and it uses labels such as `app: nginx` and `deployment: v4` to identify and select its pods. The pod template defines a single container running the `nginx:1.16.1` image, with the command-line argument `nginx -T`, which starts Nginx in test mode to display the configuration. The container exposes port 80 for web traffic. \\n\\nThis configuration automates the deployment and scaling of multiple Nginx instances to ensure high availability, with each pod providing a running Nginx server listening on port 80.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:\\n  name: my-nginx-v4\\nspec:\\n  replicas: 5\\n  selector:\\n    app: nginx\\n    deployment: v4\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n        deployment: v4\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1\\n        args: [\"nginx\", \"-T\"]\\n        ports:\\n        - containerPort: 80\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes replication controller configuration written in YAML. It specifies a resource of type `ReplicationController` that manages five identical pods running the Nginx server. The controller is named `my-nginx-v4`, and it uses labels such as `app: nginx` and `deployment: v4` to identify and select its pods. The pod template defines a single container running the `nginx:1.16.1` image, with the command-line argument `nginx -T`, which starts Nginx in test mode to display the configuration. The container exposes port 80 for web traffic. \\n\\nThis configuration automates the deployment and scaling of multiple Nginx instances to ensure high availability, with each pod providing a running Nginx server listening on port 80.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.16.1.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d8222f49-4c44-58cc-aa42-4d7dabece286'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes StorageClass named \"local-storage.\" It specifies that the provisioner is \"kubernetes.io/no-provisioner,\" indicating that this StorageClass does not support automatic volume provisioning; instead, storage must be manually configured. The \"volumeBindingMode\" set to \"WaitForFirstConsumer\" means that PersistentVolume claims will not be bound until a pod requests the storage, allowing Kubernetes to make more informed placement decisions based on the pod’s scheduling needs. Overall, this configuration is used for managing local storage resources in a Kubernetes cluster where manual handling and specific binding behavior are required.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: local-storage\\nprovisioner: kubernetes.io/no-provisioner # indicates that this StorageClass does not support automatic provisioning\\nvolumeBindingMode: WaitForFirstConsumer\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-local.yaml', 'summary': 'This content defines a Kubernetes StorageClass named \"local-storage.\" It specifies that the provisioner is \"kubernetes.io/no-provisioner,\" indicating that this StorageClass does not support automatic volume provisioning; instead, storage must be manually configured. The \"volumeBindingMode\" set to \"WaitForFirstConsumer\" means that PersistentVolume claims will not be bound until a pod requests the storage, allowing Kubernetes to make more informed placement decisions based on the pod’s scheduling needs. Overall, this configuration is used for managing local storage resources in a Kubernetes cluster where manual handling and specific binding behavior are required.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d88a578c-ef93-56aa-b777-03c7b32d5054'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes PodSecurityPolicy (PSP) configuration, which defines security rules for pod creation and operation within a cluster. It specifies a baseline security policy named \"baseline\" with annotations for AppArmor profiles and security options, indicating the use of the \"runtime/default\" profile for AppArmor and allowing all seccomp profiles. The policy restricts privileged containers and limits allowed capabilities to a predefined set that grants necessary permissions while minimizing security risks.\\n\\nThe configuration allows a broad range of volume types except hostPath, supporting configurations such as configMaps, secrets, CSI volumes, and cloud provider storage options. It disallows host networking, IPC, and PID namespaces, and does not enforce a read-only root filesystem for added flexibility. Furthermore, rules for running as any user, group, or SELinux context are enabled, meaning there is no restriction on user IDs, supplementary groups, or fsGroup. However, it notes that security context options like SELinux are not fully expressed by PSPs, especially on nodes using SELinux instead of AppArmor.\\n\\nOverall, this PSP establishes a secure baseline by restricting privileged access, limiting capabilities, and allowing only safe volume types, while maintaining flexibility in user and security context configurations.\\napiVersion: policy/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: baseline\\n  annotations:\\n    # Optional: Allow the default AppArmor profile, requires setting the default.\\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: \\'runtime/default\\'\\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  \\'runtime/default\\'\\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: \\'*\\'\\nspec:\\n  privileged: false\\n  # The moby default capability set, minus NET_RAW\\n  allowedCapabilities:\\n    - \\'CHOWN\\'\\n    - \\'DAC_OVERRIDE\\'\\n    - \\'FSETID\\'\\n    - \\'FOWNER\\'\\n    - \\'MKNOD\\'\\n    - \\'SETGID\\'\\n    - \\'SETUID\\'\\n    - \\'SETFCAP\\'\\n    - \\'SETPCAP\\'\\n    - \\'NET_BIND_SERVICE\\'\\n    - \\'SYS_CHROOT\\'\\n    - \\'KILL\\'\\n    - \\'AUDIT_WRITE\\'\\n  # Allow all volume types except hostpath\\n  volumes:\\n    # \\'core\\' volume types\\n    - \\'configMap\\'\\n    - \\'emptyDir\\'\\n    - \\'projected\\'\\n    - \\'secret\\'\\n    - \\'downwardAPI\\'\\n    # Assume that ephemeral CSI drivers & persistentVolumes set up by the cluster admin are safe to use.\\n    - \\'csi\\'\\n    - \\'persistentVolumeClaim\\'\\n    - \\'ephemeral\\'\\n    # Allow all other non-hostpath volume types.\\n    - \\'awsElasticBlockStore\\'\\n    - \\'azureDisk\\'\\n    - \\'azureFile\\'\\n    - \\'cephFS\\'\\n    - \\'cinder\\'\\n    - \\'fc\\'\\n    - \\'flexVolume\\'\\n    - \\'flocker\\'\\n    - \\'gcePersistentDisk\\'\\n    - \\'gitRepo\\'\\n    - \\'glusterfs\\'\\n    - \\'iscsi\\'\\n    - \\'nfs\\'\\n    - \\'photonPersistentDisk\\'\\n    - \\'portworxVolume\\'\\n    - \\'quobyte\\'\\n    - \\'rbd\\'\\n    - \\'scaleIO\\'\\n    - \\'storageos\\'\\n    - \\'vsphereVolume\\'\\n  hostNetwork: false\\n  hostIPC: false\\n  hostPID: false\\n  readOnlyRootFilesystem: false\\n  runAsUser:\\n    rule: \\'RunAsAny\\'\\n  seLinux:\\n    # This policy assumes the nodes are using AppArmor rather than SELinux.\\n    # The PSP SELinux API cannot express the SELinux Pod Security Standards,\\n    # so if using SELinux, you must choose a more restrictive default.\\n    rule: \\'RunAsAny\\'\\n  supplementalGroups:\\n    rule: \\'RunAsAny\\'\\n  fsGroup:\\n    rule: \\'RunAsAny\\'\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes PodSecurityPolicy (PSP) configuration, which defines security rules for pod creation and operation within a cluster. It specifies a baseline security policy named \"baseline\" with annotations for AppArmor profiles and security options, indicating the use of the \"runtime/default\" profile for AppArmor and allowing all seccomp profiles. The policy restricts privileged containers and limits allowed capabilities to a predefined set that grants necessary permissions while minimizing security risks.\\n\\nThe configuration allows a broad range of volume types except hostPath, supporting configurations such as configMaps, secrets, CSI volumes, and cloud provider storage options. It disallows host networking, IPC, and PID namespaces, and does not enforce a read-only root filesystem for added flexibility. Furthermore, rules for running as any user, group, or SELinux context are enabled, meaning there is no restriction on user IDs, supplementary groups, or fsGroup. However, it notes that security context options like SELinux are not fully expressed by PSPs, especially on nodes using SELinux instead of AppArmor.\\n\\nOverall, this PSP establishes a secure baseline by restricting privileged access, limiting capabilities, and allowing only safe volume types, while maintaining flexibility in user and security context configurations.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\baseline-psp.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d88eee6a-76b1-5251-bf9c-e44feb2b8979'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages the deployment of an Nginx container. The deployment specifies a selector to match the label \"app: nginx\" and uses a pod template with the same label to ensure proper identification and management of the pods. The container runs the \"nginx:1.14.2\" image, which is an Nginx web server version. Overall, this configuration automates the process of deploying and managing Nginx instances within a Kubernetes cluster, facilitating scalability and maintainability of the web server deployment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages the deployment of an Nginx container. The deployment specifies a selector to match the label \"app: nginx\" and uses a pod template with the same label to ensure proper identification and management of the pods. The container runs the \"nginx:1.14.2\" image, which is an Nginx web server version. Overall, this configuration automates the process of deploying and managing Nginx instances within a Kubernetes cluster, facilitating scalability and maintainability of the web server deployment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment-no-replicas.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('d9cf5da5-ad1a-5e95-80a2-ec5b7587c1cc'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with specific security and container settings. It specifies the use of the `v1` API version and creates a Pod named \"default-pod\" with labels and annotations for identification and security enhancements, such as enabling the default seccomp profile for system call filtering. The Pod contains a single container named \"test-container\" that uses the `hashicorp/http-echo:0.2.3` image, which is configured to output a custom message (\"just made some syscalls!\") when run. Additionally, the security context of the container is set to disallow privilege escalation, ensuring better security by preventing the container from gaining higher privileges. Overall, this configuration demonstrates how to deploy a simple, secure Pod with a custom message using Kubernetes, emphasizing security best practices with seccomp profile and privilege restrictions.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-pod\\n  labels:\\n    app: default-pod\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/pod: runtime/default\\nspec:\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:0.2.3\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod with specific security and container settings. It specifies the use of the `v1` API version and creates a Pod named \"default-pod\" with labels and annotations for identification and security enhancements, such as enabling the default seccomp profile for system call filtering. The Pod contains a single container named \"test-container\" that uses the `hashicorp/http-echo:0.2.3` image, which is configured to output a custom message (\"just made some syscalls!\") when run. Additionally, the security context of the container is set to disallow privilege escalation, ensuring better security by preventing the container from gaining higher privileges. Overall, this configuration demonstrates how to deploy a simple, secure Pod with a custom message using Kubernetes, emphasizing security best practices with seccomp profile and privilege restrictions.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\default-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('da742aff-9278-5099-bcd2-f1b282a5fb3f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Service named \"my-service\". The service includes metadata such as labels for identification, specifically labeling it as \"MyApp\". It uses a selector based on the label \"app.kubernetes.io/name: MyApp\" to target the appropriate set of pods. The service exposes port 80 using TCP protocol, enabling network access to the application\\'s pods through this port. This setup ensures that traffic directed to port 80 is routed to the pods matching the label, facilitating internal or external communication with the application.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\n  labels:\\n    app.kubernetes.io/name: MyApp\\nspec:\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Service named \"my-service\". The service includes metadata such as labels for identification, specifically labeling it as \"MyApp\". It uses a selector based on the label \"app.kubernetes.io/name: MyApp\" to target the appropriate set of pods. The service exposes port 80 using TCP protocol, enabling network access to the application\\'s pods through this port. This setup ensures that traffic directed to port 80 is routed to the pods matching the label, facilitating internal or external communication with the application.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-default-svc.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('dc4901a4-7f9a-5af4-8edb-1692271450be'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named `security-context-demo-4` with a single container. The container uses the `gcr.io/google-samples/hello-app:2.0` image. A key feature of this setup is the security context, where specific Linux capabilities, `NET_ADMIN` and `SYS_TIME`, are explicitly added to enhance the container’s permissions. This configuration demonstrates how to customize container security settings by granting additional privileges, which can be useful for scenarios requiring network configuration or system time adjustments within the container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo-4\\nspec:\\n  containers:\\n  - name: sec-ctx-4\\n    image: gcr.io/google-samples/hello-app:2.0\\n    securityContext:\\n      capabilities:\\n        add: [\"NET_ADMIN\", \"SYS_TIME\"]\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named `security-context-demo-4` with a single container. The container uses the `gcr.io/google-samples/hello-app:2.0` image. A key feature of this setup is the security context, where specific Linux capabilities, `NET_ADMIN` and `SYS_TIME`, are explicitly added to enhance the container’s permissions. This configuration demonstrates how to customize container security settings by granting additional privileges, which can be useful for scenarios requiring network configuration or system time adjustments within the container.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-4.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('dc592085-1134-5479-812f-7755ff4b4000'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Service resource in YAML format. The Service is named \"hello\" and is configured to expose an application with labels \"app: hello\" and \"tier: backend\". It specifies a TCP port 80 for external access, directing traffic to an internal target port named \"http\". This setup enables communication between external clients and the backend application running within a Kubernetes cluster, providing an abstraction layer for managing network access.\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: hello\\nspec:\\n  selector:\\n    app: hello\\n    tier: backend\\n  ports:\\n  - protocol: TCP\\n    port: 80\\n    targetPort: http\\n...', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes Service resource in YAML format. The Service is named \"hello\" and is configured to expose an application with labels \"app: hello\" and \"tier: backend\". It specifies a TCP port 80 for external access, directing traffic to an internal target port named \"http\". This setup enables communication between external clients and the backend application running within a Kubernetes cluster, providing an abstraction layer for managing network access.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('dde04a4b-badf-53e9-8185-1fe773e5f1e9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content provides a Kubernetes configuration deploying a PHP-Apache application along with its associated service. The first part defines a Deployment that manages pods running the containerized PHP-Apache server, specifying resource limits and requests for CPU to ensure proper resource allocation. The Deployment uses a specific container image and exposes port 80. The second part creates a Service that exposes this deployment on port 80, providing a stable endpoint for accessing the application. Overall, it automates the deployment and exposure of a containerized web application in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: php-apache\\nspec:\\n  selector:\\n    matchLabels:\\n      run: php-apache\\n  template:\\n    metadata:\\n      labels:\\n        run: php-apache\\n    spec:\\n      containers:\\n      - name: php-apache\\n        image: registry.k8s.io/hpa-example\\n        ports:\\n        - containerPort: 80\\n        resources:\\n          limits:\\n            cpu: 500m\\n          requests:\\n            cpu: 200m\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: php-apache\\n  labels:\\n    run: php-apache\\nspec:\\n  ports:\\n  - port: 80\\n  selector:\\n    run: php-apache\\n', 'chunk': '1/1', 'summary': 'This content provides a Kubernetes configuration deploying a PHP-Apache application along with its associated service. The first part defines a Deployment that manages pods running the containerized PHP-Apache server, specifying resource limits and requests for CPU to ensure proper resource allocation. The Deployment uses a specific container image and exposes port 80. The second part creates a Service that exposes this deployment on port 80, providing a stable endpoint for accessing the application. Overall, it automates the deployment and exposure of a containerized web application in a Kubernetes cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\php-apache.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('de8d07b9-7d88-5b15-9eec-703a4bbfbe17'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a Pod named \"memory-demo-2\" within the \"mem-example\" namespace, containing a single container. The container uses the \"polinux/stress\" image, which is designed for stress testing system resources. The container requests 50Mi of memory and has a limit of 100Mi, ensuring resource management within the cluster. The container executes the \"stress\" command with arguments to simulate memory load by creating one virtual memory worker that allocates 250MB of memory and then hangs, which can be useful for testing memory management and performance under pressure in a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: memory-demo-2\\n  namespace: mem-example\\nspec:\\n  containers:\\n  - name: memory-demo-2-ctr\\n    image: polinux/stress\\n    resources:\\n      requests:\\n        memory: \"50Mi\"\\n      limits:\\n        memory: \"100Mi\"\\n    command: [\"stress\"]\\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"250M\", \"--vm-hang\", \"1\"]\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a Pod named \"memory-demo-2\" within the \"mem-example\" namespace, containing a single container. The container uses the \"polinux/stress\" image, which is designed for stress testing system resources. The container requests 50Mi of memory and has a limit of 100Mi, ensuring resource management within the cluster. The container executes the \"stress\" command with arguments to simulate memory load by creating one virtual memory worker that allocates 250MB of memory and then hangs, which can be useful for testing memory management and performance under pressure in a Kubernetes environment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('df374030-bebb-5a1c-b4b4-160332202aed'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration manifest that defines a pod named \"lifecycle-demo\" with a single container running the \"nginx\" image. It includes lifecycle hooks to execute specific commands during the container\\'s lifecycle events. The `postStart` hook runs a shell command to write a message to a file once the container starts, indicating successful initialization. The `preStop` hook executes a command sequence to gracefully shut down nginx by sending a quit signal, then waits for the nginx process to terminate before the container stops. This setup showcases how to automate tasks and manage container lifecycle events effectively within Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: lifecycle-demo\\nspec:\\n  containers:\\n  - name: lifecycle-demo-container\\n    image: nginx\\n    lifecycle:\\n      postStart:\\n        exec:\\n          command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"]\\n      preStop:\\n        exec:\\n          command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"]\\n\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration manifest that defines a pod named \"lifecycle-demo\" with a single container running the \"nginx\" image. It includes lifecycle hooks to execute specific commands during the container\\'s lifecycle events. The `postStart` hook runs a shell command to write a message to a file once the container starts, indicating successful initialization. The `preStop` hook executes a command sequence to gracefully shut down nginx by sending a quit signal, then waits for the nginx process to terminate before the container stops. This setup showcases how to automate tasks and manage container lifecycle events effectively within Kubernetes.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\lifecycle-events.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('df38a82f-94db-5b1f-93b9-6399727fd0ad'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML file defines a Kubernetes ClusterRole named \"csr-creator\" that grants permissions related to certificate signing requests. It specifies rules that allow actions such as create, get, list, and watch on the certificatesigningrequests resource within the certificates.k8s.io API group. This setup enables designated users or services to manage certificate signing requests, which are essential for secure communication and identity verification in a Kubernetes cluster. The ClusterRole effectively authorizes specific operations on certificate signing requests at the cluster level, facilitating secure and controlled certificate management.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: csr-creator\\nrules:\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests\\n  verbs:\\n  - create\\n  - get\\n  - list\\n  - watch\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-create.yaml', 'summary': 'This YAML file defines a Kubernetes ClusterRole named \"csr-creator\" that grants permissions related to certificate signing requests. It specifies rules that allow actions such as create, get, list, and watch on the certificatesigningrequests resource within the certificates.k8s.io API group. This setup enables designated users or services to manage certificate signing requests, which are essential for secure communication and identity verification in a Kubernetes cluster. The ClusterRole effectively authorizes specific operations on certificate signing requests at the cluster level, facilitating secure and controlled certificate management.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e10bf94b-d0a8-50ea-9c0a-867399962f13'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes custom resource definition (CRD) in YAML format. It defines a resource of kind `ReplicaLimit` with the API version `rules.example.com/v1`. The resource is named \"replica-limit-test.example.com\" and is placed in the \"default\" namespace. The key parameter specifies a maximum replica limit of 3, which likely controls the number of pod replicas that can be deployed or scaled for a specific application or component. This YAML does not contain executable code but rather configuration data used to enforce deployment constraints within a Kubernetes cluster.\\napiVersion: rules.example.com/v1\\nkind: ReplicaLimit\\nmetadata:\\n  name: \"replica-limit-test.example.com\"\\n  namespace: \"default\"\\nmaxReplicas: 3\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes custom resource definition (CRD) in YAML format. It defines a resource of kind `ReplicaLimit` with the API version `rules.example.com/v1`. The resource is named \"replica-limit-test.example.com\" and is placed in the \"default\" namespace. The key parameter specifies a maximum replica limit of 3, which likely controls the number of pod replicas that can be deployed or scaled for a specific application or component. This YAML does not contain executable code but rather configuration data used to enforce deployment constraints within a Kubernetes cluster.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e208b4a3-f379-5c4b-99c2-ac3eaffb4258'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a LimitRange resource named \"limit-mem-cpu-per-pod.\" It specifies maximum resource limits for CPU and memory that can be allocated to each pod within a namespace. Specifically, it limits each pod to a maximum of 2 CPU units and 2 GiB of memory, helping to control resource usage and prevent any single pod from consuming excessive resources.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: limit-mem-cpu-per-pod\\nspec:\\n  limits:\\n  - max:\\n      cpu: \"2\"\\n      memory: \"2Gi\"\\n    type: Pod\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes configuration defines a LimitRange resource named \"limit-mem-cpu-per-pod.\" It specifies maximum resource limits for CPU and memory that can be allocated to each pod within a namespace. Specifically, it limits each pod to a maximum of 2 CPU units and 2 GiB of memory, helping to control resource usage and prevent any single pod from consuming excessive resources.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-mem-cpu-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e2eeeaf7-38b3-536f-9dbb-a5142f3194ad'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes manifest configuring a DaemonSet, which ensures that a specific pod runs on every node in the cluster. The DaemonSet named \"example-daemonset\" deploys a container running the \"pause\" image, commonly used as a minimal placeholder or to hold network namespace. Additionally, it includes an initContainer called \"log-machine-id\" that executes a shell command to read the machine\\'s ID from the host and log it to a file located in /var/log inside the pod.\\n\\nThe initContainer\\'s role is to gather and log the host machine\\'s unique identifier by mounting the host\\'s /etc/machine-id file into the container as a read-only volume. This volume, along with the log directory /var/log (mounted as another hostPath volume), is shared with the initContainer and the main container, enabling persistent storage of the logged machine ID on the host. Overall, this configuration is useful for logging or auditing purposes, collecting host-specific information systematically across all nodes in the Kubernetes cluster.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: example-daemonset\\nspec:\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: example\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: example\\n    spec:\\n      containers:\\n      - name: pause\\n        image: registry.k8s.io/pause\\n      initContainers:\\n      - name: log-machine-id\\n        image: busybox:1.37\\n        command: [\\'sh\\', \\'-c\\', \\'cat /etc/machine-id > /var/log/machine-id.log\\']\\n        volumeMounts:\\n        - name: machine-id\\n          mountPath: /etc/machine-id\\n          readOnly: true\\n        - name: log-dir\\n          mountPath: /var/log\\n      volumes:\\n      - name: machine-id\\n        hostPath:\\n          path: /etc/machine-id\\n          type: File\\n      - name: log-dir\\n        hostPath:\\n          path: /var/log', 'chunk': '1/1', 'summary': 'This content is a Kubernetes manifest configuring a DaemonSet, which ensures that a specific pod runs on every node in the cluster. The DaemonSet named \"example-daemonset\" deploys a container running the \"pause\" image, commonly used as a minimal placeholder or to hold network namespace. Additionally, it includes an initContainer called \"log-machine-id\" that executes a shell command to read the machine\\'s ID from the host and log it to a file located in /var/log inside the pod.\\n\\nThe initContainer\\'s role is to gather and log the host machine\\'s unique identifier by mounting the host\\'s /etc/machine-id file into the container as a read-only volume. This volume, along with the log directory /var/log (mounted as another hostPath volume), is shared with the initContainer and the main container, enabling persistent storage of the logged machine ID on the host. Overall, this configuration is useful for logging or auditing purposes, collecting host-specific information systematically across all nodes in the Kubernetes cluster.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\basic-daemonset.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e3f9c7ca-12b2-5f88-96b0-d38069b39559'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML content defines a Kubernetes Pod configuration for deploying an Nginx container. The pod is named \"nginx\" and is labeled with \"env: test\" to identify its environment. The container within the pod uses the official Nginx image, with an image pull policy set to \"IfNotPresent,\" meaning it will only pull the image if it\\'s not already available locally. Additionally, the pod has a toleration that allows it to be scheduled on nodes with taints matching the specified key \"example-key,\" provided the effect is \"NoSchedule.\" This configuration ensures the Nginx server runs in the testing environment and can be scheduled on appropriately tainted nodes in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\n  labels:\\n    env: test\\nspec:\\n  containers:\\n  - name: nginx\\n    image: nginx\\n    imagePullPolicy: IfNotPresent\\n  tolerations:\\n  - key: \"example-key\"\\n    operator: \"Exists\"\\n    effect: \"NoSchedule\"\\n', 'subchunk': '1/1', 'summary': 'This YAML content defines a Kubernetes Pod configuration for deploying an Nginx container. The pod is named \"nginx\" and is labeled with \"env: test\" to identify its environment. The container within the pod uses the official Nginx image, with an image pull policy set to \"IfNotPresent,\" meaning it will only pull the image if it\\'s not already available locally. Additionally, the pod has a toleration that allows it to be scheduled on nodes with taints matching the specified key \"example-key,\" provided the effect is \"NoSchedule.\" This configuration ensures the Nginx server runs in the testing environment and can be scheduled on appropriately tainted nodes in the cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-toleration.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e4169396-ed0e-5a2c-be36-ff52d2821fb1'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The given content is a Kubernetes YAML configuration defining a `ValidatingAdmissionPolicyBinding`. This resource links a specific admission policy to certain resources within the cluster. The `metadata` section specifies the name of the binding, while the `spec` section details the policy that is being applied, including its name (`demo-policy.example.com`) and that its validation action is set to `Deny`. Additionally, it specifies that the policy will be applied only to resources in namespaces labeled with `environment: test`. This setup ensures that the validation policy will enforce rules or checks on resources within the designated testing environment, potentially denying resource creations or modifications if they do not meet the policy’s criteria.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicyBinding\\nmetadata:\\n  name: \"demo-binding-test.example.com\"\\nspec:\\n  policyName: \"demo-policy.example.com\"\\n  validationActions: [Deny]\\n  matchResources:\\n    namespaceSelector:\\n      matchLabels:\\n        environment: test\\n', 'subchunk': '1/1', 'summary': 'The given content is a Kubernetes YAML configuration defining a `ValidatingAdmissionPolicyBinding`. This resource links a specific admission policy to certain resources within the cluster. The `metadata` section specifies the name of the binding, while the `spec` section details the policy that is being applied, including its name (`demo-policy.example.com`) and that its validation action is set to `Deny`. Additionally, it specifies that the policy will be applied only to resources in namespaces labeled with `environment: test`. This setup ensures that the validation policy will enforce rules or checks on resources within the designated testing environment, potentially denying resource creations or modifications if they do not meet the policy’s criteria.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\basic-example-binding.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e44d805f-205c-534d-9570-03bd76aa40d9'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a PodDisruptionBudget (PDB) for a Kubernetes application, specifically targeting pods with the label \"app: zookeeper\". The PDB ensures that at least two Zookeeper pods remain available during voluntary disruptions (such as maintenance or upgrades). This helps maintain high availability and prevents significant downtime by controlling the number of pods that can be safely disrupted at any given time.\\napiVersion: policy/v1\\nkind: PodDisruptionBudget\\nmetadata:\\n  name: zk-pdb\\nspec:\\n  minAvailable: 2\\n  selector:\\n    matchLabels:\\n      app: zookeeper\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a PodDisruptionBudget (PDB) for a Kubernetes application, specifically targeting pods with the label \"app: zookeeper\". The PDB ensures that at least two Zookeeper pods remain available during voluntary disruptions (such as maintenance or upgrades). This helps maintain high availability and prevents significant downtime by controlling the number of pods that can be safely disrupted at any given time.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\zookeeper-pod-disruption-budget-minavailable.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e4b46647-9a8c-5a96-bfba-328deecf149b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a PodDisruptionBudget (PDB) in Kubernetes, which helps ensure the availability of certain pods during voluntary disruptions like maintenance or updates. The PDB named \"zk-pdb\" targets pods labeled with \"app: zookeeper\". It specifies that at most one pod can be unavailable at any given time (\"maxUnavailable: 1\"), thereby maintaining service stability and preventing excessive downtime of the Zookeeper application during disruptions. The configuration is primarily theoretical, focusing on how to set policies for pod availability in a Kubernetes cluster.\\napiVersion: policy/v1\\nkind: PodDisruptionBudget\\nmetadata:\\n  name: zk-pdb\\nspec:\\n  maxUnavailable: 1\\n  selector:\\n    matchLabels:\\n      app: zookeeper\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\zookeeper-pod-disruption-budget-maxunavailable.yaml', 'summary': 'This YAML configuration defines a PodDisruptionBudget (PDB) in Kubernetes, which helps ensure the availability of certain pods during voluntary disruptions like maintenance or updates. The PDB named \"zk-pdb\" targets pods labeled with \"app: zookeeper\". It specifies that at most one pod can be unavailable at any given time (\"maxUnavailable: 1\"), thereby maintaining service stability and preventing excessive downtime of the Zookeeper application during disruptions. The configuration is primarily theoretical, focusing on how to set policies for pod availability in a Kubernetes cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e5347a43-e807-5faf-8b28-d49c8e30daab'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Secret of type `kubernetes.io/ssh-auth`, which is used to securely store SSH authentication data. The secret is named `secret-ssh-auth` and contains an SSH private key encoded in base64 format. This setup enables secure authentication for processes like access to Git repositories or remote servers within Kubernetes deployments, ensuring sensitive credentials are stored securely and can be referenced by pods or other resources.\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret-ssh-auth\\ntype: kubernetes.io/ssh-auth\\ndata:\\n  # the data is abbreviated in this example\\n  ssh-privatekey: |\\n    UG91cmluZzYlRW1vdGljb24lU2N1YmE=', 'subchunk': '1/1', 'summary': 'This content describes a Kubernetes Secret of type `kubernetes.io/ssh-auth`, which is used to securely store SSH authentication data. The secret is named `secret-ssh-auth` and contains an SSH private key encoded in base64 format. This setup enables secure authentication for processes like access to Git repositories or remote servers within Kubernetes deployments, ensuring sensitive credentials are stored securely and can be referenced by pods or other resources.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\ssh-auth-secret.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e5ee3a2a-79fe-5d0f-9e66-fa3c5c208a01'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes StorageClass configuration in YAML, defining a storage class named \"low-latency.\" It specifies that this class is not the default one and uses a custom CSI driver for provisioning storage. The reclaim policy is set to \"Retain,\" meaning volumes are preserved after they are released, and volume expansion is allowed. Mount options include \"discard,\" which can enable TRIM or UNMAP features at the storage layer, potentially improving performance for SSDs. The volume binding mode \"WaitForFirstConsumer\" delays volume binding until a pod requests it, optimizing resource allocation. Additionally, it includes a provider-specific parameter to ensure guaranteed read/write latency, supporting low-latency storage needs.\\n\\nThis configuration is tailored for scenarios demanding high-performance storage, and it uses specific parameters and settings to optimize the storage provisioning and management process within a Kubernetes environment.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: low-latency\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \"false\"\\nprovisioner: csi-driver.example-vendor.example\\nreclaimPolicy: Retain # default value is Delete\\nallowVolumeExpansion: true\\nmountOptions:\\n  - discard # this might enable UNMAP / TRIM at the block storage layer\\nvolumeBindingMode: WaitForFirstConsumer\\nparameters:\\n  guaranteedReadWriteLatency: \"true\" # provider-specific\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes StorageClass configuration in YAML, defining a storage class named \"low-latency.\" It specifies that this class is not the default one and uses a custom CSI driver for provisioning storage. The reclaim policy is set to \"Retain,\" meaning volumes are preserved after they are released, and volume expansion is allowed. Mount options include \"discard,\" which can enable TRIM or UNMAP features at the storage layer, potentially improving performance for SSDs. The volume binding mode \"WaitForFirstConsumer\" delays volume binding until a pod requests it, optimizing resource allocation. Additionally, it includes a provider-specific parameter to ensure guaranteed read/write latency, supporting low-latency storage needs.\\n\\nThis configuration is tailored for scenarios demanding high-performance storage, and it uses specific parameters and settings to optimize the storage provisioning and management process within a Kubernetes environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass-low-latency.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e5f3f1b7-09f7-5399-a20e-6872c4e7f902'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Deployment for an IIS web server. It specifies the deployment of three replicas of the containerized application to ensure availability and load balancing. The deployment uses the `microsoft/iis` Docker image and assigns resource limits: a maximum of 128MiB of memory and two CPUs per container. The container listens on port 80, which is exposed within the cluster, enabling HTTP traffic to reach the IIS server. Overall, this configuration automates the deployment, scaling, and resource management of multiple IIS instances within a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iis\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: iis\\n  template:\\n    metadata:\\n      labels:\\n        app: iis\\n    spec:\\n      containers:\\n      - name: iis\\n        image: microsoft/iis\\n        resources:\\n          limits:\\n            memory: \"128Mi\"\\n            cpu: 2\\n        ports:\\n        - containerPort: 80\\n\\n', 'chunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Deployment for an IIS web server. It specifies the deployment of three replicas of the containerized application to ensure availability and load balancing. The deployment uses the `microsoft/iis` Docker image and assigns resource limits: a maximum of 128MiB of memory and two CPUs per container. The container listens on port 80, which is exposed within the cluster, enabling HTTP traffic to reach the IIS server. Overall, this configuration automates the deployment, scaling, and resource management of multiple IIS instances within a Kubernetes environment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-resource.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e7a263dd-555b-5c1b-a638-d3d26c07aff3'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes YAML configuration defining a ReplicaSet. It specifies creating three replicas of a pod that runs an nginx container. The ReplicaSet uses label selectors to identify the pods it manages, matching the label \"pod-is-for: garbage-collection-example\". The pod template within the ReplicaSet also includes this label, ensuring proper orchestration and management of these pods. This setup ensures that three identical nginx pods are maintained, providing high availability and load balancing for applications.\\napiVersion: apps/v1\\nkind: ReplicaSet\\nmetadata:\\n  name: my-repset\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      pod-is-for: garbage-collection-example\\n  template:\\n    metadata:\\n      labels:\\n        pod-is-for: garbage-collection-example\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes YAML configuration defining a ReplicaSet. It specifies creating three replicas of a pod that runs an nginx container. The ReplicaSet uses label selectors to identify the pods it manages, matching the label \"pod-is-for: garbage-collection-example\". The pod template within the ReplicaSet also includes this label, ensuring proper orchestration and management of these pods. This setup ensures that three identical nginx pods are maintained, providing high availability and load balancing for applications.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replicaset.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e7d37da6-0957-58c2-aa66-43ac3835e225'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"mypod\" that runs a container using the Redis image. The container mounts a volume at \"/etc/foo\" in read-only mode, which is linked to a secret volume named \"foo\". The secret volume sources its data from a secret called \"mysecret\" and is marked as optional, meaning the Pod can still run if the secret is not present. Overall, this setup demonstrates how to securely inject secret data into a container via Kubernetes secrets, ensuring sensitive information like passwords or keys can be mounted as files within the container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: mypod\\nspec:\\n  containers:\\n  - name: mypod\\n    image: redis\\n    volumeMounts:\\n    - name: foo\\n      mountPath: \"/etc/foo\"\\n      readOnly: true\\n  volumes:\\n  - name: foo\\n    secret:\\n      secretName: mysecret\\n      optional: true', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"mypod\" that runs a container using the Redis image. The container mounts a volume at \"/etc/foo\" in read-only mode, which is linked to a secret volume named \"foo\". The secret volume sources its data from a secret called \"mysecret\" and is marked as optional, meaning the Pod can still run if the secret is not present. Overall, this setup demonstrates how to securely inject secret data into a container via Kubernetes secrets, ensuring sensitive information like passwords or keys can be mounted as files within the container.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\optional-secret.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e89aab7a-14ed-5a17-b520-ecaabf852440'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration in YAML format that demonstrates how to reference resource requests and limits within container environment variables using `resourceFieldRef`. The Pod includes a single container based on the BusyBox image, which runs an infinite loop printing environment variables that fetch the container\\'s CPU and memory requests and limits.\\n\\nThe code explicitly assigns environment variables (`MY_CPU_REQUEST`, `MY_CPU_LIMIT`, `MY_MEM_REQUEST`, `MY_MEM_LIMIT`) by referencing the container\\'s resource specifications via `resourceFieldRef`. These references dynamically extract the respective resource request or limit for CPU and memory, enabling the container to access resource constraints defined in the Pod specification at runtime.\\n\\nThis setup is useful for introspection within containers, allowing applications to adjust behavior based on resource allocations. The container\\'s command repeatedly outputs these environment variables every 10 seconds, illustrating how resource information can be programmatically accessed inside Kubernetes pods.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-envars-resourcefieldref\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"sh\", \"-c\"]\\n      args:\\n      - while true; do\\n          echo -en \\'\\\\n\\';\\n          printenv MY_CPU_REQUEST MY_CPU_LIMIT;\\n          printenv MY_MEM_REQUEST MY_MEM_LIMIT;\\n          sleep 10;\\n        done;\\n      resources:\\n        requests:\\n          memory: \"32Mi\"\\n          cpu: \"125m\"\\n        limits:\\n          memory: \"64Mi\"\\n          cpu: \"250m\"\\n      env:\\n        - name: MY_CPU_REQUEST\\n          valueFrom:\\n            resourceFieldRef:\\n              containerName: test-container\\n              resource: requests.cpu\\n        - name: MY_CPU_LIMIT\\n          valueFrom:\\n            resourceFieldRef:\\n              containerName: test-container\\n              resource: limits.cpu\\n        - name: MY_MEM_REQUEST\\n          valueFrom:\\n            resourceFieldRef:\\n              containerName: test-container\\n              resource: requests.memory\\n        - name: MY_MEM_LIMIT\\n          valueFrom:\\n            resourceFieldRef:\\n              containerName: test-container\\n              resource: limits.memory\\n  restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': \"This content defines a Kubernetes Pod configuration in YAML format that demonstrates how to reference resource requests and limits within container environment variables using `resourceFieldRef`. The Pod includes a single container based on the BusyBox image, which runs an infinite loop printing environment variables that fetch the container's CPU and memory requests and limits.\\n\\nThe code explicitly assigns environment variables (`MY_CPU_REQUEST`, `MY_CPU_LIMIT`, `MY_MEM_REQUEST`, `MY_MEM_LIMIT`) by referencing the container's resource specifications via `resourceFieldRef`. These references dynamically extract the respective resource request or limit for CPU and memory, enabling the container to access resource constraints defined in the Pod specification at runtime.\\n\\nThis setup is useful for introspection within containers, allowing applications to adjust behavior based on resource allocations. The container's command repeatedly outputs these environment variables every 10 seconds, illustrating how resource information can be programmatically accessed inside Kubernetes pods.\", 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-envars-container.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e945ed2f-ceb2-5686-89c8-06afa14f7ebd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes configuration defines a Pod named \"dapi-test-pod\" with a single container called \"test-container\" that runs a BusyBox image. The container executes the command \"/bin/sh -c env\" to display environment variables. It sources its environment variables from a ConfigMap named \"special-config\" using the \"envFrom\" directive. The restart policy is set to \"Never,\" meaning the pod won\\'t automatically restart after completion or failure. Overall, this setup is useful for testing or inspecting environment configurations defined in ConfigMaps within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dapi-test-pod\\nspec:\\n  containers:\\n    - name: test-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\\n      envFrom:\\n      - configMapRef:\\n          name: special-config\\n  restartPolicy: Never\\n', 'subchunk': '1/1', 'summary': 'This Kubernetes configuration defines a Pod named \"dapi-test-pod\" with a single container called \"test-container\" that runs a BusyBox image. The container executes the command \"/bin/sh -c env\" to display environment variables. It sources its environment variables from a ConfigMap named \"special-config\" using the \"envFrom\" directive. The restart policy is set to \"Never,\" meaning the pod won\\'t automatically restart after completion or failure. Overall, this setup is useful for testing or inspecting environment configurations defined in ConfigMaps within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-envFrom.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e958de32-cb34-57c1-81a3-7316b8481113'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes PodSecurityPolicy configuration in YAML format. It defines a policy named \"privileged\" that grants extensive permissions to pods, including privileged mode (which allows containers to perform operations similar to the host), privilege escalation, and the use of all Linux capabilities. The policy also permits the use of any volume types and allows host network, IPC, and PID access, effectively giving the pod significant control over the host environment. Additionally, it sets flexible security context rules, allowing any user ID, SELinux context, supplemental groups, and filesystem group. This policy is highly permissive and should be applied carefully in environments where such elevated privileges are necessary for specific workloads.\\napiVersion: policy/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: privileged\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: \\'*\\'\\nspec:\\n  privileged: true\\n  allowPrivilegeEscalation: true\\n  allowedCapabilities:\\n  - \\'*\\'\\n  volumes:\\n  - \\'*\\'\\n  hostNetwork: true\\n  hostPorts:\\n  - min: 0\\n    max: 65535\\n  hostIPC: true\\n  hostPID: true\\n  runAsUser:\\n    rule: \\'RunAsAny\\'\\n  seLinux:\\n    rule: \\'RunAsAny\\'\\n  supplementalGroups:\\n    rule: \\'RunAsAny\\'\\n  fsGroup:\\n    rule: \\'RunAsAny\\'\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes PodSecurityPolicy configuration in YAML format. It defines a policy named \"privileged\" that grants extensive permissions to pods, including privileged mode (which allows containers to perform operations similar to the host), privilege escalation, and the use of all Linux capabilities. The policy also permits the use of any volume types and allows host network, IPC, and PID access, effectively giving the pod significant control over the host environment. Additionally, it sets flexible security context rules, allowing any user ID, SELinux context, supplemental groups, and filesystem group. This policy is highly permissive and should be applied carefully in environments where such elevated privileges are necessary for specific workloads.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\privileged-psp.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e9aa9487-f621-5dbc-815e-a220cc5d647b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes configuration file defining a ReplicationController. This resource manages three replicas of an Nginx container, ensuring these are consistently running and available. The configuration specifies the use of the Nginx image, assigns labels for identification, and exposes port 80 within the containers. Essentially, it automates the deployment, scaling, and management of multiple Nginx instances to maintain desired application availability and load balancing.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:\\n  name: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    app: nginx\\n  template:\\n    metadata:\\n      name: nginx\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx\\n        ports:\\n        - containerPort: 80\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes configuration file defining a ReplicationController. This resource manages three replicas of an Nginx container, ensuring these are consistently running and available. The configuration specifies the use of the Nginx image, assigns labels for identification, and exposes port 80 within the containers. Essentially, it automates the deployment, scaling, and management of multiple Nginx instances to maintain desired application availability and load balancing.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('e9dbebcc-fad7-5427-943a-6e34fe5de32e'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes LimitRange resource named \"cpu-min-max-demo-lr\" in YAML format. The LimitRange sets constraints on CPU resources for containers within a namespace, specifying that each container must have a CPU request of at least 200 millicores (\"200m\") and can be limited to a maximum of 800 millicores (\"800m\"). This helps in managing resource allocation, ensuring that containers do not consume excessive CPU or fall below a minimum required amount, thus providing resource fairness and stability within the cluster.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: cpu-min-max-demo-lr\\nspec:\\n  limits:\\n  - max:\\n      cpu: \"800m\"\\n    min:\\n      cpu: \"200m\"\\n    type: Container\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints.yaml', 'summary': 'This content defines a Kubernetes LimitRange resource named \"cpu-min-max-demo-lr\" in YAML format. The LimitRange sets constraints on CPU resources for containers within a namespace, specifying that each container must have a CPU request of at least 200 millicores (\"200m\") and can be limited to a maximum of 800 millicores (\"800m\"). This helps in managing resource allocation, ensuring that containers do not consume excessive CPU or fall below a minimum required amount, thus providing resource fairness and stability within the cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ea1db7e7-50dd-5c5e-a349-11934537f285'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided code defines a Kubernetes Pod configuration using YAML syntax. It specifies a pod with the name \"nginx\" that contains two containers: one running the \"nginx\" web server and another using the \"busybox:1.28\" image to execute a sleep command for one hour. The configuration enables the sharing of the process namespace between containers, allowing them to see each other\\'s processes, which is useful for debugging or process monitoring. The \"shell\" container is granted extra capabilities, specifically \"SYS_PTRACE,\" which provides debugging privileges such as process tracing. It also sets up interactive features with stdin and TTY enabled, making it suitable for debugging or interactive sessions. Overall, this setup facilitates monitoring or debugging within a Kubernetes pod by sharing process information across containers and giving necessary privileges to the debugging container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  shareProcessNamespace: true\\n  containers:\\n  - name: nginx\\n    image: nginx\\n  - name: shell\\n    image: busybox:1.28\\n    command: [\"sleep\", \"3600\"]\\n    securityContext:\\n      capabilities:\\n        add:\\n        - SYS_PTRACE\\n    stdin: true\\n    tty: true\\n', 'chunk': '1/1', 'summary': 'The provided code defines a Kubernetes Pod configuration using YAML syntax. It specifies a pod with the name \"nginx\" that contains two containers: one running the \"nginx\" web server and another using the \"busybox:1.28\" image to execute a sleep command for one hour. The configuration enables the sharing of the process namespace between containers, allowing them to see each other\\'s processes, which is useful for debugging or process monitoring. The \"shell\" container is granted extra capabilities, specifically \"SYS_PTRACE,\" which provides debugging privileges such as process tracing. It also sets up interactive features with stdin and TTY enabled, making it suitable for debugging or interactive sessions. Overall, this setup facilitates monitoring or debugging within a Kubernetes pod by sharing process information across containers and giving necessary privileges to the debugging container.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\share-process-namespace.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ea754fa9-125b-5059-b102-c9d3d2f1120f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration specified in YAML format. It defines a pod named \"envvars-multiple-secrets\" that contains a container running the nginx image. The key feature of this configuration is the use of environment variables whose values are sourced from Kubernetes secrets. Specifically, two environment variables, BACKEND_USERNAME and DB_USERNAME, are set by referencing secret objects (\"backend-user\" and \"db-user\" respectively) and their keys (\"backend-username\" and \"db-username\"). This approach ensures sensitive data like usernames are securely injected into the container environment without hardcoding them into the container image or configuration.\\n\\nThe configuration demonstrates best practices for managing sensitive information, leveraging Kubernetes secrets to dynamically populate environment variables in containers. This method enhances security and flexibility by decoupling secret management from container runtime configurations.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: envvars-multiple-secrets\\nspec:\\n  containers:\\n  - name: envars-test-container\\n    image: nginx\\n    env:\\n    - name: BACKEND_USERNAME\\n      valueFrom:\\n        secretKeyRef:\\n          name: backend-user\\n          key: backend-username\\n    - name: DB_USERNAME\\n      valueFrom:\\n        secretKeyRef:\\n          name: db-user\\n          key: db-username\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration specified in YAML format. It defines a pod named \"envvars-multiple-secrets\" that contains a container running the nginx image. The key feature of this configuration is the use of environment variables whose values are sourced from Kubernetes secrets. Specifically, two environment variables, BACKEND_USERNAME and DB_USERNAME, are set by referencing secret objects (\"backend-user\" and \"db-user\" respectively) and their keys (\"backend-username\" and \"db-username\"). This approach ensures sensitive data like usernames are securely injected into the container environment without hardcoding them into the container image or configuration.\\n\\nThe configuration demonstrates best practices for managing sensitive information, leveraging Kubernetes secrets to dynamically populate environment variables in containers. This method enhances security and flexibility by decoupling secret management from container runtime configurations.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-multiple-secret-env-variable.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('eb5588f5-8020-5f1f-8a41-7e0604f80734'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes namespace named \"production.\" The namespace is a logical partition within a Kubernetes cluster, used to organize and manage resources (such as pods, services, and deployments) separately from other parts of the cluster. The metadata section assigns the name \"production\" to the namespace and adds a label with the same name for easier identification and management. This setup helps create an isolated environment to run applications securely and efficiently.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: production\\n  labels:\\n    name: production\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes namespace named \"production.\" The namespace is a logical partition within a Kubernetes cluster, used to organize and manage resources (such as pods, services, and deployments) separately from other parts of the cluster. The metadata section assigns the name \"production\" to the namespace and adds a label with the same name for easier identification and management. This setup helps create an isolated environment to run applications securely and efficiently.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-prod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ebf1dda9-27f9-55a5-bb5c-f21afad47fc6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes manifest defines a Service resource named \"redis.\" The purpose of this service is to expose the Redis application within the cluster, enabling other pods to access it via a stable network endpoint. The configuration specifies that the service listens on port 6379, which is the default port for Redis, and forwards traffic to the same port on the selected pods. The selector \"app: redis\" links this service to the pods labeled with \"app: redis,\" ensuring that traffic directed to the service reaches the appropriate Redis instances. Overall, this setup facilitates internal communication with Redis containers, providing load balancing and stable network access within the Kubernetes environment.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: redis\\nspec:\\n  ports:\\n    - port: 6379\\n      targetPort: 6379\\n  selector:\\n    app: redis\\n', 'chunk': '1/1', 'summary': 'This Kubernetes manifest defines a Service resource named \"redis.\" The purpose of this service is to expose the Redis application within the cluster, enabling other pods to access it via a stable network endpoint. The configuration specifies that the service listens on port 6379, which is the default port for Redis, and forwards traffic to the same port on the selected pods. The selector \"app: redis\" links this service to the pods labeled with \"app: redis,\" ensuring that traffic directed to the service reaches the appropriate Redis instances. Overall, this setup facilitates internal communication with Redis containers, providing load balancing and stable network access within the Kubernetes environment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\redis-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ef067a7b-e665-5772-85b2-a5956d817bf2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Pod configuration in YAML format, which includes specifications for security settings and container details. The Pod, named \"violation-pod,\" is labeled accordingly and uses a specified seccomp profile (\"profiles/violation.json\") via the `securityContext` to enforce security restrictions at the kernel level, limiting system calls made by containers for security purposes. \\n\\nWithin the Pod, there is a single container named \"test-container\" that uses the \"hashicorp/http-echo:1.0\" image. This container is configured to run with a command that outputs the text \"just made some syscalls!\" when executed. Additionally, the container’s `securityContext` explicitly disallows privilege escalation (`allowPrivilegeEscalation: false`) to prevent processes from gaining elevated privileges, further enhancing container security.\\n\\nOverall, this configuration demonstrates how to implement Kubernetes security best practices by applying seccomp profiles and controlling privilege escalation within containers.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: violation-pod\\n  labels:\\n    app: violation-pod\\nspec:\\n  securityContext:\\n    seccompProfile:\\n      type: Localhost\\n      localhostProfile: profiles/violation.json\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:1.0\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\violation-pod.yaml', 'summary': 'This content describes a Kubernetes Pod configuration in YAML format, which includes specifications for security settings and container details. The Pod, named \"violation-pod,\" is labeled accordingly and uses a specified seccomp profile (\"profiles/violation.json\") via the `securityContext` to enforce security restrictions at the kernel level, limiting system calls made by containers for security purposes. \\n\\nWithin the Pod, there is a single container named \"test-container\" that uses the \"hashicorp/http-echo:1.0\" image. This container is configured to run with a command that outputs the text \"just made some syscalls!\" when executed. Additionally, the container’s `securityContext` explicitly disallows privilege escalation (`allowPrivilegeEscalation: false`) to prevent processes from gaining elevated privileges, further enhancing container security.\\n\\nOverall, this configuration demonstrates how to implement Kubernetes security best practices by applying seccomp profiles and controlling privilege escalation within containers.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f0b2a289-791b-520c-b758-5d65bcc0083a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes CronJob configuration, which schedules and runs jobs periodically in a Kubernetes cluster. It defines a CronJob named \"hello\" that executes every minute, following the cron schedule \"* * * * *\". The job runs a container based on the BusyBox image, which is a minimal Linux distribution often used for simple scripts. The container executes a command that displays the current date and outputs the message \"Hello from the Kubernetes cluster.\" The restart policy is set to \"OnFailure,\" meaning the job will restart only if it fails. This configuration automates periodic tasks in Kubernetes, useful for scheduled maintenance, notifications, or other repetitive operations.\\napiVersion: batch/v1\\nkind: CronJob\\nmetadata:\\n  name: hello\\nspec:\\n  schedule: \"* * * * *\"\\n  jobTemplate:\\n    spec:\\n      template:\\n        spec:\\n          containers:\\n          - name: hello\\n            image: busybox:1.28\\n            imagePullPolicy: IfNotPresent\\n            command:\\n            - /bin/sh\\n            - -c\\n            - date; echo Hello from the Kubernetes cluster\\n          restartPolicy: OnFailure\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes CronJob configuration, which schedules and runs jobs periodically in a Kubernetes cluster. It defines a CronJob named \"hello\" that executes every minute, following the cron schedule \"* * * * *\". The job runs a container based on the BusyBox image, which is a minimal Linux distribution often used for simple scripts. The container executes a command that displays the current date and outputs the message \"Hello from the Kubernetes cluster.\" The restart policy is set to \"OnFailure,\" meaning the job will restart only if it fails. This configuration automates periodic tasks in Kubernetes, useful for scheduled maintenance, notifications, or other repetitive operations.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\cronjob.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f0bf42f6-a168-51a1-9cbc-c3f6a4866b88'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content details the configuration and deployment of a custom Kubernetes scheduler named \"my-scheduler.\" It includes Kubernetes manifest files defining a ServiceAccount for the scheduler, RBAC (Role-Based Access Control) bindings to grant necessary permissions, a ConfigMap containing the scheduler\\'s configuration, and a Deployment to run the scheduler pod.\\n\\nThe ServiceAccount, named \"my-scheduler,\" operates within the \"kube-system\" namespace. Several RBAC RoleBindings associate this ServiceAccount with important cluster roles like \"system:kube-scheduler,\" \"system:volume-scheduler,\" and \"extension-apiserver-authentication-reader,\" thereby granting it appropriate permissions for scheduling, volume management, and API authentication.\\n\\nThe ConfigMap \"my-scheduler-config\" stores the scheduler\\'s configuration in YAML format, specifying API version, scheduler profile, and disabling leader election. The Deployment creates a single replica pod running the custom scheduler container, which uses the image from \"gcr.io/my-gcp-project/my-kube-scheduler:1.0\" and reads its configuration from the mounted ConfigMap. It includes health probes for liveness and readiness checks to ensure the scheduler\\'s proper functioning. The container runs with a specific command to start the kube-scheduler with the provided configuration, and the pod is configured to run in the network namespace without privileged access.\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: my-scheduler\\n  namespace: kube-system\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: my-scheduler-as-kube-scheduler\\nsubjects:\\n- kind: ServiceAccount\\n  name: my-scheduler\\n  namespace: kube-system\\nroleRef:\\n  kind: ClusterRole\\n  name: system:kube-scheduler\\n  apiGroup: rbac.authorization.k8s.io\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: my-scheduler-as-volume-scheduler\\nsubjects:\\n- kind: ServiceAccount\\n  name: my-scheduler\\n  namespace: kube-system\\nroleRef:\\n  kind: ClusterRole\\n  name: system:volume-scheduler\\n  apiGroup: rbac.authorization.k8s.io\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: my-scheduler-extension-apiserver-authentication-reader\\n  namespace: kube-system\\nroleRef:\\n  kind: Role\\n  name: extension-apiserver-authentication-reader\\n  apiGroup: rbac.authorization.k8s.io\\nsubjects:\\n- kind: ServiceAccount\\n  name: my-scheduler\\n  namespace: kube-system\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: my-scheduler-config\\n  namespace: kube-system\\ndata:\\n  my-scheduler-config.yaml: |\\n    apiVersion: kubescheduler.config.k8s.io/v1beta2\\n    kind: KubeSchedulerConfiguration\\n    profiles:\\n      - schedulerName: my-scheduler\\n    leaderElection:\\n      leaderElect: false\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    component: scheduler\\n    tier: control-plane\\n  name: my-scheduler\\n  namespace: kube-system\\nspec:\\n  selector:\\n    matchLabels:\\n      component: scheduler\\n      tier: control-plane\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        component: scheduler\\n        tier: control-plane\\n        version: second\\n    spec:\\n      serviceAccountName: my-scheduler\\n      containers:\\n      - command:\\n        - /usr/local/bin/kube-scheduler\\n        - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml\\n        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0\\n        livenessProbe:\\n          httpGet:\\n            path: /healthz\\n            port: 10259\\n            scheme: HTTPS\\n          initialDelaySeconds: 15\\n        name: kube-second-scheduler\\n        readinessProbe:\\n          httpGet:\\n            path: /healthz\\n            port: 10259\\n            scheme: HTTPS\\n        resources:\\n          requests:\\n            cpu: \\'0.1\\'\\n        securityContext:\\n          privileged: false\\n        volumeMounts:\\n          - name: config-volume\\n            mountPath: /etc/kubernetes/my-scheduler\\n      hostNetwork: false\\n      hostPID: false\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: my-scheduler-config\\n', 'subchunk': '1/1', 'summary': 'The provided content details the configuration and deployment of a custom Kubernetes scheduler named \"my-scheduler.\" It includes Kubernetes manifest files defining a ServiceAccount for the scheduler, RBAC (Role-Based Access Control) bindings to grant necessary permissions, a ConfigMap containing the scheduler\\'s configuration, and a Deployment to run the scheduler pod.\\n\\nThe ServiceAccount, named \"my-scheduler,\" operates within the \"kube-system\" namespace. Several RBAC RoleBindings associate this ServiceAccount with important cluster roles like \"system:kube-scheduler,\" \"system:volume-scheduler,\" and \"extension-apiserver-authentication-reader,\" thereby granting it appropriate permissions for scheduling, volume management, and API authentication.\\n\\nThe ConfigMap \"my-scheduler-config\" stores the scheduler\\'s configuration in YAML format, specifying API version, scheduler profile, and disabling leader election. The Deployment creates a single replica pod running the custom scheduler container, which uses the image from \"gcr.io/my-gcp-project/my-kube-scheduler:1.0\" and reads its configuration from the mounted ConfigMap. It includes health probes for liveness and readiness checks to ensure the scheduler\\'s proper functioning. The container runs with a specific command to start the kube-scheduler with the provided configuration, and the pod is configured to run in the network namespace without privileged access.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\my-scheduler.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f104d948-a0de-5837-8059-8b7145aac5fe'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Pod configuration that includes a health check mechanism. It defines a Pod named \"pod-with-http-healthcheck\" containing an nginx container. The configuration specifies a liveness probe using an HTTP GET request to the URL \"/_status/healthz\" on port 80. The liveness probe helps identify if the container is running correctly and needs to be restarted if it becomes unhealthy. The probe is configured with an initial delay of 30 seconds after startup, allowing the container sufficient time to initialize, and a timeout of 1 second for the health check response. Overall, this setup ensures that the Kubernetes system can monitor the container\\'s health and maintain reliability.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-http-healthcheck\\nspec:\\n  containers:\\n  - name: nginx\\n    image: nginx\\n    # defines the health checking\\n    livenessProbe:\\n      # an http probe\\n      httpGet:\\n        path: /_status/healthz\\n        port: 80\\n      # length of time to wait for a pod to initialize\\n      # after pod startup, before applying health checking\\n      initialDelaySeconds: 30\\n      timeoutSeconds: 1\\n    ports:\\n    - containerPort: 80\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Pod configuration that includes a health check mechanism. It defines a Pod named \"pod-with-http-healthcheck\" containing an nginx container. The configuration specifies a liveness probe using an HTTP GET request to the URL \"/_status/healthz\" on port 80. The liveness probe helps identify if the container is running correctly and needs to be restarted if it becomes unhealthy. The probe is configured with an initial delay of 30 seconds after startup, allowing the container sufficient time to initialize, and a timeout of 1 second for the health check response. Overall, this setup ensures that the Kubernetes system can monitor the container\\'s health and maintain reliability.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\pod-with-http-healthcheck.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f11aca34-27d6-5ed0-b728-9396151149c7'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML that defines resource constraints for a container. The pod, named `constraints-mem-demo-2`, contains a single container using the `nginx` image. It specifies resource requests and limits for memory: the container requests 800Mi of memory (the minimum guaranteed), and has a limit of 1.5Gi (the maximum it can use). These configurations help manage and optimize resource allocation within a Kubernetes cluster by ensuring the container gets sufficient memory while preventing it from consuming excessive resources.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-mem-demo-2\\nspec:\\n  containers:\\n  - name: constraints-mem-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"1.5Gi\"\\n      requests:\\n        memory: \"800Mi\"\\n', 'subchunk': '1/1', 'summary': 'This content is a Kubernetes Pod configuration written in YAML that defines resource constraints for a container. The pod, named `constraints-mem-demo-2`, contains a single container using the `nginx` image. It specifies resource requests and limits for memory: the container requests 800Mi of memory (the minimum guaranteed), and has a limit of 1.5Gi (the maximum it can use). These configurations help manage and optimize resource allocation within a Kubernetes cluster by ensuring the container gets sufficient memory while preventing it from consuming excessive resources.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-2.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f1e921b7-581a-5abe-9274-1374da890f21'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes StatefulSet for deploying a MySQL database cluster with three replicas, suited for high availability and replication setup. It leverages init containers and sidecar containers to initialize and clone data appropriately, ensuring each node has a unique server ID and proper replication configuration.\\n\\nThe init containers perform critical setup tasks: one generates a server ID based on the pod\\'s ordinal index, avoiding conflicts, and copies configuration files based on whether the pod is primary or a replica. The second init container, using the `xtrabackup` image, clones data from the preceding replica (except for the primary), streamlining the initialization process. The main MySQL container runs the server with resource requests and probes for liveness and readiness, ensuring the service stays healthy. Additionally, an `xtrabackup` sidecar handles incremental backups, manages replication positions based on binary log info, and waits for the MySQL server to be ready before starting replication.\\n\\nThe code efficiently sets up a self-healing, scalable MySQL environment with automated data cloning and replication, using multiple containers and configuration management tools to implement robust database clustering in Kubernetes.\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: mysql\\nspec:\\n  selector:\\n    matchLabels:\\n      app: mysql\\n      app.kubernetes.io/name: mysql\\n  serviceName: mysql\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: mysql\\n        app.kubernetes.io/name: mysql\\n    spec:\\n      initContainers:\\n      - name: init-mysql\\n        image: mysql:5.7\\n        command:\\n        - bash\\n        - \"-c\"\\n        - |\\n          set -ex\\n          # Generate mysql server-id from pod ordinal index.\\n          [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1\\n          ordinal=${BASH_REMATCH[1]}\\n          echo [mysqld] > /mnt/conf.d/server-id.cnf\\n          # Add an offset to avoid reserved server-id=0 value.\\n          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\n          if [[ $ordinal -eq 0 ]]; then\\n            cp /mnt/config-map/primary.cnf /mnt/conf.d/\\n          else\\n            cp /mnt/config-map/replica.cnf /mnt/conf.d/\\n          fi\\n        volumeMounts:\\n        - name: conf\\n          mountPath: /mnt/conf.d\\n        - name: config-map\\n          mountPath: /mnt/config-map\\n      - name: clone-mysql\\n        image: gcr.io/google-samples/xtrabackup:1.0\\n        command:\\n        - bash\\n        - \"-c\"\\n        - |\\n          set -ex\\n          # Skip the clone if data already exists.\\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\\n          # Skip the clone on primary (ordinal index 0).\\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\\n          ordinal=${BASH_REMATCH[1]}\\n          [[ $ordinal -eq 0 ]] && exit 0\\n          # Clone data from previous peer.\\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\\n          # Prepare the backup.\\n          xtrabackup --prepare --target-dir=/var/lib/mysql\\n        volumeMounts:\\n        - name: data\\n          mountPath: /var/lib/mysql\\n          subPath: mysql\\n        - name: conf\\n          mountPath: /etc/mysql/conf.d\\n      containers:\\n      - name: mysql\\n        image: mysql:5.7\\n        env:\\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\\n          value: \"1\"\\n        ports:\\n        - name: mysql\\n          containerPort: 3306\\n        volumeMounts:\\n        - name: data\\n          mountPath: /var/lib/mysql\\n          subPath: mysql\\n        - name: conf\\n          mountPath: /etc/mysql/conf.d\\n        resources:\\n          requests:\\n            cpu: 500m\\n            memory: 1Gi\\n        livenessProbe:\\n          exec:\\n            command: [\"mysqladmin\", \"ping\"]\\n          initialDelaySeconds: 30\\n          periodSeconds: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          exec:\\n            # Check we can execute queries over TCP (skip-networking is off).\\n            command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"]\\n          initialDelaySeconds: 5\\n          periodSeconds: 2\\n          timeoutSeconds: 1\\n      - name: xtrabackup\\n        image: gcr.io/google-samples/xtrabackup:1.0\\n        ports:\\n        - name: xtrabackup\\n          containerPort: 3307\\n        command:\\n        - bash\\n        - \"-c\"\\n        - |\\n          set -ex\\n          cd /var/lib/mysql\\n\\n          # Determine binlog position of cloned data, if any.\\n          if [[ -f xtrabackup_slave_info && \"x$(<xtrabackup_slave_info)\" != \"x\" ]]; then\\n            # XtraBackup already generated a partial \"CHANGE MASTER TO\" query\\n            # because we\\'re cloning from an existing replica. (Need to remove the tailing semicolon!)\\n            cat xtrabackup_slave_info | sed -E \\'s/;$//g\\' > change_master_to.sql.in\\n            # Ignore xtrabackup_binlog_info in this case (it\\'s useless).\\n            rm -f xtrabackup_slave_info xtrabackup_binlog_info\\n          elif [[ -f xtrabackup_binlog_info ]]; then\\n            # We\\'re cloning directly from primary. Parse binlog position.\\n            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n            rm -f xtrabackup_binlog_info xtrabackup_slave_info\\n            echo \"CHANGE MASTER TO MASTER_LOG_FILE=\\'${BASH_REMATCH[1]}\\',\\\\\\n                  MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in\\n          fi\\n\\n          # Check if we need to complete a clone by starting replication.\\n          if [[ -f change_master_to.sql.in ]]; then\\n            echo \"Waiting for mysqld to be ready (accepting connections)\"\\n            until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done\\n\\n            echo \"Initializing replication from clone position\"\\n            mysql -h 127.0.0.1 \\\\\\n                  -e \"$(<change_master_to.sql.in), \\\\\\n                          MASTER_HOST=\\'mysql-0.mysql\\', \\\\\\n                          MASTER_USER=\\'root\\', \\\\\\n                          MASTER_PASSWORD=\\'\\', \\\\\\n                          MASTER_CONNECT_RETRY=10; \\\\\\n                        START SLAVE;\" || exit 1\\n            # In case of container restart, attempt this at-most-once.\\n            mv change_master_to.sql.in change_master_to.sql.orig\\n          fi\\n\\n          # Start a server to send backups when requested by peers.\\n          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\\\\\n            \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\"\\n        volumeMounts:\\n        - name: data\\n          mountPath: /var/lib/mysql\\n          subPath: mysql\\n        - name: conf\\n          mountPath: /etc/mysql/conf.d\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n      volumes:\\n      - name: conf\\n        emptyDir: {}\\n      - name: config-map\\n        configMap:\\n          name: mysql\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: data\\n    spec:\\n      accessModes: [\"ReadWriteOnce\"]\\n      resources:\\n        requests:\\n          storage: 10Gi\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-statefulset.yaml', 'summary': \"The provided YAML configuration defines a Kubernetes StatefulSet for deploying a MySQL database cluster with three replicas, suited for high availability and replication setup. It leverages init containers and sidecar containers to initialize and clone data appropriately, ensuring each node has a unique server ID and proper replication configuration.\\n\\nThe init containers perform critical setup tasks: one generates a server ID based on the pod's ordinal index, avoiding conflicts, and copies configuration files based on whether the pod is primary or a replica. The second init container, using the `xtrabackup` image, clones data from the preceding replica (except for the primary), streamlining the initialization process. The main MySQL container runs the server with resource requests and probes for liveness and readiness, ensuring the service stays healthy. Additionally, an `xtrabackup` sidecar handles incremental backups, manages replication positions based on binary log info, and waits for the MySQL server to be ready before starting replication.\\n\\nThe code efficiently sets up a self-healing, scalable MySQL environment with automated data cloning and replication, using multiple containers and configuration management tools to implement robust database clustering in Kubernetes.\", 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f28f6781-789e-59ce-a2ec-b25d164e7f1d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod with a focus on security contexts. The pod is named \"security-context-demo\" and specifies user and group IDs at both the pod level and container level. The pod\\'s security context sets the user ID (1000), group ID (3000), and an additional group (2000) for all containers. It also assigns a supplemental group (4000), which provides extra group permissions. A volume is created using an empty directory, mounted inside the container at \"/data/demo\". \\n\\nWithin the container, the image used is BusyBox version 1.28, executing a command that keeps the container running for one hour. The container\\'s security context disables privilege escalation, enhancing security by preventing the container from gaining higher privileges. Overall, this configuration emphasizes applying security best practices by setting strict user/group permissions and disabling privilege escalation within a Kubernetes Pod.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo\\nspec:\\n  securityContext:\\n    runAsUser: 1000\\n    runAsGroup: 3000\\n    fsGroup: 2000\\n    supplementalGroups: [4000]\\n  volumes:\\n  - name: sec-ctx-vol\\n    emptyDir: {}\\n  containers:\\n  - name: sec-ctx-demo\\n    image: busybox:1.28\\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\\n    volumeMounts:\\n    - name: sec-ctx-vol\\n      mountPath: /data/demo\\n    securityContext:\\n      allowPrivilegeEscalation: false\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context.yaml', 'summary': 'This YAML configuration defines a Kubernetes Pod with a focus on security contexts. The pod is named \"security-context-demo\" and specifies user and group IDs at both the pod level and container level. The pod\\'s security context sets the user ID (1000), group ID (3000), and an additional group (2000) for all containers. It also assigns a supplemental group (4000), which provides extra group permissions. A volume is created using an empty directory, mounted inside the container at \"/data/demo\". \\n\\nWithin the container, the image used is BusyBox version 1.28, executing a command that keeps the container running for one hour. The container\\'s security context disables privilege escalation, enhancing security by preventing the container from gaining higher privileges. Overall, this configuration emphasizes applying security best practices by setting strict user/group permissions and disabling privilege escalation within a Kubernetes Pod.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f2da64d8-72bb-5312-b812-3361bc305ff2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Pod configuration in YAML format. The Pod is named \"counter\" and contains a single container called \"count\" that uses the BusyBox image. The container runs a shell script as its argument, which initializes a counter variable `i` to zero and enters an infinite loop. Inside the loop, it prints the current value of `i` along with the current date and time, then increments `i` by one and pauses for one second before repeating. This setup effectively creates a simple counter that outputs a timestamped count every second, useful for monitoring, testing, or demonstrating continuous output in a containerized environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: counter\\nspec:\\n  containers:\\n  - name: count\\n    image: busybox:1.28\\n    args: [/bin/sh, -c,\\n            \\'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 1; done\\']\\n', 'subchunk': '1/1', 'summary': 'This content describes a Kubernetes Pod configuration in YAML format. The Pod is named \"counter\" and contains a single container called \"count\" that uses the BusyBox image. The container runs a shell script as its argument, which initializes a counter variable `i` to zero and enters an infinite loop. Inside the loop, it prints the current value of `i` along with the current date and time, then increments `i` by one and pauses for one second before repeating. This setup effectively creates a simple counter that outputs a timestamped count every second, useful for monitoring, testing, or demonstrating continuous output in a containerized environment.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\counter-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f373925e-9212-5c60-b329-e9e1b92146c6'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"dnsutils\" in the default namespace. The Pod contains a single container using the \"agnhost:2.39\" image from the Kubernetes registry, which is typically used for testing and debugging network-related functions, such as DNS. The image pull policy is set to \"IfNotPresent,\" meaning the image will only be downloaded if it isn\\'t already available locally. The Pod has a restart policy of \"Always,\" ensuring it automatically restarts if it crashes or is terminated. Overall, this setup is useful for deploying a lightweight, network diagnostic utility within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: dnsutils\\n  namespace: default\\nspec:\\n  containers:\\n  - name: dnsutils\\n    image: registry.k8s.io/e2e-test-images/agnhost:2.39\\n    imagePullPolicy: IfNotPresent\\n  restartPolicy: Always\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"dnsutils\" in the default namespace. The Pod contains a single container using the \"agnhost:2.39\" image from the Kubernetes registry, which is typically used for testing and debugging network-related functions, such as DNS. The image pull policy is set to \"IfNotPresent,\" meaning the image will only be downloaded if it isn\\'t already available locally. The Pod has a restart policy of \"Always,\" ensuring it automatically restarts if it crashes or is terminated. Overall, this setup is useful for deploying a lightweight, network diagnostic utility within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dnsutils.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f38fa9ec-c43f-51c1-b68b-4d6f5d8bf168'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ValidatingAdmissionPolicyBinding, which links a specific validation policy to resources within a cluster. The purpose of this binding is to enforce rules during resource creation or modification, specifically to deny certain actions based on defined conditions.\\n\\nThe configuration specifies the policy’s name (\"replicalimit-policy.example.com\") and binds it to the \"replicalimit-binding-nontest\" resource. The validation action is set to \"Deny,\" meaning any requests that violate the policy will be rejected. It references a parameter set (\"replica-limit-prod.example.com\") in the default namespace that likely contains the policy\\'s parameters. The matchResources section filters resources based on their namespace labels, specifically ignoring namespaces with the label environment set to \"test\" (via the NotIn operator), so the policy applies to all other namespaces. This setup is useful for controlling resource quotas or limits in production environments while exempting testing namespaces.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicyBinding\\nmetadata:\\n  name: \"replicalimit-binding-nontest\"\\nspec:\\n  policyName: \"replicalimit-policy.example.com\"\\n  validationActions: [Deny]\\n  paramRef:\\n    name: \"replica-limit-prod.example.com\"\\n    namespace: \"default\"\\n  matchResources:\\n    namespaceSelector:\\n      matchExpressions:\\n      - key: environment\\n        operator: NotIn\\n        values:\\n        - test', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes ValidatingAdmissionPolicyBinding, which links a specific validation policy to resources within a cluster. The purpose of this binding is to enforce rules during resource creation or modification, specifically to deny certain actions based on defined conditions.\\n\\nThe configuration specifies the policy’s name (\"replicalimit-policy.example.com\") and binds it to the \"replicalimit-binding-nontest\" resource. The validation action is set to \"Deny,\" meaning any requests that violate the policy will be rejected. It references a parameter set (\"replica-limit-prod.example.com\") in the default namespace that likely contains the policy\\'s parameters. The matchResources section filters resources based on their namespace labels, specifically ignoring namespaces with the label environment set to \"test\" (via the NotIn operator), so the policy applies to all other namespaces. This setup is useful for controlling resource quotas or limits in production environments while exempting testing namespaces.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\binding-with-param-prod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f401a79f-1ca5-5cf9-8885-440df1db3462'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes YAML configuration defines a Pod named \"test-pod\" with specific scheduling gates and a container. The \"schedulingGates\" section includes two custom gates, \"example.com/foo\" and \"example.com/bar,\" which can be used to control or influence where and when the Pod is scheduled, allowing for more granular scheduling policies. The Pod contains a single container named \"pause\" that uses the \"pause\" image from the Kubernetes registry, commonly used as a placeholder or to hold network namespace for testing purposes. Overall, this configuration sets up a basic Pod with custom scheduling controls and a minimal container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: test-pod\\nspec:\\n  schedulingGates:\\n  - name: example.com/foo\\n  - name: example.com/bar\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.6\\n', 'chunk': '1/1', 'summary': 'This Kubernetes YAML configuration defines a Pod named \"test-pod\" with specific scheduling gates and a container. The \"schedulingGates\" section includes two custom gates, \"example.com/foo\" and \"example.com/bar,\" which can be used to control or influence where and when the Pod is scheduled, allowing for more granular scheduling policies. The Pod contains a single container named \"pause\" that uses the \"pause\" image from the Kubernetes registry, commonly used as a placeholder or to hold network namespace for testing purposes. Overall, this configuration sets up a basic Pod with custom scheduling controls and a minimal container.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-scheduling-gates.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f45a5c3c-6de7-5328-9493-081a6a1500cd'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Job configuration utilizing advanced indexing and failure management features introduced in recent versions. It defines a job with 10 total completions, executing them in parallel with up to 3 at a time, and employs an indexed completion mode enabling each job instance to be identified by an index. The key features include `backoffLimitPerIndex`, which limits the number of retries per individual index to 1, and `maxFailedIndexes`, which caps the total number of failed indexes at 5 before terminating the job. The job runs a Python container that intentionally fails for even-indexed tasks by checking the environment variable `JOB_COMPLETION_INDEX`, demonstrating how failure and retry limits are enforced at the index level. This configuration provides a robust way to manage multiple job attempts with granular failure control, suitable for tasks requiring retries with specific per-index failure limits.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-backoff-limit-per-index-example\\nspec:\\n  completions: 10\\n  parallelism: 3\\n  completionMode: Indexed  # required for the feature\\n  backoffLimitPerIndex: 1  # maximal number of failures per index\\n  maxFailedIndexes: 5      # maximal number of failed indexes before terminating the Job execution\\n  template:\\n    spec:\\n      restartPolicy: Never # required for the feature\\n      containers:\\n      - name: example\\n        image: python\\n        command:           # The jobs fails as there is at least one failed index\\n                           # (all even indexes fail in here), yet all indexes\\n                           # are executed as maxFailedIndexes is not exceeded.\\n        - python3\\n        - -c\\n        - |\\n          import os, sys\\n          print(\"Hello world\")\\n          if int(os.environ.get(\"JOB_COMPLETION_INDEX\")) % 2 == 0:\\n            sys.exit(1)\\n', 'chunk': '1/1', 'summary': 'The provided content is a Kubernetes Job configuration utilizing advanced indexing and failure management features introduced in recent versions. It defines a job with 10 total completions, executing them in parallel with up to 3 at a time, and employs an indexed completion mode enabling each job instance to be identified by an index. The key features include `backoffLimitPerIndex`, which limits the number of retries per individual index to 1, and `maxFailedIndexes`, which caps the total number of failed indexes at 5 before terminating the job. The job runs a Python container that intentionally fails for even-indexed tasks by checking the environment variable `JOB_COMPLETION_INDEX`, demonstrating how failure and retry limits are enforced at the index level. This configuration provides a robust way to manage multiple job attempts with granular failure control, suitable for tasks requiring retries with specific per-index failure limits.', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-backoff-limit-per-index-example.yaml', 'subchunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f4c16f42-d10d-5752-a82a-afd946015119'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Service named \"my-service\" that enables network access to a set of application pods. It specifies the use of IPv6 for IP addressing, ensuring compatibility with IPv6 networks. The service uses a selector to identify the pods labeled with \"app.kubernetes.io/name: MyApp,\" directing traffic to those pods. It exposes port 80 using the TCP protocol and forwards incoming traffic to the target port 9376 on the selected pods. This configuration facilitates load balancing and network routing within a Kubernetes cluster for the specified application.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\nspec:\\n  ipFamily: IPv6\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n      targetPort: 9376\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-ipv6-svc.yaml', 'summary': 'The provided YAML configuration defines a Kubernetes Service named \"my-service\" that enables network access to a set of application pods. It specifies the use of IPv6 for IP addressing, ensuring compatibility with IPv6 networks. The service uses a selector to identify the pods labeled with \"app.kubernetes.io/name: MyApp,\" directing traffic to those pods. It exposes port 80 using the TCP protocol and forwards incoming traffic to the target port 9376 on the selected pods. This configuration facilitates load balancing and network routing within a Kubernetes cluster for the specified application.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f647ddf8-0333-5e52-9f37-df82c5118aab'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"fine-pod\" with specific metadata, including labels and annotations for security profiling through seccomp. The pod contains a single container that uses the \"hashicorp/http-echo:0.2.3\" image, which is designed to echo a message (\"just made some syscalls!\") when run. The container’s security context explicitly disables privilege escalation, enhancing security by preventing the container from gaining higher privileges. Overall, this configuration illustrates how to set up a secure, custom-seccomp profile for a pod with a simple echo server container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fine-pod\\n  labels:\\n    app: fine-pod\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/fine-grained.json\\nspec:\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:0.2.3\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\fine-pod.yaml', 'summary': 'This content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"fine-pod\" with specific metadata, including labels and annotations for security profiling through seccomp. The pod contains a single container that uses the \"hashicorp/http-echo:0.2.3\" image, which is designed to echo a message (\"just made some syscalls!\") when run. The container’s security context explicitly disables privilege escalation, enhancing security by preventing the container from gaining higher privileges. Overall, this configuration illustrates how to set up a secure, custom-seccomp profile for a pod with a simple echo server container.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f671e31c-8334-56ae-818a-8f6f9804ea4a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod with specific affinity rules that influence pod scheduling based on existing pod placements and labels. The file includes both pod affinity and anti-affinity settings. \\n\\nPod affinity rules specify that the pod should be scheduled on a zone (`topology.kubernetes.io/zone`) where there are already pods with the label `security: S1`, ensuring proximity to similar pods. Conversely, pod anti-affinity rules express a preference (not a strict requirement) for avoiding zones with pods labeled `security: S2`, by assigning a high weight (100) to this preference, encouraging the scheduler to avoid placing this pod in such zones if possible. \\n\\nThe container runs a minimal pause image, serving as a placeholder or test container. Overall, this configuration aims to control pod placement by favoring certain zones based on labels, optimizing for proximity or segregation as needed.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: with-pod-affinity\\nspec:\\n  affinity:\\n    podAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n      - labelSelector:\\n          matchExpressions:\\n          - key: security\\n            operator: In\\n            values:\\n            - S1\\n        topologyKey: topology.kubernetes.io/zone\\n    podAntiAffinity:\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 100\\n        podAffinityTerm:\\n          labelSelector:\\n            matchExpressions:\\n            - key: security\\n              operator: In\\n              values:\\n              - S2\\n          topologyKey: topology.kubernetes.io/zone\\n  containers:\\n  - name: with-pod-affinity\\n    image: registry.k8s.io/pause:3.8\\n', 'chunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod with specific affinity rules that influence pod scheduling based on existing pod placements and labels. The file includes both pod affinity and anti-affinity settings. \\n\\nPod affinity rules specify that the pod should be scheduled on a zone (`topology.kubernetes.io/zone`) where there are already pods with the label `security: S1`, ensuring proximity to similar pods. Conversely, pod anti-affinity rules express a preference (not a strict requirement) for avoiding zones with pods labeled `security: S2`, by assigning a high weight (100) to this preference, encouraging the scheduler to avoid placing this pod in such zones if possible. \\n\\nThe container runs a minimal pause image, serving as a placeholder or test container. Overall, this configuration aims to control pod placement by favoring certain zones based on labels, optimizing for proximity or segregation as needed.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-pod-affinity.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f6c37523-baac-5a01-865a-e2068d4f888f'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content describes a Kubernetes Service configuration in YAML format. It defines a service named \"my-service\" with labels identifying its application. The service is set to prefer dual-stack networking using IPv6 and IPv4, enabling it to handle traffic over both protocols. It selects pods labeled with \"app.kubernetes.io/name: MyApp\" to route requests to, and exposes port 80 using the TCP protocol. This setup facilitates flexible network communication by supporting dual-stack IP addresses while providing a straightforward way to expose applications running within the cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\n  labels:\\n    app.kubernetes.io/name: MyApp\\nspec:\\n  ipFamilyPolicy: PreferDualStack\\n  ipFamilies:\\n  - IPv6\\n  - IPv4\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-preferred-ipfamilies-svc.yaml', 'summary': 'This content describes a Kubernetes Service configuration in YAML format. It defines a service named \"my-service\" with labels identifying its application. The service is set to prefer dual-stack networking using IPv6 and IPv4, enabling it to handle traffic over both protocols. It selects pods labeled with \"app.kubernetes.io/name: MyApp\" to route requests to, and exposes port 80 using the TCP protocol. This setup facilitates flexible network communication by supporting dual-stack IP addresses while providing a straightforward way to expose applications running within the cluster.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f77bbc1d-63bb-510e-9ca9-8bc24512b5b4'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a YAML configuration defining three custom resources of kind \"Shirt\" within a Kubernetes environment. Each resource specifies metadata including a unique name and a specification section that details the attributes of the shirt, such as its color and size. The configuration uses a custom API version \"stable.example.com/v1\", indicating that these are custom resource definitions (CRDs) managed within a Kubernetes cluster.\\n\\nThe code describes three shirt objects: \"example1\" (blue, size S), \"example2\" (blue, size M), and \"example3\" (green, size M). This setup could be used for managing a collection of shirts in an application or system that interacts with these custom resources, allowing users to specify different shirt configurations through declarative YAML manifests.\\n---\\napiVersion: stable.example.com/v1\\nkind: Shirt\\nmetadata:\\n  name: example1\\nspec:\\n  color: blue\\n  size: S\\n---\\napiVersion: stable.example.com/v1\\nkind: Shirt\\nmetadata:\\n  name: example2\\nspec:\\n  color: blue\\n  size: M\\n---\\napiVersion: stable.example.com/v1\\nkind: Shirt\\nmetadata:\\n  name: example3\\nspec:\\n  color: green\\n  size: M\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resources.yaml', 'summary': 'The provided content is a YAML configuration defining three custom resources of kind \"Shirt\" within a Kubernetes environment. Each resource specifies metadata including a unique name and a specification section that details the attributes of the shirt, such as its color and size. The configuration uses a custom API version \"stable.example.com/v1\", indicating that these are custom resource definitions (CRDs) managed within a Kubernetes cluster.\\n\\nThe code describes three shirt objects: \"example1\" (blue, size S), \"example2\" (blue, size M), and \"example3\" (green, size M). This setup could be used for managing a collection of shirts in an application or system that interacts with these custom resources, allowing users to specify different shirt configurations through declarative YAML manifests.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f7c9b4ac-20e1-5358-984a-8e1fcdc0ca38'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes ConfigMap resource, which is used to manage configuration data separately from application code. The ConfigMap is named \"special-config\" and resides in the \"default\" namespace. It contains key-value pairs in the data section, with keys \"SPECIAL_LEVEL\" and \"SPECIAL_TYPE\" assigned the values \"very\" and \"charm,\" respectively. This setup allows applications running within the cluster to reference these configuration values dynamically, promoting better separation of concerns and easier configuration management.\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: special-config\\n  namespace: default\\ndata:\\n  SPECIAL_LEVEL: very\\n  SPECIAL_TYPE: charm\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes ConfigMap resource, which is used to manage configuration data separately from application code. The ConfigMap is named \"special-config\" and resides in the \"default\" namespace. It contains key-value pairs in the data section, with keys \"SPECIAL_LEVEL\" and \"SPECIAL_TYPE\" assigned the values \"very\" and \"charm,\" respectively. This setup allows applications running within the cluster to reference these configuration values dynamically, promoting better separation of concerns and easier configuration management.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configmap-multikeys.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('f7e916d2-01c7-560d-b0a6-ef4416441065'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Namespace resource configuration in YAML format. It creates a namespace named \"my-baseline-namespace\" and applies specific security labels related to pod security standards. These labels enforce or warn about security policies at the baseline level, using the latest version of standards. The labels help ensure that Pods within this namespace adhere to predefined security policies, promoting a consistent security baseline across the environment.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: my-baseline-namespace\\n  labels:\\n    pod-security.kubernetes.io/enforce: baseline\\n    pod-security.kubernetes.io/enforce-version: latest\\n    pod-security.kubernetes.io/warn: baseline\\n    pod-security.kubernetes.io/warn-version: latest', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-baseline.yaml', 'summary': 'This content defines a Kubernetes Namespace resource configuration in YAML format. It creates a namespace named \"my-baseline-namespace\" and applies specific security labels related to pod security standards. These labels enforce or warn about security policies at the baseline level, using the latest version of standards. The labels help ensure that Pods within this namespace adhere to predefined security policies, promoting a consistent security baseline across the environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fa07d360-3aab-5811-a0a0-caf3002eb13a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided content is a Kubernetes Job configuration written in YAML format. It defines a batch job named \"job-wq-1\" designed to execute multiple instances of a containerized workload. The job specifies that it should run a total of 8 completions, with 2 jobs running in parallel at a time. The container within the pod runs an image hosted on Google Container Registry, and it is configured with environment variables for connecting to a RabbitMQ message broker, indicating that the job likely involves message queue processing. The restart policy is set to \"OnFailure,\" meaning a pod will be restarted only if it fails during execution. This configuration enables scalable and fault-tolerant message-processing workloads within a Kubernetes cluster.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-wq-1\\nspec:\\n  completions: 8\\n  parallelism: 2\\n  template:\\n    metadata:\\n      name: job-wq-1\\n    spec:\\n      containers:\\n      - name: c\\n        image: gcr.io/<project>/job-wq-1\\n        env:\\n        - name: BROKER_URL\\n          value: amqp://guest:guest@rabbitmq-service:5672\\n        - name: QUEUE\\n          value: job1\\n      restartPolicy: OnFailure\\n', 'subchunk': '1/1', 'summary': 'The provided content is a Kubernetes Job configuration written in YAML format. It defines a batch job named \"job-wq-1\" designed to execute multiple instances of a containerized workload. The job specifies that it should run a total of 8 completions, with 2 jobs running in parallel at a time. The container within the pod runs an image hosted on Google Container Registry, and it is configured with environment variables for connecting to a RabbitMQ message broker, indicating that the job likely involves message queue processing. The restart policy is set to \"OnFailure,\" meaning a pod will be restarted only if it fails during execution. This configuration enables scalable and fault-tolerant message-processing workloads within a Kubernetes cluster.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\job.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fa27ddb9-7cc1-5707-97f0-14ade65c8138'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes PodSecurityPolicy resource, which enforces security guidelines for pod execution. The policy, named \"example,\" disallows privileged pods to enhance security. It permits any SELinux context, supplemental groups, user IDs, and filesystem groups by setting their rules to \"RunAsAny,\" meaning there are no restrictions for these configurations. The policy also allows any volume types by specifying volumes as \\'*\\' in the configuration. Overall, this PodSecurityPolicy provides a flexible security framework while specifically preventing privileged container execution.\\napiVersion: policy/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: example\\nspec:\\n  privileged: false  # Don\\'t allow privileged pods!\\n  # The rest fills in some required fields.\\n  seLinux:\\n    rule: RunAsAny\\n  supplementalGroups:\\n    rule: RunAsAny\\n  runAsUser:\\n    rule: RunAsAny\\n  fsGroup:\\n    rule: RunAsAny\\n  volumes:\\n  - \\'*\\'\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes PodSecurityPolicy resource, which enforces security guidelines for pod execution. The policy, named \"example,\" disallows privileged pods to enhance security. It permits any SELinux context, supplemental groups, user IDs, and filesystem groups by setting their rules to \"RunAsAny,\" meaning there are no restrictions for these configurations. The policy also allows any volume types by specifying volumes as \\'*\\' in the configuration. Overall, this PodSecurityPolicy provides a flexible security framework while specifically preventing privileged container execution.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\example-psp.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('faf0eaeb-ad79-5345-adf0-18e6814c3d66'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Service for RabbitMQ. It specifies the API version and type of resource, with metadata such as labels and a name for the service. The specification includes a port (5672), which is the default port for RabbitMQ messaging, and a selector that matches pods labeled with specific app identifiers related to the task queue and RabbitMQ components. This setup enables network access to RabbitMQ instances running within the cluster, allowing other applications to connect via the specified port through this service.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  labels:\\n    component: rabbitmq\\n  name: rabbitmq-service\\nspec:\\n  ports:\\n  - port: 5672\\n  selector:\\n    app.kubernetes.io/name: task-queue\\n    app.kubernetes.io/component: rabbitmq\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Service for RabbitMQ. It specifies the API version and type of resource, with metadata such as labels and a name for the service. The specification includes a port (5672), which is the default port for RabbitMQ messaging, and a selector that matches pods labeled with specific app identifiers related to the task queue and RabbitMQ components. This setup enables network access to RabbitMQ instances running within the cluster, allowing other applications to connect via the specified port through this service.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\rabbitmq-service.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fb20f795-a908-57e0-a58a-ccb90ced0678'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content is a Kubernetes Deployment configuration written in YAML, aimed at deploying three replicas of a containerized application. The deployment creates pods with a single Alpine Linux container that retrieves an environment variable from a ConfigMap named \"fruits.\" The environment variable \"FRUITS\" gets its value from the key \"fruits\" within this ConfigMap. The container runs an infinite loop, periodically outputting the current date and a message indicating the contents of the fruit basket, then sleeps for ten seconds before repeating. Essentially, this setup demonstrates how to inject configuration data via ConfigMaps into containers and how to run a continuous process that utilizes environment variables fetched from external configurations.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-env-var\\n  labels:\\n    app.kubernetes.io/name: configmap-env-var\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-env-var\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-env-var\\n    spec:\\n      containers:\\n        - name: alpine\\n          image: alpine:3\\n          env:\\n            - name: FRUITS\\n              valueFrom:\\n                configMapKeyRef:\\n                  key: fruits\\n                  name: fruits\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) The basket is full of $FRUITS\";\\n                sleep 10; done;\\n          ports:\\n            - containerPort: 80', 'chunk': '1/1', 'summary': 'This content is a Kubernetes Deployment configuration written in YAML, aimed at deploying three replicas of a containerized application. The deployment creates pods with a single Alpine Linux container that retrieves an environment variable from a ConfigMap named \"fruits.\" The environment variable \"FRUITS\" gets its value from the key \"fruits\" within this ConfigMap. The container runs an infinite loop, periodically outputting the current date and a message indicating the contents of the fruit basket, then sleeps for ten seconds before repeating. Essentially, this setup demonstrates how to inject configuration data via ConfigMaps into containers and how to run a continuous process that utilizes environment variables fetched from external configurations.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-envvar.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fbc66c5b-6169-5dc4-a20d-84af9688b654'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes StorageClass resource for Azure Files. It specifies the API version and resource type, with metadata indicating its name as \"azurefile.\" The provisioner used is \"kubernetes.io/azure-file,\" which enables dynamic provisioning of Azure File shares. The parameters include the SKU (Standard_LRS), the storage location (eastus), and a placeholder for the storage account name. This setup allows Kubernetes to automatically provision Azure File shares with the specified settings, facilitating persistent storage for containers in a cloud environment.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: azurefile\\nprovisioner: kubernetes.io/azure-file\\nparameters:\\n  skuName: Standard_LRS\\n  location: eastus\\n  storageAccount: azure_storage_account_name # example value\\n', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-azure-file.yaml', 'summary': 'This YAML configuration defines a Kubernetes StorageClass resource for Azure Files. It specifies the API version and resource type, with metadata indicating its name as \"azurefile.\" The provisioner used is \"kubernetes.io/azure-file,\" which enables dynamic provisioning of Azure File shares. The parameters include the SKU (Standard_LRS), the storage location (eastus), and a placeholder for the storage account name. This setup allows Kubernetes to automatically provision Azure File shares with the specified settings, facilitating persistent storage for containers in a cloud environment.', 'subchunk': '1/1', 'chunk': '1/1'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fc155af9-be95-5718-9206-16363c82f47b'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This YAML configuration defines a Kubernetes Pod named \"iis\" that runs a container using the Microsoft IIS image based on Windows Server Core 1709. The container is configured to expose port 80 for web server traffic. Additionally, the Pod is scheduled on nodes with the Windows operating system, as specified by the node selector. This setup is typically used to deploy a Windows-based IIS web server in a Kubernetes environment, enabling web hosting and application deployment in a containerized infrastructure.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: iis\\n  labels:\\n    name: iis\\nspec:\\n  containers:\\n    - name: iis\\n      image: microsoft/iis:windowsservercore-1709\\n      ports:\\n        - containerPort: 80\\n  nodeSelector:\\n    \"kubernetes.io/os\": windows\\n', 'subchunk': '1/1', 'summary': 'This YAML configuration defines a Kubernetes Pod named \"iis\" that runs a container using the Microsoft IIS image based on Windows Server Core 1709. The container is configured to expose port 80 for web server traffic. Additionally, the Pod is scheduled on nodes with the Windows operating system, as specified by the node selector. This setup is typically used to deploy a Windows-based IIS web server in a Kubernetes environment, enabling web hosting and application deployment in a containerized infrastructure.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\simple-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fe46714e-c107-5016-ad9c-b4b5378b96a2'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes StorageClass resource named \"fast\" using the RADOS Block Device (RBD) provisioner for Ceph storage. It specifies configuration parameters such as the Ceph monitor address, admin and user credentials secret names, the storage pool, and filesystem type. The StorageClass enables dynamic provisioning of persistent volumes with specific performance and storage characteristics, although the provisioner used (`kubernetes.io/rbd`) is marked as deprecated. It also includes settings for image format and features to optimize storage layering.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: fast\\nprovisioner: kubernetes.io/rbd # This provisioner is deprecated\\nparameters:\\n  monitors: 198.19.254.105:6789\\n  adminId: kube\\n  adminSecretName: ceph-secret\\n  adminSecretNamespace: kube-system\\n  pool: kube\\n  userId: kube\\n  userSecretName: ceph-secret-user\\n  userSecretNamespace: default\\n  fsType: ext4\\n  imageFormat: \"2\"\\n  imageFeatures: \"layering\"\\n', 'subchunk': '1/1', 'summary': 'This content defines a Kubernetes StorageClass resource named \"fast\" using the RADOS Block Device (RBD) provisioner for Ceph storage. It specifies configuration parameters such as the Ceph monitor address, admin and user credentials secret names, the storage pool, and filesystem type. The StorageClass enables dynamic provisioning of persistent volumes with specific performance and storage characteristics, although the provisioner used (`kubernetes.io/rbd`) is marked as deprecated. It also includes settings for image format and features to optimize storage layering.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-ceph-rbd.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('fe9f437e-1d7d-5dbc-a53f-eb0acd707397'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This content defines a Kubernetes Pod configuration using YAML. The pod is named \"memory-demo\" within the \"mem-example\" namespace and contains a single container based on the \"polinux/stress\" image. The container is configured with resource requests and limits for memory: it requests 100MiB and limits it to 200MiB. The container runs the \"stress\" command with specific arguments to simulate memory pressure, instructing it to create a single worker (`--vm 1`) that consumes 150MiB of RAM (`--vm-bytes 150M`) and includes a hang period (`--vm-hang 1`) to prolong the stress condition. This setup is typically used for testing how Kubernetes handles memory stress scenarios within a container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: memory-demo\\n  namespace: mem-example\\nspec:\\n  containers:\\n  - name: memory-demo-ctr\\n    image: polinux/stress\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n      limits:\\n        memory: \"200Mi\"\\n    command: [\"stress\"]\\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"]\\n', 'chunk': '1/1', 'summary': 'This content defines a Kubernetes Pod configuration using YAML. The pod is named \"memory-demo\" within the \"mem-example\" namespace and contains a single container based on the \"polinux/stress\" image. The container is configured with resource requests and limits for memory: it requests 100MiB and limits it to 200MiB. The container runs the \"stress\" command with specific arguments to simulate memory pressure, instructing it to create a single worker (`--vm 1`) that consumes 150MiB of RAM (`--vm-bytes 150M`) and includes a hang period (`--vm-hang 1`) to prolong the stress condition. This setup is typically used for testing how Kubernetes handles memory stress scenarios within a container.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('feeae62e-55ef-509f-b73d-bcd24ad1aa0a'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# The provided YAML configuration defines a Kubernetes Pod named \"quota-mem-cpu-demo\" with a single container running the nginx image. The resource specifications set both limits and requests for memory and CPU. Specifically, the container requests 600Mi of memory and 400m (millicores) of CPU, ensuring these resources are allocated when the Pod runs. It also sets limits of 800Mi of memory and 800m of CPU to restrict the maximum resources the container can consume. This configuration helps manage resource utilization and ensures the container operates within specified boundaries.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: quota-mem-cpu-demo\\nspec:\\n  containers:\\n  - name: quota-mem-cpu-demo-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"800Mi\"\\n        cpu: \"800m\"\\n      requests:\\n        memory: \"600Mi\"\\n        cpu: \"400m\"\\n', 'subchunk': '1/1', 'summary': 'The provided YAML configuration defines a Kubernetes Pod named \"quota-mem-cpu-demo\" with a single container running the nginx image. The resource specifications set both limits and requests for memory and CPU. Specifically, the container requests 600Mi of memory and 400m (millicores) of CPU, ensuring these resources are allocated when the Pod runs. It also sets limits of 800Mi of memory and 800m of CPU to restrict the maximum resources the container can consume. This configuration helps manage resource utilization and ensures the container operates within specified boundaries.', 'chunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu-pod.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n",
      "Object(uuid=_WeaviateUUIDInt('ff46191e-2909-54f9-b9f1-fda20f3f0aec'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# This Kubernetes manifest defines a Pod named \"command-demo\" that contains a container running the Debian image. The container is configured to execute the `printenv` command with specific environment variables (`HOSTNAME` and `KUBERNETES_PORT`) as arguments. The purpose is to display the values of these environment variables when the container runs. The restart policy is set to \"OnFailure,\" meaning the Pod will automatically restart only if the container encounters an error during execution. In essence, the code demonstrates how to run a simple command within a container to output environment variables in a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: command-demo\\n  labels:\\n    purpose: demonstrate-command\\nspec:\\n  containers:\\n  - name: command-demo-container\\n    image: debian\\n    command: [\"printenv\"]\\n    args: [\"HOSTNAME\", \"KUBERNETES_PORT\"]\\n  restartPolicy: OnFailure\\n', 'chunk': '1/1', 'summary': 'This Kubernetes manifest defines a Pod named \"command-demo\" that contains a container running the Debian image. The container is configured to execute the `printenv` command with specific environment variables (`HOSTNAME` and `KUBERNETES_PORT`) as arguments. The purpose is to display the values of these environment variables when the container runs. The restart policy is set to \"OnFailure,\" meaning the Pod will automatically restart only if the container encounters an error during execution. In essence, the code demonstrates how to run a simple command within a container to output environment variables in a Kubernetes environment.', 'subchunk': '1/1', 'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\commands.yaml'}, references=None, vector={}, collection='Kubernetes_code')\n"
     ]
    }
   ],
   "source": [
    "with weaviate_helper.connect() as client:\n",
    "    collection = client.collections.get(name=\"kubernetes_code\")\n",
    "    for obj in collection.iterator():\n",
    "        print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9333cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Kubernetes_fundamentals \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://127.0.0.1:8080/v1/schema/Kubernetes_fundamentals \"HTTP/1.1 200 OK\"\n",
      "INFO:src.ingest.ingest_helper:Collection 'kubernetes_fundamentals' deleted.\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Kubernetes_code \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://127.0.0.1:8080/v1/schema/Kubernetes_code \"HTTP/1.1 200 OK\"\n",
      "INFO:src.ingest.ingest_helper:Collection 'kubernetes_code' deleted.\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Kubernetes_fundamentals \"HTTP/1.1 404 Not Found\"\n",
      "d:\\Python\\MasterIA\\TFM\\TFM\\.venv\\lib\\site-packages\\weaviate\\collections\\classes\\config.py:1950: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8080/v1/schema \"HTTP/1.1 200 OK\"\n",
      "INFO:src.database.weviate:Collection 'kubernetes_fundamentals' created.\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\01-prerequisites.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "[{'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\01-prerequisites.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\02-jumpbox.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\03-compute-resources.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\04-certificate-authority.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\05-kubernetes-configuration-files.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\06-data-encryption-keys.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\07-bootstrapping-etcd.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\08-bootstrapping-kubernetes-controllers.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\09-bootstrapping-kubernetes-workers.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\10-configuring-kubectl.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\11-pod-network-routes.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\12-smoke-test.md', 'content': '', 'file_extension': 'md'}, {'path': 'knowledge\\\\kubernetes\\\\kubernetes-the-hard-way-master\\\\docs\\\\13-cleanup.md', 'content': '', 'file_extension': 'md'}]\n",
      "***************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 2 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\02-jumpbox.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=3\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 3 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\03-compute-resources.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=8\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 8 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\04-certificate-authority.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=4\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 4 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\05-kubernetes-configuration-files.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=5\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 5 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\06-data-encryption-keys.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=3\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 3 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\07-bootstrapping-etcd.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=4\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 4 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\08-bootstrapping-kubernetes-controllers.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=5\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 5 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\09-bootstrapping-kubernetes-workers.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=5\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 5 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\10-configuring-kubectl.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=3\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 3 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\11-pod-network-routes.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=3\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 3 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\12-smoke-test.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=6\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 6 docs\n",
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge\\kubernetes\\kubernetes-the-hard-way-master\\docs\\13-cleanup.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 2 docs\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Kubernetes_fundamentals \"HTTP/1.1 200 OK\"\n",
      "INFO:src.ingest.ingest_helper:Documents inserted into collection 'kubernetes_fundamentals'.\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Kubernetes_code \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8080/v1/schema \"HTTP/1.1 200 OK\"\n",
      "INFO:src.database.weviate:Collection 'kubernetes_code' created.\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-dev.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\namespace-prod.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-available-cgroupv2.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\admin\\resource\\memory-available.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\Dockerfile' skipped. Reason: File extension 'Dockerfile' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\rabbitmq\\worker.py' skipped. Reason: File extension 'py' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\Dockerfile' skipped. Reason: File extension 'Dockerfile' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\rediswq.py' skipped. Reason: File extension 'py' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\application\\job\\redis\\worker.py' skipped. Reason: File extension 'py' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\game-env-file.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\game.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\ui-env-file.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\configmap\\ui.properties' skipped. Reason: File extension 'properties' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\examples.go' skipped. Reason: File extension 'go' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\examples_test.go' skipped. Reason: File extension 'go' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\config\\redis-config' skipped. Reason: File extension 'redis-config' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\audit.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\fine-grained.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\pods\\security\\seccomp\\profiles\\violation.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\README.md' skipped. Reason: File extension 'md' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\kind-with-cluster-level-baseline-pod-security.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\security\\kind-with-namespace-level-baseline-pod-security.sh' skipped. Reason: File extension 'sh' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\Dockerfile' skipped. Reason: File extension 'Dockerfile' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\service\\access\\frontend-nginx.conf' skipped. Reason: File extension 'conf' not in include list ['yaml', 'yml']\n",
      "INFO:src.ingest.ingest_helper:File 'knowledge\\kubernetes\\website-main\\content\\en\\examples\\tls\\server-signing-config.json' skipped. Reason: File extension 'json' not in include list ['yaml', 'yml']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "[{'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-approve.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-create.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-sign.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\deployment-replicas-policy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\endpoints-aggregated.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\image-matches-namespace-environment.policy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-clusterrole.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-clusterrolebinding.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-role.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-rolebinding-with-clusterrole.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\simple-rolebinding-with-role.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-audit-annotation.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-match-conditions.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\cloud\\\\ccm-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\busybox.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dns-horizontal-autoscaler.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dnsutils.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\egress-selector-configuration.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-agent.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-rbac.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\konnectivity\\\\konnectivity-server.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\fluentd-sidecar-config.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-agent-sidecar.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-streaming-sidecar.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-dev.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-prod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-4.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-defaults.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-mem-cpu-container.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-mem-cpu-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-memory-ratio-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-1.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod-4.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-constraints.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\memory-defaults.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\pvc-limit-greater.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\pvc-limit-lower.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-mem-cpu.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects-pvc-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects-pvc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-objects.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\storagelimits.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\clusterrole.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\my-scheduler.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod1.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\pod3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\snowflake-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\basic-daemonset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-statefulset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-patch.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-retainkeys.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-scale.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-sidecar.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-update.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\hpa\\\\php-apache.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\cronjob.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job-vol.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-sidecar.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-tmpl.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\job.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\rabbitmq-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\rabbitmq\\\\rabbitmq-statefulset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\job.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\redis-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\redis-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-pv.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-services.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-statefulset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-app.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-with-request.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\php-apache.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\shell-demo.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\simple_deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment-no-replicas.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\update_deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web-parallel.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\mysql-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\wordpress-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\zookeeper\\\\zookeeper.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\audit\\\\audit-policy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-conflict-with-limitrange-cpu.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-no-conflict-with-limitrange-cpu.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\problematic-limit-range.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configmap-multikeys.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configmaps.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\configure-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\env-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\immutable-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\configmap\\\\new-immutable-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\daemonset-label-selector.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\daemonset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset-update.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\frontend.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\hpa-rs.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-backoff-limit-per-index-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-config-issue.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-failjob.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-pod-failure-policy-ignore.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-success-policy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\nginx-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replicaset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.14.2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.16.1.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resource-definition.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resources.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\counter-pod-err.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\counter-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\event-exporter.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\fluentd-gcp-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\fluentd-gcp-ds.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\node-problem-detector-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\node-problem-detector.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\termination.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-capacity-reservation.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-and-sidecar-container.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-envvar.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-volume.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-two-containers.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-immutable-configmap-as-volume.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\mutatingadmissionpolicy\\\\applyconfiguration-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\mutatingadmissionpolicy\\\\json-patch-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\commands.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\example-redis-config.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\config\\\\redis-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\image-volumes.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\init-containers.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-envars-container.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-envars-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume-resources.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dependent-envars.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\envars.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-multiple-secret-env-variable.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-secret-envFrom.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\pod-single-secret-env-variable.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret-envars-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\lifecycle-events.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-env-var-valueFrom.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-envFrom.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-volume-specific-key.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-configmap-volume.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-multiple-configmap-env-variable.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-preferred-affinity.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-required-affinity.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-specific-node.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-projected-svc-token.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-rs.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-single-configmap-env-variable.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-affinity-preferred-weight.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-node-affinity.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-pod-affinity.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-scheduling-gates.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-toleration.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-without-scheduling-gates.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\private-reg-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\exec-liveness.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\grpc-liveness.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\http-liveness.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\pod-with-http-healthcheck.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\pod-with-tcp-socket-healthcheck.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\probe\\\\tcp-liveness-readiness.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-4.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-5.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-cpu-request-limit.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-memory-request-limit.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-resources.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\hello-apparmor.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\audit-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\default-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\fine-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\violation-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\fields.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\audit-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\default-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\fine-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\ga\\\\violation-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\kind.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-2.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-3.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-4.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-5.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-6.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\share-process-namespace.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\simple-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-clustertrustbundle.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-secret-downwardapi-configmap.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-secrets-nondefault-permission-mode.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected-service-account-token.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\projected.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-claim.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-duplicate.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\pv-volume.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\storage\\\\redis.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint-with-nodeaffinity.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\two-constraints.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\two-container-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\user-namespaces-stateless.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\baseline-psp.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\example-psp.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\high-priority-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\priority-class-resourcequota.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\privileged-psp.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\quota.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\restricted-psp.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\zookeeper-pod-disruption-budget-maxunavailable.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\zookeeper-pod-disruption-budget-minavailable.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priority-and-fairness\\\\health-for-strangers.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priority-and-fairness\\\\list-events-default-service-account.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priorityclass\\\\low-priority-class.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\basicauth-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\bootstrap-token-secret-base64.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\bootstrap-token-secret-literal.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\dockercfg-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\dotfile-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\optional-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\serviceaccount\\\\mysecretname.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\serviceaccount-token-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\ssh-auth-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\secret\\\\tls-auth-secret.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\example-baseline-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-baseline.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-privileged.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-restricted.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-deployment.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\hello-application.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\explore-graceful-termination-nginx.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\load-balancer-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\curlpod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\custom-dns.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\default-ingressclass.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-default-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-ipfamilies-ipv6.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-ipv6-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-prefer-ipv6-lb-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-preferred-ipfamilies-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-preferred-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\example-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\external-lb.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\hostaliases-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\ingress-resource-backend.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\ingress-wildcard-host.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\minimal-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\name-virtual-host-ingress-no-third-host.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\name-virtual-host-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\namespaced-params.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-allow-all-egress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-allow-all-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-all.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-egress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\network-policy-default-deny-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\networkpolicy-multiport-egress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\networkpolicy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-policy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-secure-app.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-svc.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\run-my-nginx.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\simple-fanout-example.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\test-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\tls-example-ingress.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\nginx-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\pod-with-graceful-termination.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\simple-service.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\rro.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\pod-volume-binding.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-aws-ebs.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-aws-efs.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-azure-file.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-ceph-rbd.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-local.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-nfs.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-portworx-volume.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass\\\\storageclass-topology.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\storage\\\\storageclass-low-latency.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\basic-example-binding.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\basic-example-policy.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\binding-with-param-prod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\binding-with-param.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\failure-policy-ignore.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\policy-with-param.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param-prod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\typechecking-multiple-match.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\typechecking.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\configmap-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\daemonset.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-hyperv.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-resource.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\emptydir-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\hostpath-volume-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\run-as-username-container.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\run-as-username-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\secret-pod.yaml', 'content': '', 'file_extension': 'yaml'}, {'path': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\simple-pod.yaml', 'content': '', 'file_extension': 'yaml'}]\n",
      "***************************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\Python\\MasterIA\\TFM\\TFM\\TFM\\src\\ingest\\ingest_helper.py:180\u001b[0m, in \u001b[0;36mIngestHelper.ingest_knowledge\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_generate_all_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs_for_summary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mingest_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_knowledge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\MasterIA\\TFM\\TFM\\TFM\\src\\ingest\\ingest_helper.py:182\u001b[0m, in \u001b[0;36mIngestHelper.ingest_knowledge\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_generate_all_summaries(docs_for_summary))\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_generate_all_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs_for_summary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(docs_for_summary, summaries):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "ingest_helper.ingest_knowledge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e5afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://127.0.0.1:8080/v1/schema/Test \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "with weaviate_helper.connect() as client:\n",
    "    client.collections.delete(name=\"test\")\n",
    "\n",
    "weaviate_helper.create_collection(collection_name=\"test\", description=\"test description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8718c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.embeddings.late_chunking:chunk_file: processing path=knowledge/kubernetes/website-main/content/en/docs/reference/kubectl/quick-reference.md\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: docs=23\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]\n",
      "INFO:src.embeddings.late_chunking:generate_late_chunking_embeddings: completed embeddings for 23 docs\n"
     ]
    }
   ],
   "source": [
    "docs, canonical_doc = late_chunking_helper.chunk_file(\n",
    "    path=\"knowledge/kubernetes/website-main/content/en/docs/reference/kubectl/quick-reference.md\",\n",
    ")\n",
    "docs = late_chunking_helper.generate_late_chunking_embeddings(docs, canonical_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9813ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Test \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "weaviate_helper.batch_insert(docs, collection_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c73f37c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.embeddings.late_chunking:generate_query_embedding: query='deployment with horizontal scaling'\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/schema/Kubernetes_code \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"deployment with horizontal scaling\"\n",
    "query_embedding = late_chunking_helper.generate_query_embedding(query)\n",
    "rag_results = weaviate_helper.rag_query(collection_name=\"kubernetes_code\", query_embedding=query_embedding, limit=100)\n",
    "rag_results = [rag_result.properties for rag_result in rag_results]\n",
    "len(rag_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bbc2fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.embeddings.late_chunking:re_rank: 100 docs, top_k=None\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "INFO:src.embeddings.late_chunking:re_rank: returning 100 ranked docs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'content': '# This YAML configuration defines a Horizontal Pod Autoscaler (HPA) for a Kubernetes deployment, specifically targeting a ReplicaSet named \"frontend.\" The HPA automatically adjusts the number of pod replicas between 3 and 10, based on CPU utilization. It aims to maintain CPU usage at around 50%, scaling out or in to ensure the application remains responsive while efficiently using resources. This setup helps improve application scalability and reliability by dynamically managing pod counts according to workload demand.\\napiVersion: autoscaling/v1\\nkind: HorizontalPodAutoscaler\\nmetadata:\\n  name: frontend-scaler\\nspec:\\n  scaleTargetRef:\\n    kind: ReplicaSet\\n    name: frontend\\n  minReplicas: 3\\n  maxReplicas: 10\\n  targetCPUUtilizationPercentage: 50\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\hpa-rs.yaml',\n",
       "   'summary': 'This YAML configuration defines a Horizontal Pod Autoscaler (HPA) for a Kubernetes deployment, specifically targeting a ReplicaSet named \"frontend.\" The HPA automatically adjusts the number of pod replicas between 3 and 10, based on CPU utilization. It aims to maintain CPU usage at around 50%, scaling out or in to ensure the application remains responsive while efficiently using resources. This setup helps improve application scalability and reliability by dynamically managing pod counts according to workload demand.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.78125),\n",
       " ({'content': '# This configuration defines a Horizontal Pod Autoscaler (HPA) in Kubernetes, which automatically adjusts the number of pod replicas based on resource utilization. Specifically, it targets a Deployment named \"php-apache\" and maintains the replica count between 1 and 10. The autoscaler monitors CPU utilization and aims to keep the average CPU usage at around 50%, dynamically scaling the number of pods up or down to meet this target. This setup helps ensure efficient resource use and maintain application performance under variable load.\\napiVersion: autoscaling/v2\\nkind: HorizontalPodAutoscaler\\nmetadata:\\n  name: php-apache\\nspec:\\n  scaleTargetRef:\\n    apiVersion: apps/v1\\n    kind: Deployment\\n    name: php-apache\\n  minReplicas: 1\\n  maxReplicas: 10\\n  metrics:\\n  - type: Resource\\n    resource:\\n      name: cpu\\n      target:\\n        type: Utilization\\n        averageUtilization: 50\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\hpa\\\\php-apache.yaml',\n",
       "   'summary': 'This configuration defines a Horizontal Pod Autoscaler (HPA) in Kubernetes, which automatically adjusts the number of pod replicas based on resource utilization. Specifically, it targets a Deployment named \"php-apache\" and maintains the replica count between 1 and 10. The autoscaler monitors CPU utilization and aims to keep the average CPU usage at around 50%, dynamically scaling the number of pods up or down to meet this target. This setup helps ensure efficient resource use and maintain application performance under variable load.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.76953125),\n",
       " ({'content': '# This content is a Kubernetes Deployment manifest written in YAML, which defines how to deploy an application on a Kubernetes cluster. The deployment creates two replicas of an Nginx container, ensuring high availability. The deployment specifies that the containers should have resource limits, capping memory usage at 128Mi and CPU at 500m, to prevent resource exhaustion. The container will listen on port 80. Overall, this YAML configuration automates the deployment, scaling, and resource management of the Nginx application within a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx\\n        resources:\\n          limits:\\n            memory: \"128Mi\"\\n            cpu: \"500m\"\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Deployment manifest written in YAML, which defines how to deploy an application on a Kubernetes cluster. The deployment creates two replicas of an Nginx container, ensuring high availability. The deployment specifies that the containers should have resource limits, capping memory usage at 128Mi and CPU at 500m, to prevent resource exhaustion. The container will listen on port 80. Overall, this YAML configuration automates the deployment, scaling, and resource management of the Nginx application within a Kubernetes environment.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-with-request.yaml'},\n",
       "  0.76171875),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration expressed in YAML. It defines a deployment named \"snowflake\" with two replicas, ensuring that two pods running the specified container will be maintained at all times. The deployment uses a container image from the Kubernetes registry, specifically \"registry.k8s.io/serve_hostname,\" and sets the image pull policy to \"Always,\" ensuring the latest image version is retrieved during each deployment or pod update. This configuration provides a scalable and resilient setup for running containerized applications within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app: snowflake\\n  name: snowflake\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: snowflake\\n  template:\\n    metadata:\\n      labels:\\n        app: snowflake\\n    spec:\\n      containers:\\n      - image: registry.k8s.io/serve_hostname\\n        imagePullPolicy: Always\\n        name: snowflake\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration expressed in YAML. It defines a deployment named \"snowflake\" with two replicas, ensuring that two pods running the specified container will be maintained at all times. The deployment uses a container image from the Kubernetes registry, specifically \"registry.k8s.io/serve_hostname,\" and sets the image pull policy to \"Always,\" ensuring the latest image version is retrieved during each deployment or pod update. This configuration provides a scalable and resilient setup for running containerized applications within a Kubernetes cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\snowflake-deployment.yaml'},\n",
       "  0.76171875),\n",
       " ({'content': '# This content provides a Kubernetes configuration deploying a PHP-Apache application along with its associated service. The first part defines a Deployment that manages pods running the containerized PHP-Apache server, specifying resource limits and requests for CPU to ensure proper resource allocation. The Deployment uses a specific container image and exposes port 80. The second part creates a Service that exposes this deployment on port 80, providing a stable endpoint for accessing the application. Overall, it automates the deployment and exposure of a containerized web application in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: php-apache\\nspec:\\n  selector:\\n    matchLabels:\\n      run: php-apache\\n  template:\\n    metadata:\\n      labels:\\n        run: php-apache\\n    spec:\\n      containers:\\n      - name: php-apache\\n        image: registry.k8s.io/hpa-example\\n        ports:\\n        - containerPort: 80\\n        resources:\\n          limits:\\n            cpu: 500m\\n          requests:\\n            cpu: 200m\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: php-apache\\n  labels:\\n    run: php-apache\\nspec:\\n  ports:\\n  - port: 80\\n  selector:\\n    run: php-apache\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content provides a Kubernetes configuration deploying a PHP-Apache application along with its associated service. The first part defines a Deployment that manages pods running the containerized PHP-Apache server, specifying resource limits and requests for CPU to ensure proper resource allocation. The Deployment uses a specific container image and exposes port 80. The second part creates a Service that exposes this deployment on port 80, providing a stable endpoint for accessing the application. Overall, it automates the deployment and exposure of a containerized web application in a Kubernetes cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\php-apache.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.7578125),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages four replicas of an Nginx web server container, instead of the previously configured two. The deployment specifies the use of the \"nginx:1.16.1\" Docker image and maps port 80 of the container to the network, ensuring multiple instances of Nginx are running for load balancing or high availability. The configuration utilizes labels for identification and selector matching to manage the pods efficiently.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 4 # Update the replicas from 2 to 4\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages four replicas of an Nginx web server container, instead of the previously configured two. The deployment specifies the use of the \"nginx:1.16.1\" Docker image and maps port 80 of the container to the network, ensuring multiple instances of Nginx are running for load balancing or high availability. The configuration utilizes labels for identification and selector matching to manage the pods efficiently.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-scale.yaml'},\n",
       "  0.75390625),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"capacity-reservation\" with a single replica of a pod. The deployment uses labels and annotations for identification and descriptive purposes. The pod template specifies a container named \"pause\" that runs the \"registry.k8s.io/pause:3.6\" image, which is commonly used as a placeholder or to reserve capacity in Kubernetes. Resource requests and limits are set to ensure the container requests 50 millicores of CPU and 512 MiB of memory, with limits also at 512 MiB. The deployment incorporates affinity rules, specifically a pod anti-affinity configuration, which aims to distribute the overhead Pods across different nodes by avoiding placing similar pods on the same hostname, thus enhancing fault tolerance and resource distribution. This configuration facilitates capacity reservation and optimal placement strategies within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: capacity-reservation\\n  # You should decide what namespace to deploy this into\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: capacity-placeholder\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: capacity-placeholder\\n      annotations:\\n        kubernetes.io/description: \"Capacity reservation\"\\n    spec:\\n      priorityClassName: placeholder\\n      affinity: # Try to place these overhead Pods on different nodes\\n                # if possible\\n        podAntiAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                app: placeholder\\n            topologyKey: \"kubernetes.io/hostname\"\\n      containers:\\n      - name: pause\\n        image: registry.k8s.io/pause:3.6\\n        resources:\\n          requests:\\n            cpu: \"50m\"\\n            memory: \"512Mi\"\\n          limits:\\n            memory: \"512Mi\"\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"capacity-reservation\" with a single replica of a pod. The deployment uses labels and annotations for identification and descriptive purposes. The pod template specifies a container named \"pause\" that runs the \"registry.k8s.io/pause:3.6\" image, which is commonly used as a placeholder or to reserve capacity in Kubernetes. Resource requests and limits are set to ensure the container requests 50 millicores of CPU and 512 MiB of memory, with limits also at 512 MiB. The deployment incorporates affinity rules, specifically a pod anti-affinity configuration, which aims to distribute the overhead Pods across different nodes by avoiding placing similar pods on the same hostname, thus enhancing fault tolerance and resource distribution. This configuration facilitates capacity reservation and optimal placement strategies within a Kubernetes cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-capacity-reservation.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.75390625),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages two replicas (pods) running an Nginx server. The deployment uses labels to manage the pods and specifies the container image \"nginx:1.14.2\", exposing port 80 for web traffic. This configuration automates the deployment, scaling, and management of multiple Nginx instances within a Kubernetes cluster, ensuring high availability and load balancing.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 2 # tells deployment to run 2 pods matching the template\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages two replicas (pods) running an Nginx server. The deployment uses labels to manage the pods and specifies the container image \"nginx:1.14.2\", exposing port 80 for web traffic. This configuration automates the deployment, scaling, and management of multiple Nginx instances within a Kubernetes cluster, ensuring high availability and load balancing.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment.yaml'},\n",
       "  0.75390625),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration in YAML format, which defines how to deploy an application in a Kubernetes cluster. It specifies a Deployment resource with the API version `apps/v1` and the kind `Deployment`. The deployment is labeled for identification, and it is named `hello-world`. The configuration requests five replicas, meaning it will run five instances of the containerized application to ensure high availability and load balancing. The selector matches the label `app.kubernetes.io/name: load-balancer-example` to identify the pods managed by this deployment. The pod template within the deployment specifies metadata labels and describes the containers to run, which in this case includes a single container running the image `gcr.io/google-samples/hello-app:2.0`. The container exposes port 8080, making the application accessible when combined with a service or load balancer. Overall, this deployment configuration automates the process of deploying multiple instances of a sample hello-world application in a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app.kubernetes.io/name: load-balancer-example\\n  name: hello-world\\nspec:\\n  replicas: 5\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: load-balancer-example\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: load-balancer-example\\n    spec:\\n      containers:\\n      - image: gcr.io/google-samples/hello-app:2.0\\n        name: hello-world\\n        ports:\\n        - containerPort: 8080\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration in YAML format, which defines how to deploy an application in a Kubernetes cluster. It specifies a Deployment resource with the API version `apps/v1` and the kind `Deployment`. The deployment is labeled for identification, and it is named `hello-world`. The configuration requests five replicas, meaning it will run five instances of the containerized application to ensure high availability and load balancing. The selector matches the label `app.kubernetes.io/name: load-balancer-example` to identify the pods managed by this deployment. The pod template within the deployment specifies metadata labels and describes the containers to run, which in this case includes a single container running the image `gcr.io/google-samples/hello-app:2.0`. The container exposes port 8080, making the application accessible when combined with a service or load balancer. Overall, this deployment configuration automates the process of deploying multiple instances of a sample hello-world application in a Kubernetes environment.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\load-balancer-example.yaml'},\n",
       "  0.75390625),\n",
       " ({'content': '# This Kubernetes deployment configuration defines a deployment named \"nginx-deployment\" that manages three replicas of an Nginx container. It specifies the use of the Nginx version 1.14.2 image and exposes port 80 on each container. The deployment uses label selectors to identify the pods it manages and ensures high availability by maintaining three identical pods running the Nginx server. This configuration is used for deploying scalable and consistent web server instances in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This Kubernetes deployment configuration defines a deployment named \"nginx-deployment\" that manages three replicas of an Nginx container. It specifies the use of the Nginx version 1.14.2 image and exposes port 80 on each container. The deployment uses label selectors to identify the pods it manages and ensures high availability by maintaining three identical pods running the Nginx server. This configuration is used for deploying scalable and consistent web server instances in a Kubernetes cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\nginx-deployment.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.75390625),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment object named \"backend\" that manages three replicas of a containerized application. The deployment uses a label selector to identify its pods, which are labeled with \"app: hello\", \"tier: backend\", and \"track: stable\". The pod template specifies the container details, including the container name \"hello\" and the image sourced from Google Container Registry (\"gcr.io/google-samples/hello-go-gke:1.0\"). The container exposes port 80 for HTTP traffic. Overall, this configuration automates the deployment and scaling of a simple backend service in a Kubernetes cluster.\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: backend\\nspec:\\n  selector:\\n    matchLabels:\\n      app: hello\\n      tier: backend\\n      track: stable\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: hello\\n        tier: backend\\n        track: stable\\n    spec:\\n      containers:\\n        - name: hello\\n          image: \"gcr.io/google-samples/hello-go-gke:1.0\"\\n          ports:\\n            - name: http\\n              containerPort: 80\\n...',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\backend-deployment.yaml',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a Deployment object named \"backend\" that manages three replicas of a containerized application. The deployment uses a label selector to identify its pods, which are labeled with \"app: hello\", \"tier: backend\", and \"track: stable\". The pod template specifies the container details, including the container name \"hello\" and the image sourced from Google Container Registry (\"gcr.io/google-samples/hello-go-gke:1.0\"). The container exposes port 80 for HTTP traffic. Overall, this configuration automates the deployment and scaling of a simple backend service in a Kubernetes cluster.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.75390625),\n",
       " ({'content': '# This content describes a Kubernetes deployment configuration written in YAML. It specifies a deployment named \"hello-world\" that manages two replicas of a containerized application. The deployment uses the label \"run: load-balancer-example\" for selecting pods and defines a pod template with the same label. The pod runs a container named \"hello-world\" based on the image \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\" and exposes port 8080 via TCP. This setup ensures high availability by running multiple instances of the application, which a load balancer can distribute traffic to.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: hello-world\\nspec:\\n  selector:\\n    matchLabels:\\n      run: load-balancer-example\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        run: load-balancer-example\\n    spec:\\n      containers:\\n        - name: hello-world\\n          image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\\n          ports:\\n            - containerPort: 8080\\n              protocol: TCP\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content describes a Kubernetes deployment configuration written in YAML. It specifies a deployment named \"hello-world\" that manages two replicas of a containerized application. The deployment uses the label \"run: load-balancer-example\" for selecting pods and defines a pod template with the same label. The pod runs a container named \"hello-world\" based on the image \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\" and exposes port 8080 via TCP. This setup ensures high availability by running multiple instances of the application, which a load balancer can distribute traffic to.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\hello-application.yaml'},\n",
       "  0.75),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages the deployment of an Nginx container. The deployment specifies a selector to match the label \"app: nginx\" and uses a pod template with the same label to ensure proper identification and management of the pods. The container runs the \"nginx:1.14.2\" image, which is an Nginx web server version. Overall, this configuration automates the process of deploying and managing Nginx instances within a Kubernetes cluster, facilitating scalability and maintainability of the web server deployment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment-no-replicas.yaml',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"nginx-deployment\" that manages the deployment of an Nginx container. The deployment specifies a selector to match the label \"app: nginx\" and uses a pod template with the same label to ensure proper identification and management of the pods. The container runs the \"nginx:1.14.2\" image, which is an Nginx web server version. Overall, this configuration automates the process of deploying and managing Nginx instances within a Kubernetes cluster, facilitating scalability and maintainability of the web server deployment.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.75),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"pod-quota-demo,\" which manages three replicas of a pod running the Nginx container image. The deployment specifies a label \"purpose: quota-demo\" to identify its pods, enabling resource management and scheduling. This configuration is used to deploy a scalable, load-balanced group of Nginx pods for testing or demonstration purposes in a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: pod-quota-demo\\nspec:\\n  selector:\\n    matchLabels:\\n      purpose: quota-demo\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        purpose: quota-demo\\n    spec:\\n      containers:\\n      - name: pod-quota-demo\\n        image: nginx\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment resource named \"pod-quota-demo,\" which manages three replicas of a pod running the Nginx container image. The deployment specifies a label \"purpose: quota-demo\" to identify its pods, enabling resource management and scheduling. This configuration is used to deploy a scalable, load-balanced group of Nginx pods for testing or demonstration purposes in a Kubernetes environment.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\quota-pod-deployment.yaml'},\n",
       "  0.75),\n",
       " ({'content': '# This Kubernetes manifest defines a Pod named `constraints-cpu-demo-2` with a single container running the `nginx` image. The key focus is on resource management, specifically CPU constraints. The container has a CPU request of 500 millicores (0.5 CPU) and a CPU limit of 1.5 cores, indicating it requests half a CPU for initial scheduling and can use up to one and a half cores when needed. This setup ensures that the container has a guaranteed minimum CPU and a maximum utilization limit, helping manage resource allocation effectively in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo-2\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"1.5\"\\n      requests:\\n        cpu: \"500m\"\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This Kubernetes manifest defines a Pod named `constraints-cpu-demo-2` with a single container running the `nginx` image. The key focus is on resource management, specifically CPU constraints. The container has a CPU request of 500 millicores (0.5 CPU) and a CPU limit of 1.5 cores, indicating it requests half a CPU for initial scheduling and can use up to one and a half cores when needed. This setup ensures that the container has a guaranteed minimum CPU and a maximum utilization limit, helping manage resource allocation effectively in the cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-2.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.74609375),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"my-nginx\" that manages two replicas of an Nginx web server container. The configuration specifies that each container runs the Nginx image and exposes port 80. The deployment ensures that the specified number of pods (containers) are running and automatically handles scaling, updates, and self-healing of the application. This setup is commonly used to deploy and manage stateless web servers in a Kubernetes cluster efficiently.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      run: my-nginx\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        run: my-nginx\\n    spec:\\n      containers:\\n      - name: my-nginx\\n        image: nginx\\n        ports:\\n        - containerPort: 80\\n\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\run-my-nginx.yaml',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"my-nginx\" that manages two replicas of an Nginx web server container. The configuration specifies that each container runs the Nginx image and exposes port 80. The deployment ensures that the specified number of pods (containers) are running and automatically handles scaling, updates, and self-healing of the application. This setup is commonly used to deploy and manage stateless web servers in a Kubernetes cluster efficiently.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.74609375),\n",
       " ({'content': '# The provided content defines a Kubernetes deployment and service for running an NGINX web server. The service configuration creates a LoadBalancer type service named \"my-nginx-svc\" that exposes port 80 and selects pods with the label \"app: nginx,\" allowing external access to the application. The deployment configuration named \"my-nginx\" specifies three replicas of the NGINX container running the version 1.14.2 image, each listening on port 80, and ensures that pods are created with the label \"app: nginx\" to be matched by the service. This setup enables load-balanced, scalable deployment of the NGINX web server in a Kubernetes cluster.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-nginx-svc\\n  labels:\\n    app: nginx\\nspec:\\n  type: LoadBalancer\\n  ports:\\n  - port: 80\\n  selector:\\n    app: nginx\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content defines a Kubernetes deployment and service for running an NGINX web server. The service configuration creates a LoadBalancer type service named \"my-nginx-svc\" that exposes port 80 and selects pods with the label \"app: nginx,\" allowing external access to the application. The deployment configuration named \"my-nginx\" specifies three replicas of the NGINX container running the version 1.14.2 image, each listening on port 80, and ensures that pods are created with the label \"app: nginx\" to be matched by the service. This setup enables load-balanced, scalable deployment of the NGINX web server in a Kubernetes cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx-app.yaml'},\n",
       "  0.74609375),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages two replicas of an Nginx web server container. The deployment uses the selector label \"app: nginx\" to identify the Pods it manages. The Pod template specifies that each container runs the Nginx image version 1.16.1 (updating from a previous version 1.14.2), and exposes port 80 for web traffic. Overall, this configuration automates the deployment, scaling, and management of the Nginx web servers in a Kubernetes cluster, ensuring high availability and easy updates.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1 # Update the version of nginx from 1.14.2 to 1.16.1\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-update.yaml',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"nginx-deployment\" that manages two replicas of an Nginx web server container. The deployment uses the selector label \"app: nginx\" to identify the Pods it manages. The Pod template specifies that each container runs the Nginx image version 1.16.1 (updating from a previous version 1.14.2), and exposes port 80 for web traffic. Overall, this configuration automates the deployment, scaling, and management of the Nginx web servers in a Kubernetes cluster, ensuring high availability and easy updates.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.74609375),\n",
       " ({'content': '# The provided content is a Kubernetes Deployment manifest written in YAML, which automates the deployment and management of containerized applications. It specifies a deployment named \"nginx-deployment\" that creates and manages pods running the Nginx web server. The deployment uses the label \"app: nginx\" to identify the pods it manages, and it ensures that each pod runs the specified container with the image \"nginx:1.14.2\". The container is configured to listen on port 80, which is typical for web servers. Additionally, the deployment includes the \"minReadySeconds\" parameter set to 5 seconds, which requires that pods be ready for at least this duration before they are considered available for serving traffic. This YAML template provides a concise, declarative way to deploy and manage a scalable web server environment in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  minReadySeconds: 5\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes Deployment manifest written in YAML, which automates the deployment and management of containerized applications. It specifies a deployment named \"nginx-deployment\" that creates and manages pods running the Nginx web server. The deployment uses the label \"app: nginx\" to identify the pods it manages, and it ensures that each pod runs the specified container with the image \"nginx:1.14.2\". The container is configured to listen on port 80, which is typical for web servers. Additionally, the deployment includes the \"minReadySeconds\" parameter set to 5 seconds, which requires that pods be ready for at least this duration before they are considered available for serving traffic. This YAML template provides a concise, declarative way to deploy and manage a scalable web server environment in Kubernetes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\simple_deployment.yaml'},\n",
       "  0.74609375),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"retainkeys-demo\" that manages an nginx application. The deployment uses a rolling update strategy with a maximum surge of 30%, allowing new pods to be created ahead of old ones being terminated, thereby minimizing downtime. The template specifies a container named \"retainkeys-demo-ctr\" running the nginx image. Overall, this configuration automates the deployment and upgrade process of an nginx-based application using Kubernetes, ensuring smooth updates with minimal service disruption.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: retainkeys-demo\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 30%\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: retainkeys-demo-ctr\\n        image: nginx\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a deployment named \"retainkeys-demo\" that manages an nginx application. The deployment uses a rolling update strategy with a maximum surge of 30%, allowing new pods to be created ahead of old ones being terminated, thereby minimizing downtime. The template specifies a container named \"retainkeys-demo-ctr\" running the nginx image. Overall, this configuration automates the deployment and upgrade process of an nginx-based application using Kubernetes, ensuring smooth updates with minimal service disruption.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-retainkeys.yaml'},\n",
       "  0.7421875),\n",
       " ({'content': '# This content is a Kubernetes Deployment configuration written in YAML. It defines a deployment named \"iis\" that manages three replicas of a container running the Microsoft IIS web server. The deployment uses a label selector to identify the pods it manages and specifies container details including the image (\"microsoft/iis\") and a container port (80). Additionally, it includes an annotation indicating the use of Hyper-V isolation type for Windows containers, ensuring better isolation and compatibility on Windows nodes. This configuration automates the deployment and scaling of IIS web server instances within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iis\\nspec:\\n  selector:\\n    matchLabels:\\n      app: iis\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: iis\\n      annotations:\\n        experimental.windows.kubernetes.io/isolation-type: hyperv\\n    spec:\\n      containers:\\n      - name: iis\\n        image: microsoft/iis\\n        ports:\\n        - containerPort: 80\\n\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Deployment configuration written in YAML. It defines a deployment named \"iis\" that manages three replicas of a container running the Microsoft IIS web server. The deployment uses a label selector to identify the pods it manages and specifies container details including the image (\"microsoft/iis\") and a container port (80). Additionally, it includes an annotation indicating the use of Hyper-V isolation type for Windows containers, ensuring better isolation and compatibility on Windows nodes. This configuration automates the deployment and scaling of IIS web server instances within a Kubernetes cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-hyperv.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.7421875),\n",
       " ({'content': '# This Kubernetes configuration defines a Pod named \"extended-resource-demo\" with a single container running the nginx image. The key aspect of this setup is the specification of extended resources through the \"resources\" field. It requests and limits the custom resource \"example.com/dongle\" to a quantity of 3 units. This demonstrates how to allocate extended or custom resources in a container, which can be used to manage hardware or software components beyond standard CPU and memory, such as specialized devices or custom features. The configuration ensures that the container both requests and is limited to the specified amount of this extended resource.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: extended-resource-demo\\nspec:\\n  containers:\\n  - name: extended-resource-demo-ctr\\n    image: nginx\\n    resources:\\n      requests:\\n        example.com/dongle: 3\\n      limits:\\n        example.com/dongle: 3\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod.yaml',\n",
       "   'summary': 'This Kubernetes configuration defines a Pod named \"extended-resource-demo\" with a single container running the nginx image. The key aspect of this setup is the specification of extended resources through the \"resources\" field. It requests and limits the custom resource \"example.com/dongle\" to a quantity of 3 units. This demonstrates how to allocate extended or custom resources in a container, which can be used to manage hardware or software components beyond standard CPU and memory, such as specialized devices or custom features. The configuration ensures that the container both requests and is limited to the specified amount of this extended resource.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.7421875),\n",
       " ({'content': '# The provided content consists of two Kubernetes Pod configurations written in YAML. Each Pod is labeled as \"frontend\" and contains a single container running the \"hello-app\" image from Google\\'s container registry. The first Pod, named \"pod1,\" uses version 2.0 of the image, while the second Pod, \"pod2,\" runs version 1.0. \\n\\nThese configurations demonstrate how to define simple Pods with specific images, illustrating how different versions of an application can be deployed side by side within a Kubernetes cluster. The YAML files specify the API version, resource kind, metadata, labels for organization and selection, and container specifications including names and container images.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod1\\n  labels:\\n    tier: frontend\\nspec:\\n  containers:\\n  - name: hello1\\n    image: gcr.io/google-samples/hello-app:2.0\\n\\n---\\n\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod2\\n  labels:\\n    tier: frontend\\nspec:\\n  containers:\\n  - name: hello2\\n    image: gcr.io/google-samples/hello-app:1.0\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content consists of two Kubernetes Pod configurations written in YAML. Each Pod is labeled as \"frontend\" and contains a single container running the \"hello-app\" image from Google\\'s container registry. The first Pod, named \"pod1,\" uses version 2.0 of the image, while the second Pod, \"pod2,\" runs version 1.0. \\n\\nThese configurations demonstrate how to define simple Pods with specific images, illustrating how different versions of an application can be deployed side by side within a Kubernetes cluster. The YAML files specify the API version, resource kind, metadata, labels for organization and selection, and container specifications including names and container images.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-rs.yaml'},\n",
       "  0.7421875),\n",
       " ({'content': '# This content provides a Kubernetes Deployment configuration for a frontend application, specifically a guestbook app. It defines a deployment named \"frontend\" that manages three replicas of a containerized PHP-Redis application. The deployment uses labels to identify its components and specifies a pod template that includes a container running a specific image hosted in Google\\'s container registry. The container environment variable `GET_HOSTS_FROM` is set to \"dns,\" indicating that hostname resolution is handled via DNS. Resource requests allocate 100 millicpus and 100Mi of memory per container to ensure resource management. The container exposes port 80 for HTTP traffic. This configuration enables scalable, manageable deployment of a web frontend in a Kubernetes cluster.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n        app: guestbook\\n        tier: frontend\\n  template:\\n    metadata:\\n      labels:\\n        app: guestbook\\n        tier: frontend\\n    spec:\\n      containers:\\n      - name: php-redis\\n        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\\n        env:\\n        - name: GET_HOSTS_FROM\\n          value: \"dns\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content provides a Kubernetes Deployment configuration for a frontend application, specifically a guestbook app. It defines a deployment named \"frontend\" that manages three replicas of a containerized PHP-Redis application. The deployment uses labels to identify its components and specifies a pod template that includes a container running a specific image hosted in Google\\'s container registry. The container environment variable `GET_HOSTS_FROM` is set to \"dns,\" indicating that hostname resolution is handled via DNS. Resource requests allocate 100 millicpus and 100Mi of memory per container to ensure resource management. The container exposes port 80 for HTTP traffic. This configuration enables scalable, manageable deployment of a web frontend in a Kubernetes cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-deployment.yaml'},\n",
       "  0.7421875),\n",
       " ({'content': '# This Kubernetes manifest defines a Deployment resource named \"patch-demo\" with two replicas of an Nginx container. The deployment specifies a label selector to manage pods with the label \"app: nginx,\" and the pod template also includes this label. The container uses the official Nginx image and is named \"patch-demo-ctr.\" Additionally, the pod incorporates tolerations that allow it to be scheduled on nodes with a \"dedicated\" key set to \"test-team\" and effect \"NoSchedule,\" meaning the pod can be scheduled on nodes with matching taints. Overall, this configuration ensures that two Nginx pods are running and can be deployed on tainted nodes with matching tolerations.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: patch-demo\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: patch-demo-ctr\\n        image: nginx\\n      tolerations:\\n      - effect: NoSchedule\\n        key: dedicated\\n        value: test-team\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-patch.yaml',\n",
       "   'summary': 'This Kubernetes manifest defines a Deployment resource named \"patch-demo\" with two replicas of an Nginx container. The deployment specifies a label selector to manage pods with the label \"app: nginx,\" and the pod template also includes this label. The container uses the official Nginx image and is named \"patch-demo-ctr.\" Additionally, the pod incorporates tolerations that allow it to be scheduled on nodes with a \"dedicated\" key set to \"test-team\" and effect \"NoSchedule,\" meaning the pod can be scheduled on nodes with matching taints. Overall, this configuration ensures that two Nginx pods are running and can be deployed on tainted nodes with matching tolerations.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.7421875),\n",
       " ({'content': '# This Kubernetes Pod configuration defines a pod named \"mypod\" with a label \"foo: bar\". It specifies a topology spread constraint to ensure the pod is evenly distributed across different zones based on the \"zone\" topology key, with a maximum skew of 1. If the spread cannot be satisfied, scheduling is prevented (\"DoNotSchedule\"). The pod contains a single container named \"pause\" that uses the official \"pause\" image from the Kubernetes registry, which is typically used as a placeholder or for network topology purposes within a cluster. Overall, this configuration emphasizes controlled pod placement across zones to maintain high availability or fault tolerance.\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: mypod\\n  labels:\\n    foo: bar\\nspec:\\n  topologySpreadConstraints:\\n  - maxSkew: 1\\n    topologyKey: zone\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.1',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This Kubernetes Pod configuration defines a pod named \"mypod\" with a label \"foo: bar\". It specifies a topology spread constraint to ensure the pod is evenly distributed across different zones based on the \"zone\" topology key, with a maximum skew of 1. If the spread cannot be satisfied, scheduling is prevented (\"DoNotSchedule\"). The pod contains a single container named \"pause\" that uses the official \"pause\" image from the Kubernetes registry, which is typically used as a placeholder or for network topology purposes within a cluster. Overall, this configuration emphasizes controlled pod placement across zones to maintain high availability or fault tolerance.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.7421875),\n",
       " ({'content': \"# This content provides a Kubernetes manifest defining resources for deploying an Event Exporter application. It includes a ServiceAccount, a ClusterRoleBinding, and a Deployment. The ServiceAccount creates a dedicated identity for the application to run securely in the cluster. The ClusterRoleBinding grants this ServiceAccount read-only access to cluster resources by binding it to the 'view' ClusterRole. The Deployment specifies a single replica pod running the Event Exporter container, configured with the specified image version, and associates it with the created ServiceAccount to ensure appropriate permissions during operation. The container executes the '/event-exporter' command, and the deployment includes a termination grace period of 30 seconds for smooth shutdowns. Overall, these resources set up a secure and manageable deployment of the Event Exporter in a Kubernetes environment.\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: event-exporter-sa\\n  namespace: default\\n  labels:\\n    app: event-exporter\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: event-exporter-rb\\n  labels:\\n    app: event-exporter\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: view\\nsubjects:\\n- kind: ServiceAccount\\n  name: event-exporter-sa\\n  namespace: default\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: event-exporter-v0.2.3\\n  namespace: default\\n  labels:\\n    app: event-exporter\\nspec:\\n  selector:\\n    matchLabels:\\n      app: event-exporter\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: event-exporter\\n    spec:\\n      serviceAccountName: event-exporter-sa\\n      containers:\\n      - name: event-exporter\\n        image: registry.k8s.io/event-exporter:v0.2.3\\n        command:\\n        - '/event-exporter'\\n      terminationGracePeriodSeconds: 30\\n\",\n",
       "   'chunk': '1/1',\n",
       "   'summary': \"This content provides a Kubernetes manifest defining resources for deploying an Event Exporter application. It includes a ServiceAccount, a ClusterRoleBinding, and a Deployment. The ServiceAccount creates a dedicated identity for the application to run securely in the cluster. The ClusterRoleBinding grants this ServiceAccount read-only access to cluster resources by binding it to the 'view' ClusterRole. The Deployment specifies a single replica pod running the Event Exporter container, configured with the specified image version, and associates it with the created ServiceAccount to ensure appropriate permissions during operation. The container executes the '/event-exporter' command, and the deployment includes a termination grace period of 30 seconds for smooth shutdowns. Overall, these resources set up a secure and manageable deployment of the Event Exporter in a Kubernetes environment.\",\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\debug\\\\event-exporter.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.73828125),\n",
       " ({'content': '# This YAML file describes a Kubernetes deployment configuration for running an Nginx web server. It specifies that three replicas of the Nginx container should be created to ensure high availability and load balancing. The deployment uses the `nginx:1.14.2` Docker image, and the labels help Kubernetes identify and manage the pods associated with this deployment. This setup ensures consistent, scalable deployment of the Nginx server within a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  labels:\\n    app: nginx\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML file describes a Kubernetes deployment configuration for running an Nginx web server. It specifies that three replicas of the Nginx container should be created to ensure high availability and load balancing. The deployment uses the `nginx:1.14.2` Docker image, and the labels help Kubernetes identify and manage the pods associated with this deployment. This setup ensures consistent, scalable deployment of the Nginx server within a Kubernetes cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\ssa\\\\nginx-deployment.yaml'},\n",
       "  0.73828125),\n",
       " ({'content': '# This code is a Kubernetes ReplicationController configuration, which manages the deployment and scaling of a set number of identical pods. It specifies that 5 replicas of an nginx container should run simultaneously. Each pod will run the nginx:1.14.2 image and expose port 80. The ReplicationController ensures that the specified number of nginx pods are maintained, automatically creating or deleting pods to match the desired replica count, providing high availability and load balancing for the application.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:\\n  name: my-nginx\\nspec:\\n  replicas: 5\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This code is a Kubernetes ReplicationController configuration, which manages the deployment and scaling of a set number of identical pods. It specifies that 5 replicas of an nginx container should run simultaneously. Each pod will run the nginx:1.14.2 image and expose port 80. The ReplicationController ensures that the specified number of nginx pods are maintained, automatically creating or deleting pods to match the desired replica count, providing high availability and load balancing for the application.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.14.2.yaml'},\n",
       "  0.73828125),\n",
       " ({'content': '# This content is a Kubernetes Pod configuration that combines scheduling constraints and affinity rules to control pod placement. It specifies a pod named \"mypod\" with a label \"foo: bar\" and one container running a pause image, typically used for testing or as a placeholder.\\n\\nThe configuration includes **topology spread constraints**, which aim to evenly distribute pods across different zones to prevent clustering. Specifically, it sets a maximum skew of 1, meaning there should be at most one more pod in one zone compared to another, and applies this rule only to zones matching the label \"foo: bar.\" The **whenUnsatisfiable** setting \"DoNotSchedule\" prevents scheduling if the distribution cannot be achieved.\\n\\nAdditionally, **node affinity** ensures the pod is scheduled on nodes outside \"zoneC\" using a requiredDuringScheduling rule with a matchExpressions condition. This enforces placement on preferred zones, avoiding certain zones based on the node labels.\\n\\nOverall, this configuration demonstrates advanced scheduling policies to control pod distribution across zones and nodes, ensuring high availability and fault tolerance.\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: mypod\\n  labels:\\n    foo: bar\\nspec:\\n  topologySpreadConstraints:\\n  - maxSkew: 1\\n    topologyKey: zone\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:\\n        - matchExpressions:\\n          - key: zone\\n            operator: NotIn\\n            values:\\n            - zoneC\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.1',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Pod configuration that combines scheduling constraints and affinity rules to control pod placement. It specifies a pod named \"mypod\" with a label \"foo: bar\" and one container running a pause image, typically used for testing or as a placeholder.\\n\\nThe configuration includes **topology spread constraints**, which aim to evenly distribute pods across different zones to prevent clustering. Specifically, it sets a maximum skew of 1, meaning there should be at most one more pod in one zone compared to another, and applies this rule only to zones matching the label \"foo: bar.\" The **whenUnsatisfiable** setting \"DoNotSchedule\" prevents scheduling if the distribution cannot be achieved.\\n\\nAdditionally, **node affinity** ensures the pod is scheduled on nodes outside \"zoneC\" using a requiredDuringScheduling rule with a matchExpressions condition. This enforces placement on preferred zones, avoiding certain zones based on the node labels.\\n\\nOverall, this configuration demonstrates advanced scheduling policies to control pod distribution across zones and nodes, ensuring high availability and fault tolerance.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\one-constraint-with-nodeaffinity.yaml'},\n",
       "  0.73828125),\n",
       " ({'content': '# The provided content describes a Kubernetes deployment setup for a WordPress application, including a Service, PersistentVolumeClaim, and Deployment configuration. The Service exposes the WordPress frontend on port 80 and uses a LoadBalancer to facilitate external access. The PersistentVolumeClaim requests 20 GiB of persistent storage to retain the website data. The Deployment manages the WordPress pod, which runs the official WordPress Docker image with Apache, and sets environment variables to connect to a MySQL database securely by referencing a secret. It also mounts the persistent storage volume to ensure data persistence across pod restarts. This configuration together deploys a scalable, persistent WordPress environment in Kubernetes, with external accessibility and database connectivity managed through environment variables and secrets.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: wordpress\\n  labels:\\n    app: wordpress\\nspec:\\n  ports:\\n    - port: 80\\n  selector:\\n    app: wordpress\\n    tier: frontend\\n  type: LoadBalancer\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: wp-pv-claim\\n  labels:\\n    app: wordpress\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 20Gi\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: wordpress\\n  labels:\\n    app: wordpress\\nspec:\\n  selector:\\n    matchLabels:\\n      app: wordpress\\n      tier: frontend\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      labels:\\n        app: wordpress\\n        tier: frontend\\n    spec:\\n      containers:\\n      - image: wordpress:6.2.1-apache\\n        name: wordpress\\n        env:\\n        - name: WORDPRESS_DB_HOST\\n          value: wordpress-mysql\\n        - name: WORDPRESS_DB_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: mysql-pass\\n              key: password\\n        - name: WORDPRESS_DB_USER\\n          value: wordpress\\n        ports:\\n        - containerPort: 80\\n          name: wordpress\\n        volumeMounts:\\n        - name: wordpress-persistent-storage\\n          mountPath: /var/www/html\\n      volumes:\\n      - name: wordpress-persistent-storage\\n        persistentVolumeClaim:\\n          claimName: wp-pv-claim\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'The provided content describes a Kubernetes deployment setup for a WordPress application, including a Service, PersistentVolumeClaim, and Deployment configuration. The Service exposes the WordPress frontend on port 80 and uses a LoadBalancer to facilitate external access. The PersistentVolumeClaim requests 20 GiB of persistent storage to retain the website data. The Deployment manages the WordPress pod, which runs the official WordPress Docker image with Apache, and sets environment variables to connect to a MySQL database securely by referencing a secret. It also mounts the persistent storage volume to ensure data persistence across pod restarts. This configuration together deploys a scalable, persistent WordPress environment in Kubernetes, with external accessibility and database connectivity managed through environment variables and secrets.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\wordpress-deployment.yaml'},\n",
       "  0.73828125),\n",
       " ({'content': '# This Kubernetes YAML configuration defines a Pod named `constraints-cpu-demo-3` with a single container running the `nginx` image. The container has specified resource constraints, including a CPU request of 100 millicores (0.1 CPU) and a limit of 800 millicores (0.8 CPU). This setup enforces CPU resource allocation, ensuring the container has guaranteed CPU availability while preventing it from exceeding the specified limit, which helps manage resource utilization and maintain cluster stability.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo-3\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-3-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"800m\"\\n      requests:\\n        cpu: \"100m\"\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-3.yaml',\n",
       "   'summary': 'This Kubernetes YAML configuration defines a Pod named `constraints-cpu-demo-3` with a single container running the `nginx` image. The container has specified resource constraints, including a CPU request of 100 millicores (0.1 CPU) and a limit of 800 millicores (0.8 CPU). This setup enforces CPU resource allocation, ensuring the container has guaranteed CPU availability while preventing it from exceeding the specified limit, which helps manage resource utilization and maintain cluster stability.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.73828125),\n",
       " ({'content': '# This content is a Kubernetes YAML configuration that defines a Service resource named \"frontend.\" The Service is configured to select Pods with specific labels (\"app: hello\" and \"tier: frontend\") and exposes them through port 80 using TCP protocol. Its type is set to \"LoadBalancer,\" meaning it will provision an external load balancer to distribute incoming traffic to the selected Pods, making the application accessible from outside the cluster. This setup is typically used to expose frontend applications to users in a scalable and reliable manner.\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: frontend\\nspec:\\n  selector:\\n    app: hello\\n    tier: frontend\\n  ports:\\n  - protocol: \"TCP\"\\n    port: 80\\n    targetPort: 80\\n  type: LoadBalancer\\n...',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes YAML configuration that defines a Service resource named \"frontend.\" The Service is configured to select Pods with specific labels (\"app: hello\" and \"tier: frontend\") and exposes them through port 80 using TCP protocol. Its type is set to \"LoadBalancer,\" meaning it will provision an external load balancer to distribute incoming traffic to the selected Pods, making the application accessible from outside the cluster. This setup is typically used to expose frontend applications to users in a scalable and reliable manner.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-service.yaml'},\n",
       "  0.73828125),\n",
       " ({'content': '# This content defines a Kubernetes namespace named \"development.\" It specifies the API version as v1 and sets the kind to \"Namespace,\" which organizes resources within the cluster. The metadata section assigns the name \"development\" to the namespace and labels it accordingly, facilitating resource management and identification within the Kubernetes environment. This YAML configuration is used to create a dedicated environment for development activities, isolating resources from other environments like staging or production.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: development\\n  labels:\\n    name: development\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes namespace named \"development.\" It specifies the API version as v1 and sets the kind to \"Namespace,\" which organizes resources within the cluster. The metadata section assigns the name \"development\" to the namespace and labels it accordingly, facilitating resource management and identification within the Kubernetes environment. This YAML configuration is used to create a dedicated environment for development activities, isolating resources from other environments like staging or production.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\namespace-dev.yaml'},\n",
       "  0.73828125),\n",
       " ({'content': '# The provided content is a YAML configuration defining three custom resources of kind \"Shirt\" within a Kubernetes environment. Each resource specifies metadata including a unique name and a specification section that details the attributes of the shirt, such as its color and size. The configuration uses a custom API version \"stable.example.com/v1\", indicating that these are custom resource definitions (CRDs) managed within a Kubernetes cluster.\\n\\nThe code describes three shirt objects: \"example1\" (blue, size S), \"example2\" (blue, size M), and \"example3\" (green, size M). This setup could be used for managing a collection of shirts in an application or system that interacts with these custom resources, allowing users to specify different shirt configurations through declarative YAML manifests.\\n---\\napiVersion: stable.example.com/v1\\nkind: Shirt\\nmetadata:\\n  name: example1\\nspec:\\n  color: blue\\n  size: S\\n---\\napiVersion: stable.example.com/v1\\nkind: Shirt\\nmetadata:\\n  name: example2\\nspec:\\n  color: blue\\n  size: M\\n---\\napiVersion: stable.example.com/v1\\nkind: Shirt\\nmetadata:\\n  name: example3\\nspec:\\n  color: green\\n  size: M\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'The provided content is a YAML configuration defining three custom resources of kind \"Shirt\" within a Kubernetes environment. Each resource specifies metadata including a unique name and a specification section that details the attributes of the shirt, such as its color and size. The configuration uses a custom API version \"stable.example.com/v1\", indicating that these are custom resource definitions (CRDs) managed within a Kubernetes cluster.\\n\\nThe code describes three shirt objects: \"example1\" (blue, size S), \"example2\" (blue, size M), and \"example3\" (green, size M). This setup could be used for managing a collection of shirts in an application or system that interacts with these custom resources, allowing users to specify different shirt configurations through declarative YAML manifests.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\customresourcedefinition\\\\shirt-resources.yaml'},\n",
       "  0.734375),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Deployment resource called \"my-nginx\" that manages three replicas of an Nginx web server container. The deployment uses the Nginx image version 1.14.2 and exposes port 80 within each container. The purpose of this configuration is to ensure high availability and scalability of the Nginx service by running multiple identical containers that can be managed and updated collectively by Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.14.2\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Deployment resource called \"my-nginx\" that manages three replicas of an Nginx web server container. The deployment uses the Nginx image version 1.14.2 and exposes port 80 within each container. The purpose of this configuration is to ensure high availability and scalability of the Nginx service by running multiple identical containers that can be managed and updated collectively by Kubernetes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\nginx\\\\nginx-deployment.yaml'},\n",
       "  0.734375),\n",
       " ({'content': '# This content is a Kubernetes Deployment configuration written in YAML, aimed at deploying three replicas of a containerized application. The deployment creates pods with a single Alpine Linux container that retrieves an environment variable from a ConfigMap named \"fruits.\" The environment variable \"FRUITS\" gets its value from the key \"fruits\" within this ConfigMap. The container runs an infinite loop, periodically outputting the current date and a message indicating the contents of the fruit basket, then sleeps for ten seconds before repeating. Essentially, this setup demonstrates how to inject configuration data via ConfigMaps into containers and how to run a continuous process that utilizes environment variables fetched from external configurations.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-env-var\\n  labels:\\n    app.kubernetes.io/name: configmap-env-var\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-env-var\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-env-var\\n    spec:\\n      containers:\\n        - name: alpine\\n          image: alpine:3\\n          env:\\n            - name: FRUITS\\n              valueFrom:\\n                configMapKeyRef:\\n                  key: fruits\\n                  name: fruits\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) The basket is full of $FRUITS\";\\n                sleep 10; done;\\n          ports:\\n            - containerPort: 80',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Deployment configuration written in YAML, aimed at deploying three replicas of a containerized application. The deployment creates pods with a single Alpine Linux container that retrieves an environment variable from a ConfigMap named \"fruits.\" The environment variable \"FRUITS\" gets its value from the key \"fruits\" within this ConfigMap. The container runs an infinite loop, periodically outputting the current date and a message indicating the contents of the fruit basket, then sleeps for ten seconds before repeating. Essentially, this setup demonstrates how to inject configuration data via ConfigMaps into containers and how to run a continuous process that utilizes environment variables fetched from external configurations.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-envvar.yaml'},\n",
       "  0.73046875),\n",
       " ({'content': '# The provided content comprises Kubernetes manifests defining resources for deploying a ZooKeeper ensemble. The configuration includes two services, a PodDisruptionBudget, and a StatefulSet that manages three ZooKeeper pods. The `zk-hs` service is headless, allowing direct pod-to-pod communication essential for ZooKeeper quorum, with ports configured for server election and leader election. The `zk-cs` service is a cluster IP service exposing the client port 2181 for external ZooKeeper clients.\\n\\nThe PodDisruptionBudget (`zk-pdb`) ensures that at most one ZooKeeper pod is voluntarily disrupted during maintenance or updates, enhancing high availability. The core of the deployment is the StatefulSet `zk`, which manages three replicas with ordered, rolling updates, and pod anti-affinity policies to distribute pods across different nodes for fault tolerance. \\n\\nEach pod runs a container configured to start ZooKeeper with specific command-line parameters, including server count, data directories, and network ports. Readiness and liveness probes execute a custom \"zookeeper-ready\" command to monitor pod health. Persistent storage is configured via volume claim templates with 10Gi of storage, mounted at `/var/lib/zookeeper` to maintain data consistency across pod restarts. The container runs as user ID 1000 for security purposes. Overall, these manifests orchestrate a resilient, scalable ZooKeeper ensemble suitable for distributed applications requiring coordination services.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: zk-hs\\n  labels:\\n    app: zk\\nspec:\\n  ports:\\n  - port: 2888\\n    name: server\\n  - port: 3888\\n    name: leader-election\\n  clusterIP: None\\n  selector:\\n    app: zk\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: zk-cs\\n  labels:\\n    app: zk\\nspec:\\n  ports:\\n  - port: 2181\\n    name: client\\n  selector:\\n    app: zk\\n---\\napiVersion: policy/v1\\nkind: PodDisruptionBudget\\nmetadata:\\n  name: zk-pdb\\nspec:\\n  selector:\\n    matchLabels:\\n      app: zk\\n  maxUnavailable: 1\\n---\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: zk\\nspec:\\n  selector:\\n    matchLabels:\\n      app: zk\\n  serviceName: zk-hs\\n  replicas: 3\\n  updateStrategy:\\n    type: RollingUpdate\\n  podManagementPolicy: OrderedReady\\n  template:\\n    metadata:\\n      labels:\\n        app: zk\\n    spec:\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            - labelSelector:\\n                matchExpressions:\\n                  - key: \"app\"\\n                    operator: In\\n                    values:\\n                    - zk\\n              topologyKey: \"kubernetes.io/hostname\"\\n      containers:\\n      - name: kubernetes-zookeeper\\n        imagePullPolicy: Always\\n        image: \"registry.k8s.io/kubernetes-zookeeper:1.0-3.4.10\"\\n        resources:\\n          requests:\\n            memory: \"1Gi\"\\n            cpu: \"0.5\"\\n        ports:\\n        - containerPort: 2181\\n          name: client\\n        - containerPort: 2888\\n          name: server\\n        - containerPort: 3888\\n          name: leader-election\\n        command:\\n        - sh\\n        - -c\\n        - \"start-zookeeper \\\\\\n          --servers=3 \\\\\\n          --data_dir=/var/lib/zookeeper/data \\\\\\n          --data_log_dir=/var/lib/zookeeper/data/log \\\\\\n          --conf_dir=/opt/zookeeper/conf \\\\\\n          --client_port=2181 \\\\\\n          --election_port=3888 \\\\\\n          --server_port=2888 \\\\\\n          --tick_time=2000 \\\\\\n          --init_limit=10 \\\\\\n          --sync_limit=5 \\\\\\n          --heap=512M \\\\\\n          --max_client_cnxns=60 \\\\\\n          --snap_retain_count=3 \\\\\\n          --purge_interval=12 \\\\\\n          --max_session_timeout=40000 \\\\\\n          --min_session_timeout=4000 \\\\\\n          --log_level=INFO\"\\n        readinessProbe:\\n          exec:\\n            command:\\n            - sh\\n            - -c\\n            - \"zookeeper-ready 2181\"\\n          initialDelaySeconds: 10\\n          timeoutSeconds: 5\\n        livenessProbe:\\n          exec:\\n            command:\\n            - sh\\n            - -c\\n            - \"zookeeper-ready 2181\"\\n          initialDelaySeconds: 10\\n          timeoutSeconds: 5\\n        volumeMounts:\\n        - name: datadir\\n          mountPath: /var/lib/zookeeper\\n      securityContext:\\n        runAsUser: 1000\\n        fsGroup: 1000\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: datadir\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      resources:\\n        requests:\\n          storage: 10Gi\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content comprises Kubernetes manifests defining resources for deploying a ZooKeeper ensemble. The configuration includes two services, a PodDisruptionBudget, and a StatefulSet that manages three ZooKeeper pods. The `zk-hs` service is headless, allowing direct pod-to-pod communication essential for ZooKeeper quorum, with ports configured for server election and leader election. The `zk-cs` service is a cluster IP service exposing the client port 2181 for external ZooKeeper clients.\\n\\nThe PodDisruptionBudget (`zk-pdb`) ensures that at most one ZooKeeper pod is voluntarily disrupted during maintenance or updates, enhancing high availability. The core of the deployment is the StatefulSet `zk`, which manages three replicas with ordered, rolling updates, and pod anti-affinity policies to distribute pods across different nodes for fault tolerance. \\n\\nEach pod runs a container configured to start ZooKeeper with specific command-line parameters, including server count, data directories, and network ports. Readiness and liveness probes execute a custom \"zookeeper-ready\" command to monitor pod health. Persistent storage is configured via volume claim templates with 10Gi of storage, mounted at `/var/lib/zookeeper` to maintain data consistency across pod restarts. The container runs as user ID 1000 for security purposes. Overall, these manifests orchestrate a resilient, scalable ZooKeeper ensemble suitable for distributed applications requiring coordination services.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\zookeeper\\\\zookeeper.yaml'},\n",
       "  0.73046875),\n",
       " ({'content': '# This content defines a Kubernetes ReplicaSet configuration in YAML format. A ReplicaSet ensures a specified number of pod replicas are running at all times. In this case, it is named \"frontend\" and is labeled with \"app: guestbook\" and \"tier: frontend\". The ReplicaSet is set to maintain 3 replicas, matching pods with the label \"tier: frontend\". The pod template specifies a container named \"php-redis\" that uses a particular Docker image hosted on Google\\'s container registry, which likely contains the application code needed for the frontend service. Overall, this configuration automates the deployment and scaling of multiple instances of a frontend application in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: ReplicaSet\\nmetadata:\\n  name: frontend\\n  labels:\\n    app: guestbook\\n    tier: frontend\\nspec:\\n  # modify replicas according to your case\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      tier: frontend\\n  template:\\n    metadata:\\n      labels:\\n        tier: frontend\\n    spec:\\n      containers:\\n      - name: php-redis\\n        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes ReplicaSet configuration in YAML format. A ReplicaSet ensures a specified number of pod replicas are running at all times. In this case, it is named \"frontend\" and is labeled with \"app: guestbook\" and \"tier: frontend\". The ReplicaSet is set to maintain 3 replicas, matching pods with the label \"tier: frontend\". The pod template specifies a container named \"php-redis\" that uses a particular Docker image hosted on Google\\'s container registry, which likely contains the application code needed for the frontend service. Overall, this configuration automates the deployment and scaling of multiple instances of a frontend application in a Kubernetes cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\frontend.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.73046875),\n",
       " ({'content': '# This content is a Kubernetes replication controller configuration written in YAML. It specifies a resource of type `ReplicationController` that manages five identical pods running the Nginx server. The controller is named `my-nginx-v4`, and it uses labels such as `app: nginx` and `deployment: v4` to identify and select its pods. The pod template defines a single container running the `nginx:1.16.1` image, with the command-line argument `nginx -T`, which starts Nginx in test mode to display the configuration. The container exposes port 80 for web traffic. \\n\\nThis configuration automates the deployment and scaling of multiple Nginx instances to ensure high availability, with each pod providing a running Nginx server listening on port 80.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:\\n  name: my-nginx-v4\\nspec:\\n  replicas: 5\\n  selector:\\n    app: nginx\\n    deployment: v4\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n        deployment: v4\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1\\n        args: [\"nginx\", \"-T\"]\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes replication controller configuration written in YAML. It specifies a resource of type `ReplicationController` that manages five identical pods running the Nginx server. The controller is named `my-nginx-v4`, and it uses labels such as `app: nginx` and `deployment: v4` to identify and select its pods. The pod template defines a single container running the `nginx:1.16.1` image, with the command-line argument `nginx -T`, which starts Nginx in test mode to display the configuration. The container exposes port 80 for web traffic. \\n\\nThis configuration automates the deployment and scaling of multiple Nginx instances to ensure high availability, with each pod providing a running Nginx server listening on port 80.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\replication-nginx-1.16.1.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.73046875),\n",
       " ({'content': '# This content provides a comprehensive YAML configuration for deploying a Kubernetes DNS autoscaler, including the necessary RBAC (Role-Based Access Control) setup and deployment specifications. The configuration begins with creating a ServiceAccount named `kube-dns-autoscaler` in the `kube-system` namespace, which is used by the autoscaler pod for interaction with cluster resources. It then defines a ClusterRole (`system:kube-dns-autoscaler`) with specific permissions to list nodes, get and update deployment and replicaset scales, and access configmaps (noting that the configmaps rule is temporary pending a fix).\\n\\nA ClusterRoleBinding associates this role with the `kube-dns-autoscaler` ServiceAccount, enabling the pod to perform its designated actions across the cluster. The Deployment configuration specifies how to run the autoscaler pod, including resource requests, security context, and node selectors to ensure it runs on Linux nodes with appropriate security settings. Within the container, it uses the `cluster-proportional-autoscaler` image to manage DNS replica counts dynamically based on the cluster size.\\n\\nThe container\\'s command-line arguments set parameters such as the namespace, configuration map name, and target resources to scale (`<SCALE_TARGET>` placeholder). It also defines scaling logic preferences through `default-params`, like `coresPerReplica` and `nodesPerReplica`, which help determine when and how autoscaling should trigger depending on the node hardware and cluster configuration. The configuration ensures the autoscaler operates efficiently and securely within the Kubernetes environment, adjusting DNS pods based on cluster demand.\\nkind: ServiceAccount\\napiVersion: v1\\nmetadata:\\n  name: kube-dns-autoscaler\\n  namespace: kube-system\\n---\\nkind: ClusterRole\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: system:kube-dns-autoscaler\\nrules:\\n  - apiGroups: [\"\"]\\n    resources: [\"nodes\"]\\n    verbs: [\"list\", \"watch\"]\\n  - apiGroups: [\"\"]\\n    resources: [\"replicationcontrollers/scale\"]\\n    verbs: [\"get\", \"update\"]\\n  - apiGroups: [\"apps\"]\\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\\n    verbs: [\"get\", \"update\"]\\n# Remove the configmaps rule once below issue is fixed:\\n# kubernetes-incubator/cluster-proportional-autoscaler#16\\n  - apiGroups: [\"\"]\\n    resources: [\"configmaps\"]\\n    verbs: [\"get\", \"create\"]\\n---\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: system:kube-dns-autoscaler\\nsubjects:\\n  - kind: ServiceAccount\\n    name: kube-dns-autoscaler\\n    namespace: kube-system\\nroleRef:\\n  kind: ClusterRole\\n  name: system:kube-dns-autoscaler\\n  apiGroup: rbac.authorization.k8s.io\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: kube-dns-autoscaler\\n  namespace: kube-system\\n  labels:\\n    k8s-app: kube-dns-autoscaler\\n    kubernetes.io/cluster-service: \"true\"\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: kube-dns-autoscaler\\n  template:\\n    metadata:\\n      labels:\\n        k8s-app: kube-dns-autoscaler\\n    spec:\\n      priorityClassName: system-cluster-critical\\n      securityContext:\\n        seccompProfile:\\n          type: RuntimeDefault\\n        supplementalGroups: [ 65534 ]\\n        fsGroup: 65534\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      containers:\\n      - name: autoscaler\\n        image: registry.k8s.io/cpa/cluster-proportional-autoscaler:1.8.4\\n        resources:\\n            requests:\\n                cpu: \"20m\"\\n                memory: \"10Mi\"\\n        command:\\n          - /cluster-proportional-autoscaler\\n          - --namespace=kube-system\\n          - --configmap=kube-dns-autoscaler\\n          # Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base\\n          - --target=<SCALE_TARGET>\\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\\n          # If using small nodes, \"nodesPerReplica\" should dominate.\\n          - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true,\"includeUnschedulableNodes\":true}}\\n          - --logtostderr=true\\n          - --v=2\\n      tolerations:\\n      - key: \"CriticalAddonsOnly\"\\n        operator: \"Exists\"\\n      serviceAccountName: kube-dns-autoscaler\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': \"This content provides a comprehensive YAML configuration for deploying a Kubernetes DNS autoscaler, including the necessary RBAC (Role-Based Access Control) setup and deployment specifications. The configuration begins with creating a ServiceAccount named `kube-dns-autoscaler` in the `kube-system` namespace, which is used by the autoscaler pod for interaction with cluster resources. It then defines a ClusterRole (`system:kube-dns-autoscaler`) with specific permissions to list nodes, get and update deployment and replicaset scales, and access configmaps (noting that the configmaps rule is temporary pending a fix).\\n\\nA ClusterRoleBinding associates this role with the `kube-dns-autoscaler` ServiceAccount, enabling the pod to perform its designated actions across the cluster. The Deployment configuration specifies how to run the autoscaler pod, including resource requests, security context, and node selectors to ensure it runs on Linux nodes with appropriate security settings. Within the container, it uses the `cluster-proportional-autoscaler` image to manage DNS replica counts dynamically based on the cluster size.\\n\\nThe container's command-line arguments set parameters such as the namespace, configuration map name, and target resources to scale (`<SCALE_TARGET>` placeholder). It also defines scaling logic preferences through `default-params`, like `coresPerReplica` and `nodesPerReplica`, which help determine when and how autoscaling should trigger depending on the node hardware and cluster configuration. The configuration ensures the autoscaler operates efficiently and securely within the Kubernetes environment, adjusting DNS pods based on cluster demand.\",\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\dns\\\\dns-horizontal-autoscaler.yaml'},\n",
       "  0.73046875),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named `nginx-deployment` that manages a pod running an Nginx container. The deployment specifies a label selector to match the pods it manages, with the label `app: nginx`. The pod template within the deployment describes a container named `nginx` using the `nginx:1.16.1` Docker image, which listens on port 80. \\n\\nThe purpose of this configuration is to automate the deployment and management of an Nginx server within a Kubernetes cluster, ensuring it can be scaled, updated, and maintained easily. The YAML specifies the necessary parameters for Kubernetes to create and manage the containerized Nginx application correctly.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: nginx:1.16.1 # update the image\\n        ports:\\n        - containerPort: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration written in YAML. It defines a deployment named `nginx-deployment` that manages a pod running an Nginx container. The deployment specifies a label selector to match the pods it manages, with the label `app: nginx`. The pod template within the deployment describes a container named `nginx` using the `nginx:1.16.1` Docker image, which listens on port 80. \\n\\nThe purpose of this configuration is to automate the deployment and management of an Nginx server within a Kubernetes cluster, ensuring it can be scaled, updated, and maintained easily. The YAML specifies the necessary parameters for Kubernetes to create and manage the containerized Nginx application correctly.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\update_deployment.yaml'},\n",
       "  0.73046875),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration in YAML format, which defines a deployment named \"redis-leader\" running a Redis server container. This deployment ensures that one replica of the Redis leader pod is always running, with specifications for resource requests (CPU and memory) to guarantee availability. The container uses the Redis 6.0.5 image from Docker Hub and exposes port 6379 for Redis communication. In essence, the code automates the deployment of a Redis server instance within a Kubernetes cluster, providing a reliable backend component for applications that need in-memory data storage or caching.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: redis-leader\\n  labels:\\n    app: redis\\n    role: leader\\n    tier: backend\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: redis\\n  template:\\n    metadata:\\n      labels:\\n        app: redis\\n        role: leader\\n        tier: backend\\n    spec:\\n      containers:\\n      - name: leader\\n        image: \"docker.io/redis:6.0.5\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 6379',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration in YAML format, which defines a deployment named \"redis-leader\" running a Redis server container. This deployment ensures that one replica of the Redis leader pod is always running, with specifications for resource requests (CPU and memory) to guarantee availability. The container uses the Redis 6.0.5 image from Docker Hub and exposes port 6379 for Redis communication. In essence, the code automates the deployment of a Redis server instance within a Kubernetes cluster, providing a reliable backend component for applications that need in-memory data storage or caching.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-leader-deployment.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This content defines a Kubernetes Pod configuration using YAML syntax. It specifies a pod named \"mypod\" with a label \"foo: bar\" and includes topology spread constraints to evenly distribute the pod across different zones and nodes based on the label \"foo: bar\". The constraints aim to ensure fault tolerance and balanced workload distribution, with a maximum skew of 1 indicating that pods should not be unevenly distributed beyond one pod difference between zones or nodes. If the constraints cannot be satisfied, scheduling is prevented. The pod contains a single container named \"pause\" that uses a minimal pause image from the registry, typically used as a placeholder or for testing networking and scheduling policies.\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: mypod\\n  labels:\\n    foo: bar\\nspec:\\n  topologySpreadConstraints:\\n  - maxSkew: 1\\n    topologyKey: zone\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  - maxSkew: 1\\n    topologyKey: node\\n    whenUnsatisfiable: DoNotSchedule\\n    labelSelector:\\n      matchLabels:\\n        foo: bar\\n  containers:\\n  - name: pause\\n    image: registry.k8s.io/pause:3.1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\topology-spread-constraints\\\\two-constraints.yaml',\n",
       "   'summary': 'This content defines a Kubernetes Pod configuration using YAML syntax. It specifies a pod named \"mypod\" with a label \"foo: bar\" and includes topology spread constraints to evenly distribute the pod across different zones and nodes based on the label \"foo: bar\". The constraints aim to ensure fault tolerance and balanced workload distribution, with a maximum skew of 1 indicating that pods should not be unevenly distributed beyond one pod difference between zones or nodes. If the constraints cannot be satisfied, scheduling is prevented. The pod contains a single container named \"pause\" that uses a minimal pause image from the registry, typically used as a placeholder or for testing networking and scheduling policies.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.7265625),\n",
       " ({'content': '# The provided configuration defines a Kubernetes setup with a headless Service and a StatefulSet for deploying NGINX web servers. The Service, named \"nginx,\" exposes port 80 and uses a label selector to associate with the pods, enabling network communication within the cluster without a ClusterIP. The StatefulSet named \"web\" manages two replicas of an NGINX container based on the specified image, ensuring each pod has a stable network identity and persistent storage. Each pod mounts a PersistentVolumeClaim (PVC) named \"www,\" requesting 1Gi of storage with read-write once access mode, to serve or store web content. The overall design ensures scalable, stable, and data-persistent deployment of NGINX instances in a Kubernetes environment.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx\\n  labels:\\n    app: nginx\\nspec:\\n  ports:\\n  - port: 80\\n    name: web\\n  clusterIP: None\\n  selector:\\n    app: nginx\\n---\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: web\\nspec:\\n  serviceName: \"nginx\"\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: registry.k8s.io/nginx-slim:0.21\\n        ports:\\n        - containerPort: 80\\n          name: web\\n        volumeMounts:\\n        - name: www\\n          mountPath: /usr/share/nginx/html\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: www\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      resources:\\n        requests:\\n          storage: 1Gi\\n\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided configuration defines a Kubernetes setup with a headless Service and a StatefulSet for deploying NGINX web servers. The Service, named \"nginx,\" exposes port 80 and uses a label selector to associate with the pods, enabling network communication within the cluster without a ClusterIP. The StatefulSet named \"web\" manages two replicas of an NGINX container based on the specified image, ensuring each pod has a stable network identity and persistent storage. Each pod mounts a PersistentVolumeClaim (PVC) named \"www,\" requesting 1Gi of storage with read-write once access mode, to serve or store web content. The overall design ensures scalable, stable, and data-persistent deployment of NGINX instances in a Kubernetes environment.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"redis-follower\" that manages two replicas of a containerized Redis follower. The deployment specifies labels for organization and selector matching, ensuring that the correct pods are targeted. Each pod runs a container based on a specific Redis follower image, with resource requests set to limit CPU and memory usage, and exposes port 6379 for Redis communication. This configuration facilitates scalable, containerized deployment of Redis followers within a Kubernetes cluster, supporting application resilience and load balancing.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: redis-follower\\n  labels:\\n    app: redis\\n    role: follower\\n    tier: backend\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: redis\\n  template:\\n    metadata:\\n      labels:\\n        app: redis\\n        role: follower\\n        tier: backend\\n    spec:\\n      containers:\\n      - name: follower\\n        image: us-docker.pkg.dev/google-samples/containers/gke/gb-redis-follower:v2\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 6379',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes deployment configuration written in YAML. It defines a Deployment named \"redis-follower\" that manages two replicas of a containerized Redis follower. The deployment specifies labels for organization and selector matching, ensuring that the correct pods are targeted. Each pod runs a container based on a specific Redis follower image, with resource requests set to limit CPU and memory usage, and exposes port 6379 for Redis communication. This configuration facilitates scalable, containerized deployment of Redis followers within a Kubernetes cluster, supporting application resilience and load balancing.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\redis-follower-deployment.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Deployment for an IIS web server. It specifies the deployment of three replicas of the containerized application to ensure availability and load balancing. The deployment uses the `microsoft/iis` Docker image and assigns resource limits: a maximum of 128MiB of memory and two CPUs per container. The container listens on port 80, which is exposed within the cluster, enabling HTTP traffic to reach the IIS server. Overall, this configuration automates the deployment, scaling, and resource management of multiple IIS instances within a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iis\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: iis\\n  template:\\n    metadata:\\n      labels:\\n        app: iis\\n    spec:\\n      containers:\\n      - name: iis\\n        image: microsoft/iis\\n        resources:\\n          limits:\\n            memory: \"128Mi\"\\n            cpu: 2\\n        ports:\\n        - containerPort: 80\\n\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Deployment for an IIS web server. It specifies the deployment of three replicas of the containerized application to ensure availability and load balancing. The deployment uses the `microsoft/iis` Docker image and assigns resource limits: a maximum of 128MiB of memory and two CPUs per container. The container listens on port 80, which is exposed within the cluster, enabling HTTP traffic to reach the IIS server. Overall, this configuration automates the deployment, scaling, and resource management of multiple IIS instances within a Kubernetes environment.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\deploy-resource.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.7265625),\n",
       " ({'content': '# This content is a Kubernetes Deployment configuration defining a simple application stack. It creates a deployment named \"myapp\" with a single replica that runs an Alpine Linux container. The main container executes a shell command that continuously appends the word \"logging\" to a log file every second, simulating ongoing log generation. It mounts a shared volume at \"/opt\" to store logs persistently. \\n\\nAn init container called \"logshipper\" is configured to run before the main container; it also utilizes Alpine Linux and executes a command to continuously \"tail\" the log file, which allows it to process or monitor log entries as they are created. Both containers share an \"emptyDir\" volume named \"data,\" which provides temporary storage during the pod\\'s lifecycle and facilitates log sharing between the init and main containers.\\n\\nOverall, this setup demonstrates how to coordinate multiple containers within a pod using shared volumes, with the init container preparing or monitoring logs before the main application runs, highlighting concepts like volume sharing, init containers, and container orchestration in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: myapp\\n  labels:\\n    app: myapp\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: myapp\\n  template:\\n    metadata:\\n      labels:\\n        app: myapp\\n    spec:\\n      containers:\\n        - name: myapp\\n          image: alpine:latest\\n          command: [\\'sh\\', \\'-c\\', \\'while true; do echo \"logging\" >> /opt/logs.txt; sleep 1; done\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      initContainers:\\n        - name: logshipper\\n          image: alpine:latest\\n          restartPolicy: Always\\n          command: [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      volumes:\\n        - name: data\\n          emptyDir: {}',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\deployment-sidecar.yaml',\n",
       "   'summary': 'This content is a Kubernetes Deployment configuration defining a simple application stack. It creates a deployment named \"myapp\" with a single replica that runs an Alpine Linux container. The main container executes a shell command that continuously appends the word \"logging\" to a log file every second, simulating ongoing log generation. It mounts a shared volume at \"/opt\" to store logs persistently. \\n\\nAn init container called \"logshipper\" is configured to run before the main container; it also utilizes Alpine Linux and executes a command to continuously \"tail\" the log file, which allows it to process or monitor log entries as they are created. Both containers share an \"emptyDir\" volume named \"data,\" which provides temporary storage during the pod\\'s lifecycle and facilitates log sharing between the init and main containers.\\n\\nOverall, this setup demonstrates how to coordinate multiple containers within a pod using shared volumes, with the init container preparing or monitoring logs before the main application runs, highlighting concepts like volume sharing, init containers, and container orchestration in Kubernetes.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.7265625),\n",
       " ({'content': '# The provided content includes Kubernetes configuration files for deploying a MySQL database in a containerized environment. It consists of two main resources: a Service and a Deployment. The Service is set up with `clusterIP: None`, which creates a Headless Service allowing direct access to individual MySQL pods for stateful applications or discovery purposes. It exposes port 3306 and uses a label selector to target pods with the label `app: mysql`.\\n\\nThe Deployment defines the desired state for running MySQL version 5.6, including replica management and update strategies. It specifies a containerized MySQL instance with environment variables for configuration, such as setting the root password (note that in practical scenarios, secrets should be securely managed rather than hardcoded). The deployment also uses a volume mount to attach persistent storage, ensuring data durability, by linking to a PersistentVolumeClaim named `mysql-pv-claim`. These configurations automate the deployment, scaling, and persistence of MySQL within a Kubernetes cluster, enabling a reliable and manageable database service.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: mysql\\nspec:\\n  ports:\\n  - port: 3306\\n  selector:\\n    app: mysql\\n  clusterIP: None\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: mysql\\nspec:\\n  selector:\\n    matchLabels:\\n      app: mysql\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      labels:\\n        app: mysql\\n    spec:\\n      containers:\\n      - image: mysql:5.6\\n        name: mysql\\n        env:\\n          # Use secret in real usage\\n        - name: MYSQL_ROOT_PASSWORD\\n          value: password\\n        ports:\\n        - containerPort: 3306\\n          name: mysql\\n        volumeMounts:\\n        - name: mysql-persistent-storage\\n          mountPath: /var/lib/mysql\\n      volumes:\\n      - name: mysql-persistent-storage\\n        persistentVolumeClaim:\\n          claimName: mysql-pv-claim\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content includes Kubernetes configuration files for deploying a MySQL database in a containerized environment. It consists of two main resources: a Service and a Deployment. The Service is set up with `clusterIP: None`, which creates a Headless Service allowing direct access to individual MySQL pods for stateful applications or discovery purposes. It exposes port 3306 and uses a label selector to target pods with the label `app: mysql`.\\n\\nThe Deployment defines the desired state for running MySQL version 5.6, including replica management and update strategies. It specifies a containerized MySQL instance with environment variables for configuration, such as setting the root password (note that in practical scenarios, secrets should be securely managed rather than hardcoded). The deployment also uses a volume mount to attach persistent storage, ensuring data durability, by linking to a PersistentVolumeClaim named `mysql-pv-claim`. These configurations automate the deployment, scaling, and persistence of MySQL within a Kubernetes cluster, enabling a reliable and manageable database service.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-deployment.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration written in YAML, which manages the deployment of a frontend application. The deployment is named \"frontend\" and specifies a selector to match pods with specific labels, ensuring proper management and updates. It is configured to run a single replica pod.\\n\\nThe pod template within the deployment defines a container running the NGINX web server using the Docker image \"gcr.io/google-samples/hello-frontend:1.0\". It also includes a lifecycle hook, specifically the `preStop` hook, which executes the command `\"/usr/sbin/nginx -s quit\"` when the container is about to stop. This command gracefully instructs NGINX to shut down, allowing for proper termination and resource cleanup. Overall, this YAML configuration automates deploying a frontend service with a graceful shutdown process.\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  selector:\\n    matchLabels:\\n      app: hello\\n      tier: frontend\\n      track: stable\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: hello\\n        tier: frontend\\n        track: stable\\n    spec:\\n      containers:\\n        - name: nginx\\n          image: \"gcr.io/google-samples/hello-frontend:1.0\"\\n          lifecycle:\\n            preStop:\\n              exec:\\n                command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\\n...',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\access\\\\frontend-deployment.yaml',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration written in YAML, which manages the deployment of a frontend application. The deployment is named \"frontend\" and specifies a selector to match pods with specific labels, ensuring proper management and updates. It is configured to run a single replica pod.\\n\\nThe pod template within the deployment defines a container running the NGINX web server using the Docker image \"gcr.io/google-samples/hello-frontend:1.0\". It also includes a lifecycle hook, specifically the `preStop` hook, which executes the command `\"/usr/sbin/nginx -s quit\"` when the container is about to stop. This command gracefully instructs NGINX to shut down, allowing for proper termination and resource cleanup. Overall, this YAML configuration automates deploying a frontend service with a graceful shutdown process.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.7265625),\n",
       " ({'content': '# This Kubernetes manifest defines a Deployment named \"configmap-volume\" that manages three replica pods. These pods run Alpine Linux containers with a command that continuously prints the current date along with the value of a configuration parameter called \"sport\" every 10 seconds. The container mounts a ConfigMap named \"sport\" as a volume at the path \"/etc/config.\" This setup enables the container to access configuration data stored in the ConfigMap, which can be dynamically updated without redeploying the pods. The key aspect of this configuration is the use of volumes linked to ConfigMaps, allowing for flexible and centralized configuration management in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-volume\\n  labels:\\n    app.kubernetes.io/name: configmap-volume\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-volume\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-volume\\n    spec:\\n      containers:\\n        - name: alpine\\n          image: alpine:3\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) My preferred sport is $(cat /etc/config/sport)\";\\n              sleep 10; done;\\n          ports:\\n            - containerPort: 80\\n          volumeMounts:\\n            - name: config-volume\\n              mountPath: /etc/config\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: sport',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This Kubernetes manifest defines a Deployment named \"configmap-volume\" that manages three replica pods. These pods run Alpine Linux containers with a command that continuously prints the current date along with the value of a configuration parameter called \"sport\" every 10 seconds. The container mounts a ConfigMap named \"sport\" as a volume at the path \"/etc/config.\" This setup enables the container to access configuration data stored in the ConfigMap, which can be dynamically updated without redeploying the pods. The key aspect of this configuration is the use of volumes linked to ConfigMaps, allowing for flexible and centralized configuration management in Kubernetes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-as-volume.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This content defines a Kubernetes PriorityClass resource named \"placeholder,\" which is used to assign a negative priority value (-1000) to specific Pods. The purpose of this negative priority is to enable overprovisioning by ensuring these placeholder Pods have lower priority compared to other workload Pods. This allows the system to pre-allocate resources efficiently while maintaining the ability to preempt placeholder Pods if higher-priority Pods need resources. The PriorityClass is not set as the global default, meaning it applies only where explicitly specified.\\napiVersion: scheduling.k8s.io/v1\\nkind: PriorityClass\\nmetadata:\\n  name: placeholder # these Pods represent placeholder capacity\\nvalue: -1000\\nglobalDefault: false\\ndescription: \"Negative priority for placeholder pods to enable overprovisioning.\"',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes PriorityClass resource named \"placeholder,\" which is used to assign a negative priority value (-1000) to specific Pods. The purpose of this negative priority is to enable overprovisioning by ensuring these placeholder Pods have lower priority compared to other workload Pods. This allows the system to pre-allocate resources efficiently while maintaining the ability to preempt placeholder Pods if higher-priority Pods need resources. The PriorityClass is not set as the global default, meaning it applies only where explicitly specified.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\priorityclass\\\\low-priority-class.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This content is a Kubernetes Service definition in YAML format that configures network access to a set of pods running the nginx application. The service is named \"nginx-service\" and is configured to listen on port 8000, which is the external port clients will connect to. The `targetPort` specifies that traffic received on port 8000 will be forwarded to port 80 on the nginx pods. The `selector` matches pods labeled with `app: nginx`, ensuring that the service load balances incoming traffic across all such pods.\\n\\nIn terms of functionality, this configuration creates a TCP load balancer that exposes nginx pods on port 8000, rerouting traffic to their internal port 80. This allows external clients to access the nginx application through a stable, dedicated service endpoint. The YAML is a fundamental part of managing network traffic in Kubernetes, enabling decoupling of the exposed port from the internal container port and facilitating scalable, reliable access to containerized applications.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx-service\\nspec:\\n  ports:\\n  - port: 8000 # the port that this service should serve on\\n    # the container on each pod to connect to, can be a name\\n    # (e.g. \\'www\\') or a number (e.g. 80)\\n    targetPort: 80\\n    protocol: TCP\\n  # just like the selector in the deployment,\\n  # but this time it identifies the set of pods to load balance\\n  # traffic to.\\n  selector:\\n    app: nginx\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Service definition in YAML format that configures network access to a set of pods running the nginx application. The service is named \"nginx-service\" and is configured to listen on port 8000, which is the external port clients will connect to. The `targetPort` specifies that traffic received on port 8000 will be forwarded to port 80 on the nginx pods. The `selector` matches pods labeled with `app: nginx`, ensuring that the service load balances incoming traffic across all such pods.\\n\\nIn terms of functionality, this configuration creates a TCP load balancer that exposes nginx pods on port 8000, rerouting traffic to their internal port 80. This allows external clients to access the nginx application through a stable, dedicated service endpoint. The YAML is a fundamental part of managing network traffic in Kubernetes, enabling decoupling of the exposed port from the internal container port and facilitating scalable, reliable access to containerized applications.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\nginx-service.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Pod specification. It creates a pod named \"example-no-conflict-with-limitrange-cpu\" with a single container named \"demo\" that uses the \"pause:3.8\" image from the Kubernetes container registry. The resource requests and limits for CPU are set to 700 millicores (0.7 CPU). This configuration ensures that the container requests a specific amount of CPU resources and enforces a limit to prevent it from consuming more, promoting efficient resource management and avoiding contention with other pods.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: example-no-conflict-with-limitrange-cpu\\nspec:\\n  containers:\\n  - name: demo\\n    image: registry.k8s.io/pause:3.8\\n    resources:\\n      requests:\\n        cpu: 700m\\n      limits:\\n        cpu: 700m\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Pod specification. It creates a pod named \"example-no-conflict-with-limitrange-cpu\" with a single container named \"demo\" that uses the \"pause:3.8\" image from the Kubernetes container registry. The resource requests and limits for CPU are set to 700 millicores (0.7 CPU). This configuration ensures that the container requests a specific amount of CPU resources and enforces a limit to prevent it from consuming more, promoting efficient resource management and avoiding contention with other pods.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\concepts\\\\policy\\\\limit-range\\\\example-no-conflict-with-limitrange-cpu.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Pod named \"cpu-demo\" in the \"cpu-example\" namespace. The Pod contains a single container built from the \"vish/stress\" image, which is typically used to generate stress or load on system resources. The resource allocation specifies a CPU request of 0.5 cores and a limit of 1 core, meaning the container is guaranteed half a CPU but cannot exceed one. The container is configured to run with arguments \"-cpus\" and \"2,\" instructing it to utilize two CPUs for stress testing despite the resource limits, demonstrating how resource requests, limits, and container arguments work together to manage CPU workload in Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: cpu-demo\\n  namespace: cpu-example\\nspec:\\n  containers:\\n  - name: cpu-demo-ctr\\n    image: vish/stress\\n    resources:\\n      limits:\\n        cpu: \"1\"\\n      requests:\\n        cpu: \"0.5\"\\n    args:\\n    - -cpus\\n    - \"2\"\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Pod named \"cpu-demo\" in the \"cpu-example\" namespace. The Pod contains a single container built from the \"vish/stress\" image, which is typically used to generate stress or load on system resources. The resource allocation specifies a CPU request of 0.5 cores and a limit of 1 core, meaning the container is guaranteed half a CPU but cannot exceed one. The container is configured to run with arguments \"-cpus\" and \"2,\" instructing it to utilize two CPUs for stress testing despite the resource limits, demonstrating how resource requests, limits, and container arguments work together to manage CPU workload in Kubernetes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\cpu-request-limit.yaml'},\n",
       "  0.7265625),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes Deployment named \"curl-deployment\" with a single replica pod. The pod uses a container based on the \"radial/busyboxplus:curl\" image, which includes basic Unix utilities along with curl for network testing. The container runs an infinite loop to keep it alive, allowing ongoing interactions or debugging.\\n\\nThe key feature of this deployment is the use of an attached secret-based volume (\"secret-volume\"). This volume mounts a Kubernetes secret called \"nginxsecret\" to the directory \"/etc/nginx/ssl\" inside the container. This setup enables secure access to sensitive data, such as SSL certificates or keys, within the container environment, facilitating secure communication or configuration for services like nginx. Overall, the YAML provides a minimal setup for deploying a container that can access secret data securely for tasks such as SSL/TLS operations or other secret-dependent processes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: curl-deployment\\nspec:\\n  selector:\\n    matchLabels:\\n      app: curlpod\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: curlpod\\n    spec:\\n      volumes:\\n      - name: secret-volume\\n        secret:\\n          secretName: nginxsecret\\n      containers:\\n      - name: curlpod\\n        command:\\n        - sh\\n        - -c\\n        - while true; do sleep 1; done\\n        image: radial/busyboxplus:curl\\n        volumeMounts:\\n        - mountPath: /etc/nginx/ssl\\n          name: secret-volume\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided YAML configuration defines a Kubernetes Deployment named \"curl-deployment\" with a single replica pod. The pod uses a container based on the \"radial/busyboxplus:curl\" image, which includes basic Unix utilities along with curl for network testing. The container runs an infinite loop to keep it alive, allowing ongoing interactions or debugging.\\n\\nThe key feature of this deployment is the use of an attached secret-based volume (\"secret-volume\"). This volume mounts a Kubernetes secret called \"nginxsecret\" to the directory \"/etc/nginx/ssl\" inside the container. This setup enables secure access to sensitive data, such as SSL certificates or keys, within the container environment, facilitating secure communication or configuration for services like nginx. Overall, the YAML provides a minimal setup for deploying a container that can access secret data securely for tasks such as SSL/TLS operations or other secret-dependent processes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\curlpod.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# This content is a Kubernetes Deployment configuration that orchestrates a set of pods running an nginx server with a shared volume and an init container. The deployment creates three replicas of the pod, each consisting of an nginx container and an init container named \"alpine.\" The init container continuously writes the current date and a preferred color (fetched from a ConfigMap named \"color\") into an HTML file located in a shared volume, which nginx then serves via its default document root. \\n\\nThe configuration includes two volumes: a shared empty directory (`shared-data`) used for communication between containers, and a ConfigMap volume (`config-volume`) that provides configuration data (the preferred color). The init container mounts both volumes, periodically updating an HTML file with the latest data, while the nginx container mounts the shared directory to serve this content dynamically. This setup demonstrates the use of init containers for initialization tasks, shared volumes for data exchange, and ConfigMaps for configuration management in Kubernetes.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-sidecar-container\\n  labels:\\n    app.kubernetes.io/name: configmap-sidecar-container\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-sidecar-container\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-sidecar-container\\n    spec:\\n      volumes:\\n        - name: shared-data\\n          emptyDir: {}\\n        - name: config-volume\\n          configMap:\\n            name: color\\n      containers:\\n        - name: nginx\\n          image: nginx\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /usr/share/nginx/html\\n      initContainers:\\n        - name: alpine\\n          image: alpine:3\\n          restartPolicy: Always\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /pod-data\\n            - name: config-volume\\n              mountPath: /etc/config\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) My preferred color is $(cat /etc/config/color)\" > /pod-data/index.html;\\n              sleep 10; done;\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Deployment configuration that orchestrates a set of pods running an nginx server with a shared volume and an init container. The deployment creates three replicas of the pod, each consisting of an nginx container and an init container named \"alpine.\" The init container continuously writes the current date and a preferred color (fetched from a ConfigMap named \"color\") into an HTML file located in a shared volume, which nginx then serves via its default document root. \\n\\nThe configuration includes two volumes: a shared empty directory (`shared-data`) used for communication between containers, and a ConfigMap volume (`config-volume`) that provides configuration data (the preferred color). The init container mounts both volumes, periodically updating an HTML file with the latest data, while the nginx container mounts the shared directory to serve this content dynamically. This setup demonstrates the use of init containers for initialization tasks, shared volumes for data exchange, and ConfigMaps for configuration management in Kubernetes.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-and-sidecar-container.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# The given content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"extended-resource-demo-2\" that contains a single container running the nginx image. The configuration specifies resource requests and limits for a custom extended resource called \"example.com/dongle,\" setting both to 2 units. This ensures that the container requests a minimum of 2 units of the extended resource and is limited to using up to 2 units, enabling efficient resource allocation and management within a Kubernetes cluster that supports custom resources.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: extended-resource-demo-2\\nspec:\\n  containers:\\n  - name: extended-resource-demo-2-ctr\\n    image: nginx\\n    resources:\\n      requests:\\n        example.com/dongle: 2\\n      limits:\\n        example.com/dongle: 2\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The given content is a Kubernetes Pod configuration written in YAML. It defines a pod named \"extended-resource-demo-2\" that contains a single container running the nginx image. The configuration specifies resource requests and limits for a custom extended resource called \"example.com/dongle,\" setting both to 2 units. This ensures that the container requests a minimum of 2 units of the extended resource and is limited to using up to 2 units, enabling efficient resource allocation and management within a Kubernetes cluster that supports custom resources.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\extended-resource-pod-2.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# The provided content is a Kubernetes YAML manifest that defines a Pod with specific resource requests and limits. The Pod, named \"pod-resources-demo,\" is located in the \"pod-resources-example\" namespace. It requests and limits a total of 1 CPU and 100Mi of memory at the Pod level. Inside, it contains two containers: one running Nginx with a request and limit of 0.5 CPU and 50Mi of memory, and another running Fedora with a command to sleep indefinitely. This setup demonstrates resource management, illustrating how CPU and memory allocations can be specified at both the Pod and container levels to ensure proper resource allocation and isolation within a Kubernetes cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-resources-demo\\n  namespace: pod-resources-example\\nspec:\\n  resources:\\n    limits:\\n      cpu: \"1\"\\n      memory: \"200Mi\"\\n    requests:\\n      cpu: \"1\"\\n      memory: \"100Mi\"\\n  containers:\\n  - name: pod-resources-demo-ctr-1\\n    image: nginx\\n    resources:\\n      limits:\\n        cpu: \"0.5\"\\n        memory: \"100Mi\"\\n      requests:\\n        cpu: \"0.5\"\\n        memory: \"50Mi\"\\n  - name: pod-resources-demo-ctr-2\\n    image: fedora\\n    command:\\n    - sleep\\n    - inf \\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\pod-level-resources.yaml',\n",
       "   'summary': 'The provided content is a Kubernetes YAML manifest that defines a Pod with specific resource requests and limits. The Pod, named \"pod-resources-demo,\" is located in the \"pod-resources-example\" namespace. It requests and limits a total of 1 CPU and 100Mi of memory at the Pod level. Inside, it contains two containers: one running Nginx with a request and limit of 0.5 CPU and 50Mi of memory, and another running Fedora with a command to sleep indefinitely. This setup demonstrates resource management, illustrating how CPU and memory allocations can be specified at both the Pod and container levels to ensure proper resource allocation and isolation within a Kubernetes cluster.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.72265625),\n",
       " ({'content': '# The provided content is a Kubernetes custom resource definition (CRD) in YAML format. It defines a resource of kind `ReplicaLimit` with the API version `rules.example.com/v1`. The resource is named \"replica-limit-test.example.com\" and is placed in the \"default\" namespace. The key parameter specifies a maximum replica limit of 3, which likely controls the number of pod replicas that can be deployed or scaled for a specific application or component. This YAML does not contain executable code but rather configuration data used to enforce deployment constraints within a Kubernetes cluster.\\napiVersion: rules.example.com/v1\\nkind: ReplicaLimit\\nmetadata:\\n  name: \"replica-limit-test.example.com\"\\n  namespace: \"default\"\\nmaxReplicas: 3\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes custom resource definition (CRD) in YAML format. It defines a resource of kind `ReplicaLimit` with the API version `rules.example.com/v1`. The resource is named \"replica-limit-test.example.com\" and is placed in the \"default\" namespace. The key parameter specifies a maximum replica limit of 3, which likely controls the number of pod replicas that can be deployed or scaled for a specific application or component. This YAML does not contain executable code but rather configuration data used to enforce deployment constraints within a Kubernetes cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\validatingadmissionpolicy\\\\replicalimit-param.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# The content is a Kubernetes YAML configuration defining a Deployment that manages three replica pods running an Alpine Linux container. The container continuously executes a shell command that prints the current date and the contents of a configuration file, updating every 10 seconds. The configuration uses a ConfigMap volume named `company-name-20150801`, which is mounted at `/etc/config` inside each container, making the configuration data accessible to the running application. This setup enables dynamic configuration management and demonstrates how ConfigMaps can be mounted as volumes to provide configuration data to containers.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: immutable-configmap-volume\\n  labels:\\n    app.kubernetes.io/name: immutable-configmap-volume\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: immutable-configmap-volume\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: immutable-configmap-volume\\n    spec:\\n      containers:\\n        - name: alpine\\n          image: alpine:3\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) The name of the company is $(cat /etc/config/company_name)\";\\n              sleep 10; done;\\n          ports:\\n            - containerPort: 80\\n          volumeMounts:\\n            - name: config-volume\\n              mountPath: /etc/config\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: company-name-20150801',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The content is a Kubernetes YAML configuration defining a Deployment that manages three replica pods running an Alpine Linux container. The container continuously executes a shell command that prints the current date and the contents of a configuration file, updating every 10 seconds. The configuration uses a ConfigMap volume named `company-name-20150801`, which is mounted at `/etc/config` inside each container, making the configuration data accessible to the running application. This setup enables dynamic configuration management and demonstrates how ConfigMaps can be mounted as volumes to provide configuration data to containers.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-immutable-configmap-as-volume.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# This content defines a Kubernetes Deployment configuration with two containers sharing a volume and using a ConfigMap. The deployment, named \"configmap-two-containers,\" creates three replicas of a pod, each containing an nginx server and an alpine container. The nginx container mounts a shared emptyDir volume at \"/usr/share/nginx/html\" to serve static content. The alpine container, running a shell, mounts the same shared volume at \"/pod-data\" for writing output, and a ConfigMap named \"color\" at \"/etc/config\" to access configuration data. The alpine container executes a continuous loop that writes the current date and a message indicating the preferred color (from the ConfigMap) into the file \"/pod-data/index.html\" every 10 seconds. This setup enables dynamic content updates and data sharing between containers within the pod.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: configmap-two-containers\\n  labels:\\n    app.kubernetes.io/name: configmap-two-containers\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: configmap-two-containers\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: configmap-two-containers\\n    spec:\\n      volumes:\\n        - name: shared-data\\n          emptyDir: {}\\n        - name: config-volume\\n          configMap:\\n            name: color\\n      containers:\\n        - name: nginx\\n          image: nginx\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /usr/share/nginx/html\\n        - name: alpine\\n          image: alpine:3\\n          volumeMounts:\\n            - name: shared-data\\n              mountPath: /pod-data\\n            - name: config-volume\\n              mountPath: /etc/config\\n          command:\\n            - /bin/sh\\n            - -c\\n            - while true; do echo \"$(date) My preferred color is $(cat /etc/config/color)\" > /pod-data/index.html;\\n              sleep 10; done;\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes Deployment configuration with two containers sharing a volume and using a ConfigMap. The deployment, named \"configmap-two-containers,\" creates three replicas of a pod, each containing an nginx server and an alpine container. The nginx container mounts a shared emptyDir volume at \"/usr/share/nginx/html\" to serve static content. The alpine container, running a shell, mounts the same shared volume at \"/pod-data\" for writing output, and a ConfigMap named \"color\" at \"/etc/config\" to access configuration data. The alpine container executes a continuous loop that writes the current date and a message indicating the preferred color (from the ConfigMap) into the file \"/pod-data/index.html\" every 10 seconds. This setup enables dynamic content updates and data sharing between containers within the pod.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\deployments\\\\deployment-with-configmap-two-containers.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# The provided configuration is a Kubernetes Pod definition in YAML format. It specifies a pod with the name `constraints-cpu-demo-4`, which contains a single container named `constraints-cpu-demo-4-ctr` using the Docker image `vish/stress`. This setup likely aims to run the stress testing tool within the container to simulate CPU load or performance constraints.\\n\\nThe code focuses on deploying a container that can be used for CPU or resource testing, which is useful in understanding how Kubernetes handles resource constraints or for performance benchmarking. It is a basic example showing how to define a pod with a specific image, but it does not include resource limit configurations or commands, which could be added to extend its functionality for resource management experiments.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: constraints-cpu-demo-4\\nspec:\\n  containers:\\n  - name: constraints-cpu-demo-4-ctr\\n    image: vish/stress\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided configuration is a Kubernetes Pod definition in YAML format. It specifies a pod with the name `constraints-cpu-demo-4`, which contains a single container named `constraints-cpu-demo-4-ctr` using the Docker image `vish/stress`. This setup likely aims to run the stress testing tool within the container to simulate CPU load or performance constraints.\\n\\nThe code focuses on deploying a container that can be used for CPU or resource testing, which is useful in understanding how Kubernetes handles resource constraints or for performance benchmarking. It is a basic example showing how to define a pod with a specific image, but it does not include resource limit configurations or commands, which could be added to extend its functionality for resource management experiments.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\cpu-constraints-pod-4.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# The provided content is a Kubernetes configuration defining a headless Service and a StatefulSet for deploying nginx web servers. The Service exposes port 80 and is labeled for app identification, with clusterIP set to None to create a headless service that enables direct Pod communication and stable network identities. The StatefulSet manages two nginx Pods with parallel deployment, each linked to the Service and configured to run a specific nginx image. Each Pod mounts a persistent volume for storing web content, ensuring data persistence across Pod restarts. The volume claim template allocates 1 GiB of storage per Pod, enabling reliable and scalable web server deployment with stable network identities, ideal for stateful applications like web servers or databases.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx\\n  labels:\\n    app: nginx\\nspec:\\n  ports:\\n  - port: 80\\n    name: web\\n  clusterIP: None\\n  selector:\\n    app: nginx\\n---\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: web\\nspec:\\n  serviceName: \"nginx\"\\n  podManagementPolicy: \"Parallel\"\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - name: nginx\\n        image: registry.k8s.io/nginx-slim:0.24\\n        ports:\\n        - containerPort: 80\\n          name: web\\n        volumeMounts:\\n        - name: www\\n          mountPath: /usr/share/nginx/html\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: www\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      resources:\\n        requests:\\n          storage: 1Gi\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes configuration defining a headless Service and a StatefulSet for deploying nginx web servers. The Service exposes port 80 and is labeled for app identification, with clusterIP set to None to create a headless service that enables direct Pod communication and stable network identities. The StatefulSet manages two nginx Pods with parallel deployment, each linked to the Service and configured to run a specific nginx image. Each Pod mounts a persistent volume for storing web content, ensuring data persistence across Pod restarts. The volume claim template allocates 1 GiB of storage per Pod, enabling reliable and scalable web server deployment with stable network identities, ideal for stateful applications like web servers or databases.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\web\\\\web-parallel.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# This content defines a Kubernetes ValidatingAdmissionPolicy resource, which is used to enforce custom policies during resource creation or modification. The policy is named \"demo-policy.example.com\" and is configured with a failure policy set to \"Fail,\" meaning invalid requests will be rejected.\\n\\nThe policy applies specifically to create and update operations on \"deployments\" within the \"apps\" API group and version \"v1.\" It contains a validation rule that checks whether the number of replicas specified in the deployment exceeds 50. If a deployment\\'s replica count is greater than 50, a message is generated indicating the number of replicas set. Additionally, an audit annotation logs cases where the replica count is high, capturing the same information.\\n\\nIn essence, this policy ensures that no deployment can be created or updated with more than 50 replicas, maintaining control over resource scaling by validating input and providing detailed feedback and audit logs.\\napiVersion: admissionregistration.k8s.io/v1\\nkind: ValidatingAdmissionPolicy\\nmetadata:\\n  name: \"demo-policy.example.com\"\\nspec:\\n  failurePolicy: Fail\\n  matchConstraints:\\n    resourceRules:\\n    - apiGroups:   [\"apps\"]\\n      apiVersions: [\"v1\"]\\n      operations:  [\"CREATE\", \"UPDATE\"]\\n      resources:   [\"deployments\"]\\n  validations:\\n    - expression: \"object.spec.replicas > 50\"\\n      messageExpression: \"\\'Deployment spec.replicas set to \\' + string(object.spec.replicas)\"\\n  auditAnnotations:\\n    - key: \"high-replica-count\"\\n      valueExpression: \"\\'Deployment spec.replicas set to \\' + string(object.spec.replicas)\"\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes ValidatingAdmissionPolicy resource, which is used to enforce custom policies during resource creation or modification. The policy is named \"demo-policy.example.com\" and is configured with a failure policy set to \"Fail,\" meaning invalid requests will be rejected.\\n\\nThe policy applies specifically to create and update operations on \"deployments\" within the \"apps\" API group and version \"v1.\" It contains a validation rule that checks whether the number of replicas specified in the deployment exceeds 50. If a deployment\\'s replica count is greater than 50, a message is generated indicating the number of replicas set. Additionally, an audit annotation logs cases where the replica count is high, capturing the same information.\\n\\nIn essence, this policy ensures that no deployment can be created or updated with more than 50 replicas, maintaining control over resource scaling by validating input and providing detailed feedback and audit logs.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\validating-admission-policy-audit-annotation.yaml'},\n",
       "  0.72265625),\n",
       " ({'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"high-priority\" that runs an Ubuntu container with a simple looping command that outputs \"hello\" every 10 seconds. The container requests and limits are set to 10Gi of memory and 500 millicpus CPUs, ensuring resource allocation and constraints. Additionally, the pod is assigned to a priority class called \"high,\" which influences scheduling and preemption priorities within the Kubernetes cluster. This configuration is used to ensure that the pod has guaranteed resources and high scheduling priority in the cluster.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: high-priority\\nspec:\\n  containers:\\n  - name: high-priority\\n    image: ubuntu\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"10Gi\"\\n        cpu: \"500m\"\\n      limits:\\n        memory: \"10Gi\"\\n        cpu: \"500m\"\\n  priorityClassName: high\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a pod named \"high-priority\" that runs an Ubuntu container with a simple looping command that outputs \"hello\" every 10 seconds. The container requests and limits are set to 10Gi of memory and 500 millicpus CPUs, ensuring resource allocation and constraints. Additionally, the pod is assigned to a priority class called \"high,\" which influences scheduling and preemption priorities within the Kubernetes cluster. This configuration is used to ensure that the pod has guaranteed resources and high scheduling priority in the cluster.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\high-priority-pod.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.72265625),\n",
       " ({'content': '# This content is a Kubernetes YAML configuration file for defining a batch Job resource. It specifies a job named \"job-wq-2\" that runs with a parallelism of 2, meaning two pods will execute concurrently. The job runs containers based on the image stored in Google Container Registry (`gcr.io/myproject/job-wq-2`). The restart policy is set to \"OnFailure,\" which ensures that only failed pods are restarted, providing fault tolerance for the batch job. Overall, this configuration facilitates parallel execution of a containerized task in a Kubernetes environment, optimizing resource usage and job completion time.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-wq-2\\nspec:\\n  parallelism: 2\\n  template:\\n    metadata:\\n      name: job-wq-2\\n    spec:\\n      containers:\\n      - name: c\\n        image: gcr.io/myproject/job-wq-2\\n      restartPolicy: OnFailure\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\redis\\\\job.yaml',\n",
       "   'summary': 'This content is a Kubernetes YAML configuration file for defining a batch Job resource. It specifies a job named \"job-wq-2\" that runs with a parallelism of 2, meaning two pods will execute concurrently. The job runs containers based on the image stored in Google Container Registry (`gcr.io/myproject/job-wq-2`). The restart policy is set to \"OnFailure,\" which ensures that only failed pods are restarted, providing fault tolerance for the batch job. Overall, this configuration facilitates parallel execution of a containerized task in a Kubernetes environment, optimizing resource usage and job completion time.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.71875),\n",
       " ({'content': '# This content is a Kubernetes deployment configuration for a MongoDB instance. It uses the `apps/v1` API and defines a deployment named \"mongo\" with metadata labels for identification. The deployment specifies a single replica of the MongoDB container, which uses the official `mongo:4.2` image. The container is configured to bind to all network interfaces (`--bind_ip 0.0.0.0`) to allow external connections. Resource requests are set low with 100 millicores of CPU and 100Mi of memory to ensure minimal resource consumption. The container exposes port 27017, which is the default port for MongoDB, allowing other services to connect to the database. Overall, this configuration automates the deployment and management of a MongoDB server in a Kubernetes environment.\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: mongo\\n  labels:\\n    app.kubernetes.io/name: mongo\\n    app.kubernetes.io/component: backend\\nspec:\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: mongo\\n      app.kubernetes.io/component: backend\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: mongo\\n        app.kubernetes.io/component: backend\\n    spec:\\n      containers:\\n      - name: mongo\\n        image: mongo:4.2\\n        args:\\n          - --bind_ip\\n          - 0.0.0.0\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n        ports:\\n        - containerPort: 27017\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes deployment configuration for a MongoDB instance. It uses the `apps/v1` API and defines a deployment named \"mongo\" with metadata labels for identification. The deployment specifies a single replica of the MongoDB container, which uses the official `mongo:4.2` image. The container is configured to bind to all network interfaces (`--bind_ip 0.0.0.0`) to allow external connections. Resource requests are set low with 100 millicores of CPU and 100Mi of memory to ensure minimal resource consumption. The container exposes port 27017, which is the default port for MongoDB, allowing other services to connect to the database. Overall, this configuration automates the deployment and management of a MongoDB server in a Kubernetes environment.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mongodb\\\\mongo-deployment.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Pod named \"nginx\" with specific scheduling preferences. It includes a node affinity rule that prefers nodes with a label key \"disktype\" set to \"ssd,\" indicating a preference for deploying the pod on faster storage devices. The pod contains a single container running the Nginx image, with an image pull policy set to fetch the image only if it is not already present locally. This setup helps optimize pod placement for better performance by leveraging node labels and ensures efficient image management.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: nginx\\nspec:\\n  affinity:\\n    nodeAffinity:\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 1\\n        preference:\\n          matchExpressions:\\n          - key: disktype\\n            operator: In\\n            values:\\n            - ssd          \\n  containers:\\n  - name: nginx\\n    image: nginx\\n    imagePullPolicy: IfNotPresent\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Pod named \"nginx\" with specific scheduling preferences. It includes a node affinity rule that prefers nodes with a label key \"disktype\" set to \"ssd,\" indicating a preference for deploying the pod on faster storage devices. The pod contains a single container running the Nginx image, with an image pull policy set to fetch the image only if it is not already present locally. This setup helps optimize pod placement for better performance by leveraging node labels and ensures efficient image management.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-nginx-preferred-affinity.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes Pod with two containers, sharing data through a volume. It specifies a Pod with a \"shared-data\" volume of type `emptyDir`, which creates an ephemeral shared storage space accessible by both containers. The first container runs an Nginx server and mounts the shared volume at `/usr/share/nginx/html`, allowing it to serve static files. The second container runs Debian and mounts the same volume at `/pod-data`. It executes a shell command that writes a simple message, \"Hello from the debian container,\" into an `index.html` file within the shared volume. This setup enables the Debian container to create content dynamically, which the Nginx container then serves, demonstrating shared storage and inter-container communication within a Kubernetes Pod.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: two-containers\\nspec:\\n\\n  restartPolicy: Never\\n\\n  volumes:\\n  - name: shared-data\\n    emptyDir: {}\\n\\n  containers:\\n\\n  - name: nginx-container\\n    image: nginx\\n    volumeMounts:\\n    - name: shared-data\\n      mountPath: /usr/share/nginx/html\\n\\n  - name: debian-container\\n    image: debian\\n    volumeMounts:\\n    - name: shared-data\\n      mountPath: /pod-data\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"echo Hello from the debian container > /pod-data/index.html\"]\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided YAML configuration defines a Kubernetes Pod with two containers, sharing data through a volume. It specifies a Pod with a \"shared-data\" volume of type `emptyDir`, which creates an ephemeral shared storage space accessible by both containers. The first container runs an Nginx server and mounts the shared volume at `/usr/share/nginx/html`, allowing it to serve static files. The second container runs Debian and mounts the same volume at `/pod-data`. It executes a shell command that writes a simple message, \"Hello from the debian container,\" into an `index.html` file within the shared volume. This setup enables the Debian container to create content dynamically, which the Nginx container then serves, demonstrating shared storage and inter-container communication within a Kubernetes Pod.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\two-container-pod.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# This content defines a Kubernetes Namespace resource configuration in YAML format. It creates a namespace named \"my-baseline-namespace\" and applies specific security labels related to pod security standards. These labels enforce or warn about security policies at the baseline level, using the latest version of standards. The labels help ensure that Pods within this namespace adhere to predefined security policies, promoting a consistent security baseline across the environment.\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: my-baseline-namespace\\n  labels:\\n    pod-security.kubernetes.io/enforce: baseline\\n    pod-security.kubernetes.io/enforce-version: latest\\n    pod-security.kubernetes.io/warn: baseline\\n    pod-security.kubernetes.io/warn-version: latest',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes Namespace resource configuration in YAML format. It creates a namespace named \"my-baseline-namespace\" and applies specific security labels related to pod security standards. These labels enforce or warn about security policies at the baseline level, using the latest version of standards. The labels help ensure that Pods within this namespace adhere to predefined security policies, promoting a consistent security baseline across the environment.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\security\\\\podsecurity-baseline.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# This content is a Kubernetes Service configuration in YAML format, used to define how applications are exposed within a cluster or externally. The service named \"my-service\" is configured to use a LoadBalancer type, which allows external access, and it prefers dual-stack networking with support for IPv6. The selector ensures that the service targets pods labeled with \"app.kubernetes.io/name: MyApp\". The service listens on TCP port 80, which is typically used for HTTP traffic. Overall, this setup facilitates external connectivity to the application (MyApp) via a load balancer, with support for both IPv4 and IPv6 addressing.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-service\\n  labels:\\n    app.kubernetes.io/name: MyApp\\nspec:\\n  ipFamilyPolicy: PreferDualStack\\n  ipFamilies:\\n  - IPv6\\n  type: LoadBalancer\\n  selector:\\n    app.kubernetes.io/name: MyApp\\n  ports:\\n    - protocol: TCP\\n      port: 80\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content is a Kubernetes Service configuration in YAML format, used to define how applications are exposed within a cluster or externally. The service named \"my-service\" is configured to use a LoadBalancer type, which allows external access, and it prefers dual-stack networking with support for IPv6. The selector ensures that the service targets pods labeled with \"app.kubernetes.io/name: MyApp\". The service listens on TCP port 80, which is typically used for HTTP traffic. Overall, this setup facilitates external connectivity to the application (MyApp) via a load balancer, with support for both IPv4 and IPv6 addressing.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\dual-stack-prefer-ipv6-lb-svc.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# The provided content is a Kubernetes configuration defining a set of resources for deploying a WordPress application with a MySQL database. It includes a Service, a PersistentVolumeClaim, and a Deployment. The Service exposes the MySQL database on port 3306 without a specific cluster IP (headless service), enabling direct access from other pods. The PersistentVolumeClaim requests 20Gi of storage for persistent data storage, ensuring data durability across pod restarts. The Deployment manages the MySQL pod, specifying environment variables for database credentials, which are securely referenced from a Secret. It uses the MySQL 8.0 Docker image and mounts the persistent storage volume at `/var/lib/mysql` to retain database data. Overall, this configuration automates the deployment, persistent storage, and networking setup for a MySQL database in a Kubernetes cluster, serving as a backend for a WordPress application.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: wordpress-mysql\\n  labels:\\n    app: wordpress\\nspec:\\n  ports:\\n    - port: 3306\\n  selector:\\n    app: wordpress\\n    tier: mysql\\n  clusterIP: None\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mysql-pv-claim\\n  labels:\\n    app: wordpress\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 20Gi\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: wordpress-mysql\\n  labels:\\n    app: wordpress\\nspec:\\n  selector:\\n    matchLabels:\\n      app: wordpress\\n      tier: mysql\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      labels:\\n        app: wordpress\\n        tier: mysql\\n    spec:\\n      containers:\\n      - image: mysql:8.0\\n        name: mysql\\n        env:\\n        - name: MYSQL_ROOT_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: mysql-pass\\n              key: password\\n        - name: MYSQL_DATABASE\\n          value: wordpress\\n        - name: MYSQL_USER\\n          value: wordpress\\n        - name: MYSQL_PASSWORD\\n          valueFrom:\\n            secretKeyRef:\\n              name: mysql-pass\\n              key: password\\n        ports:\\n        - containerPort: 3306\\n          name: mysql\\n        volumeMounts:\\n        - name: mysql-persistent-storage\\n          mountPath: /var/lib/mysql\\n      volumes:\\n      - name: mysql-persistent-storage\\n        persistentVolumeClaim:\\n          claimName: mysql-pv-claim\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes configuration defining a set of resources for deploying a WordPress application with a MySQL database. It includes a Service, a PersistentVolumeClaim, and a Deployment. The Service exposes the MySQL database on port 3306 without a specific cluster IP (headless service), enabling direct access from other pods. The PersistentVolumeClaim requests 20Gi of storage for persistent data storage, ensuring data durability across pod restarts. The Deployment manages the MySQL pod, specifying environment variables for database credentials, which are securely referenced from a Secret. It uses the MySQL 8.0 Docker image and mounts the persistent storage volume at `/var/lib/mysql` to retain database data. Overall, this configuration automates the deployment, persistent storage, and networking setup for a MySQL database in a Kubernetes cluster, serving as a backend for a WordPress application.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\wordpress\\\\mysql-deployment.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# The provided content describes a Kubernetes configuration comprising two main resources: a Service and a Deployment. The Service, named \"my-nginx,\" exposes the application on two ports—8080 (mapped to container port 80) and 443—using the NodePort type, which allows external traffic to access the service via the node’s IP and specified port. It labels the service for identification and uses selectors to route traffic to the correct pods.\\n\\nThe Deployment, also named \"my-nginx,\" manages the lifecycle of the nginx application. It specifies one replica, meaning a single pod instance will run the containerized nginx server. The pod template within the Deployment defines volume mounts for secrets and configuration data: a secret volume named \"secret-volume\" (likely containing SSL certificates) and a ConfigMap volume named \"configmap-volume\" (containing nginx configuration). The container uses an image \"bprashanth/nginxhttps:1.0\" that likely supports HTTPS, and it exposes ports 80 and 443 within the container, mounting the secret and config map at paths relevant for SSL certificates and nginx configuration files, respectively. This setup enables a secure nginx server with external access and containerized configuration management.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: my-nginx\\n  labels:\\n    run: my-nginx\\nspec:\\n  type: NodePort\\n  ports:\\n  - port: 8080\\n    targetPort: 80\\n    protocol: TCP\\n    name: http\\n  - port: 443\\n    protocol: TCP\\n    name: https\\n  selector:\\n    run: my-nginx\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: my-nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      run: my-nginx\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        run: my-nginx\\n    spec:\\n      volumes:\\n      - name: secret-volume\\n        secret:\\n          secretName: nginxsecret\\n      - name: configmap-volume\\n        configMap:\\n          name: nginxconfigmap\\n      containers:\\n      - name: nginxhttps\\n        image: bprashanth/nginxhttps:1.0\\n        ports:\\n        - containerPort: 443\\n        - containerPort: 80\\n        volumeMounts:\\n        - mountPath: /etc/nginx/ssl\\n          name: secret-volume\\n        - mountPath: /etc/nginx/conf.d\\n          name: configmap-volume\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'The provided content describes a Kubernetes configuration comprising two main resources: a Service and a Deployment. The Service, named \"my-nginx,\" exposes the application on two ports—8080 (mapped to container port 80) and 443—using the NodePort type, which allows external traffic to access the service via the node’s IP and specified port. It labels the service for identification and uses selectors to route traffic to the correct pods.\\n\\nThe Deployment, also named \"my-nginx,\" manages the lifecycle of the nginx application. It specifies one replica, meaning a single pod instance will run the containerized nginx server. The pod template within the Deployment defines volume mounts for secrets and configuration data: a secret volume named \"secret-volume\" (likely containing SSL certificates) and a ConfigMap volume named \"configmap-volume\" (containing nginx configuration). The container uses an image \"bprashanth/nginxhttps:1.0\" that likely supports HTTPS, and it exposes ports 80 and 443 within the container, mounting the secret and config map at paths relevant for SSL certificates and nginx configuration files, respectively. This setup enables a secure nginx server with external access and containerized configuration management.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\service\\\\networking\\\\nginx-secure-app.yaml'},\n",
       "  0.71875),\n",
       " ({'content': '# This YAML content defines multiple Kubernetes ResourceQuotas, each targeted at pods with different priority classes: high, medium, and low. Each ResourceQuota specifies resource limits for CPU, memory, and the number of pods, and uses scope selectors to associate quotas with specific PriorityClasses. The high priority quota allows the most resources, while the low priority quota limits resources for lower-priority pods. This setup helps manage and enforce resource distribution based on pod importance, ensuring critical workloads have sufficient resources while preventing resource exhaustion by lower-priority pods.\\napiVersion: v1\\nkind: List\\nitems:\\n- apiVersion: v1\\n  kind: ResourceQuota\\n  metadata:\\n    name: pods-high\\n  spec:\\n    hard:\\n      cpu: \"1000\"\\n      memory: \"200Gi\"\\n      pods: \"10\"\\n    scopeSelector:\\n      matchExpressions:\\n      - operator: In\\n        scopeName: PriorityClass\\n        values: [\"high\"]\\n- apiVersion: v1\\n  kind: ResourceQuota\\n  metadata:\\n    name: pods-medium\\n  spec:\\n    hard:\\n      cpu: \"10\"\\n      memory: \"20Gi\"\\n      pods: \"10\"\\n    scopeSelector:\\n      matchExpressions:\\n      - operator: In\\n        scopeName: PriorityClass\\n        values: [\"medium\"]\\n- apiVersion: v1\\n  kind: ResourceQuota\\n  metadata:\\n    name: pods-low\\n  spec:\\n    hard:\\n      cpu: \"5\"\\n      memory: \"10Gi\"\\n      pods: \"10\"\\n    scopeSelector:\\n      matchExpressions:\\n      - operator: In\\n        scopeName: PriorityClass\\n        values: [\"low\"]\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML content defines multiple Kubernetes ResourceQuotas, each targeted at pods with different priority classes: high, medium, and low. Each ResourceQuota specifies resource limits for CPU, memory, and the number of pods, and uses scope selectors to associate quotas with specific PriorityClasses. The high priority quota allows the most resources, while the low priority quota limits resources for lower-priority pods. This setup helps manage and enforce resource distribution based on pod importance, ensuring critical workloads have sufficient resources while preventing resource exhaustion by lower-priority pods.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\policy\\\\quota.yaml'},\n",
       "  0.71484375),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes Pod with specific affinity rules that influence pod scheduling based on existing pod placements and labels. The file includes both pod affinity and anti-affinity settings. \\n\\nPod affinity rules specify that the pod should be scheduled on a zone (`topology.kubernetes.io/zone`) where there are already pods with the label `security: S1`, ensuring proximity to similar pods. Conversely, pod anti-affinity rules express a preference (not a strict requirement) for avoiding zones with pods labeled `security: S2`, by assigning a high weight (100) to this preference, encouraging the scheduler to avoid placing this pod in such zones if possible. \\n\\nThe container runs a minimal pause image, serving as a placeholder or test container. Overall, this configuration aims to control pod placement by favoring certain zones based on labels, optimizing for proximity or segregation as needed.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: with-pod-affinity\\nspec:\\n  affinity:\\n    podAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n      - labelSelector:\\n          matchExpressions:\\n          - key: security\\n            operator: In\\n            values:\\n            - S1\\n        topologyKey: topology.kubernetes.io/zone\\n    podAntiAffinity:\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 100\\n        podAffinityTerm:\\n          labelSelector:\\n            matchExpressions:\\n            - key: security\\n              operator: In\\n              values:\\n              - S2\\n          topologyKey: topology.kubernetes.io/zone\\n  containers:\\n  - name: with-pod-affinity\\n    image: registry.k8s.io/pause:3.8\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided YAML configuration defines a Kubernetes Pod with specific affinity rules that influence pod scheduling based on existing pod placements and labels. The file includes both pod affinity and anti-affinity settings. \\n\\nPod affinity rules specify that the pod should be scheduled on a zone (`topology.kubernetes.io/zone`) where there are already pods with the label `security: S1`, ensuring proximity to similar pods. Conversely, pod anti-affinity rules express a preference (not a strict requirement) for avoiding zones with pods labeled `security: S2`, by assigning a high weight (100) to this preference, encouraging the scheduler to avoid placing this pod in such zones if possible. \\n\\nThe container runs a minimal pause image, serving as a placeholder or test container. Overall, this configuration aims to control pod placement by favoring certain zones based on labels, optimizing for proximity or segregation as needed.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\pod-with-pod-affinity.yaml'},\n",
       "  0.71484375),\n",
       " ({'content': '# This content describes a Kubernetes Job configuration written in YAML. The job runs a container using the Perl 5.34.0 image to calculate the value of Pi to 2000 digits using the BigNum library. The command executed within the container invokes Perl with specific modules and options to perform the calculation. The configuration ensures that the job does not restart upon completion or failure, with a maximum retry limit set to four attempts. Overall, this setup automates the process of performing a complex mathematical calculation within a containerized environment.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: pi\\nspec:\\n  template:\\n    spec:\\n      containers:\\n      - name: pi\\n        image: perl:5.34.0\\n        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\\n      restartPolicy: Never\\n  backoffLimit: 4\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job.yaml',\n",
       "   'summary': 'This content describes a Kubernetes Job configuration written in YAML. The job runs a container using the Perl 5.34.0 image to calculate the value of Pi to 2000 digits using the BigNum library. The command executed within the container invokes Perl with specific modules and options to perform the calculation. The configuration ensures that the job does not restart upon completion or failure, with a maximum retry limit set to four attempts. Overall, this setup automates the process of performing a complex mathematical calculation within a containerized environment.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.71484375),\n",
       " ({'content': '# This content provides a Kubernetes Pod configuration written in YAML, defining a single pod named \"busybox1\" with four containers. Each container uses the BusyBox image (version 1.28) and runs an infinite loop that echoes a message every 10 seconds, demonstrating simple, continuous shell commands within containers.\\n\\nThe configuration specifies resource requests and limits for some containers, for example, \"busybox-cnt01\" requests 100Mi of memory and 100m of CPU, with limits of 200Mi and 500m respectively. \"busybox-cnt02\" only has resource requests defined, while \"busybox-cnt03\" has limits set similarly to \"cnt01\". The other container, \"busybox-cnt04\", does not specify resource constraints. Overall, the configuration demonstrates how to run multiple containers within a single pod, control their resource usage, and execute simple shell commands remotely.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: busybox1\\nspec:\\n  containers:\\n  - name: busybox-cnt01\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt01; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt02\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt02; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n  - name: busybox-cnt03\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt03; sleep 10;done\"]\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt04\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt04; sleep 10;done\"]\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content provides a Kubernetes Pod configuration written in YAML, defining a single pod named \"busybox1\" with four containers. Each container uses the BusyBox image (version 1.28) and runs an infinite loop that echoes a message every 10 seconds, demonstrating simple, continuous shell commands within containers.\\n\\nThe configuration specifies resource requests and limits for some containers, for example, \"busybox-cnt01\" requests 100Mi of memory and 100m of CPU, with limits of 200Mi and 500m respectively. \"busybox-cnt02\" only has resource requests defined, while \"busybox-cnt03\" has limits set similarly to \"cnt01\". The other container, \"busybox-cnt04\", does not specify resource constraints. Overall, the configuration demonstrates how to run multiple containers within a single pod, control their resource usage, and execute simple shell commands remotely.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-1.yaml'},\n",
       "  0.71484375),\n",
       " ({'content': '# The provided content is a Kubernetes Pod configuration file in YAML format, defining a pod named \"busybox2\" with four containers, all based on the busybox:1.28 image. Each container runs an infinite shell loop, periodically echoing a message specific to its identifier (cnt01, cnt02, cnt03, cnt04) every 10 seconds.\\n\\nThe configuration specifies resource requests and limits for some containers to manage their CPU and memory consumption: for example, cnt01 requests 100Mi of memory and 100m of CPU, with limits set to 200Mi and 500m respectively. Notably, cnt02 lacks resource limits, and cnt04 does not specify resource requests or limits. This setup helps control resource utilization and ensures predictable performance within a shared environment.\\n\\nOverall, this YAML demonstrates a basic use case of defining multiple containers within a single pod, each running simple, continuous tasks with resource constraints applied where specified.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: busybox2\\nspec:\\n  containers:\\n  - name: busybox-cnt01\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt01; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt02\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt02; sleep 10;done\"]\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n        cpu: \"100m\"\\n  - name: busybox-cnt03\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt03; sleep 10;done\"]\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n        cpu: \"500m\"\\n  - name: busybox-cnt04\\n    image: busybox:1.28\\n    command: [\"/bin/sh\"]\\n    args: [\"-c\", \"while true; do echo hello from cnt04; sleep 10;done\"]\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes Pod configuration file in YAML format, defining a pod named \"busybox2\" with four containers, all based on the busybox:1.28 image. Each container runs an infinite shell loop, periodically echoing a message specific to its identifier (cnt01, cnt02, cnt03, cnt04) every 10 seconds.\\n\\nThe configuration specifies resource requests and limits for some containers to manage their CPU and memory consumption: for example, cnt01 requests 100Mi of memory and 100m of CPU, with limits set to 200Mi and 500m respectively. Notably, cnt02 lacks resource limits, and cnt04 does not specify resource requests or limits. This setup helps control resource utilization and ensures predictable performance within a shared environment.\\n\\nOverall, this YAML demonstrates a basic use case of defining multiple containers within a single pod, each running simple, continuous tasks with resource constraints applied where specified.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\limit-range-pod-2.yaml'},\n",
       "  0.71484375),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes Pod that demonstrates the use of the Downward API to expose container resource requests and limits as files within a volume. The Pod contains a single container running BusyBox, which executes a shell script in a continuous loop. This script reads and prints resource information—such as CPU and memory requests and limits—from specific files mounted at `/etc/podinfo`. The volume `podinfo` uses the Downward API to expose container resource constraints by creating files for each resource attribute, referencing the container\\'s resource requests and limits, and dividing the values appropriately for human-readable units.\\n\\nThe code\\'s core functionality involves mounting these resource attribute files into the container, then repeatedly reading and displaying their contents every 5 seconds. This allows dynamic introspection of resource configurations at runtime, which is useful for debugging, monitoring, or ensuring containers adhere to specified resource quotas. The configuration effectively links container resource specifications with accessible runtime data, enabling in-container introspection and validation of resource allocations.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubernetes-downwardapi-volume-example-2\\nspec:\\n  containers:\\n    - name: client-container\\n      image: registry.k8s.io/busybox:1.27.2\\n      command: [\"sh\", \"-c\"]\\n      args:\\n      - while true; do\\n          echo -en \\'\\\\n\\';\\n          if [[ -e /etc/podinfo/cpu_limit ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/cpu_limit; fi;\\n          if [[ -e /etc/podinfo/cpu_request ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/cpu_request; fi;\\n          if [[ -e /etc/podinfo/mem_limit ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/mem_limit; fi;\\n          if [[ -e /etc/podinfo/mem_request ]]; then\\n            echo -en \\'\\\\n\\'; cat /etc/podinfo/mem_request; fi;\\n          sleep 5;\\n        done;\\n      resources:\\n        requests:\\n          memory: \"32Mi\"\\n          cpu: \"125m\"\\n        limits:\\n          memory: \"64Mi\"\\n          cpu: \"250m\"\\n      volumeMounts:\\n        - name: podinfo\\n          mountPath: /etc/podinfo\\n  volumes:\\n    - name: podinfo\\n      downwardAPI:\\n        items:\\n          - path: \"cpu_limit\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: limits.cpu\\n              divisor: 1m\\n          - path: \"cpu_request\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: requests.cpu\\n              divisor: 1m\\n          - path: \"mem_limit\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: limits.memory\\n              divisor: 1Mi\\n          - path: \"mem_request\"\\n            resourceFieldRef:\\n              containerName: client-container\\n              resource: requests.memory\\n              divisor: 1Mi\\n\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': \"The provided YAML configuration defines a Kubernetes Pod that demonstrates the use of the Downward API to expose container resource requests and limits as files within a volume. The Pod contains a single container running BusyBox, which executes a shell script in a continuous loop. This script reads and prints resource information—such as CPU and memory requests and limits—from specific files mounted at `/etc/podinfo`. The volume `podinfo` uses the Downward API to expose container resource constraints by creating files for each resource attribute, referencing the container's resource requests and limits, and dividing the values appropriately for human-readable units.\\n\\nThe code's core functionality involves mounting these resource attribute files into the container, then repeatedly reading and displaying their contents every 5 seconds. This allows dynamic introspection of resource configurations at runtime, which is useful for debugging, monitoring, or ensuring containers adhere to specified resource quotas. The configuration effectively links container resource specifications with accessible runtime data, enabling in-container introspection and validation of resource allocations.\",\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\inject\\\\dapi-volume-resources.yaml'},\n",
       "  0.7109375),\n",
       " ({'content': '# This YAML configuration defines a LimitRange resource in a Kubernetes cluster. It sets constraints on PersistentVolumeClaims (PVCs), specifying that each PVC must have a minimum storage of 1Gi and a maximum storage of 2Gi. This ensures that users or applications requesting storage through PVCs adhere to these size limits, helping manage resource utilization and prevent over-provisioning or under-provisioning of storage. The configuration helps enforce storage policies at the namespace level within the cluster.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: storagelimits\\nspec:\\n  limits:\\n  - type: PersistentVolumeClaim\\n    max:\\n      storage: 2Gi\\n    min:\\n      storage: 1Gi\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a LimitRange resource in a Kubernetes cluster. It sets constraints on PersistentVolumeClaims (PVCs), specifying that each PVC must have a minimum storage of 1Gi and a maximum storage of 2Gi. This ensures that users or applications requesting storage through PVCs adhere to these size limits, helping manage resource utilization and prevent over-provisioning or under-provisioning of storage. The configuration helps enforce storage policies at the namespace level within the cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\resource\\\\storagelimits.yaml'},\n",
       "  0.7109375),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes StatefulSet for deploying a MySQL database cluster with three replicas, suited for high availability and replication setup. It leverages init containers and sidecar containers to initialize and clone data appropriately, ensuring each node has a unique server ID and proper replication configuration.\\n\\nThe init containers perform critical setup tasks: one generates a server ID based on the pod\\'s ordinal index, avoiding conflicts, and copies configuration files based on whether the pod is primary or a replica. The second init container, using the `xtrabackup` image, clones data from the preceding replica (except for the primary), streamlining the initialization process. The main MySQL container runs the server with resource requests and probes for liveness and readiness, ensuring the service stays healthy. Additionally, an `xtrabackup` sidecar handles incremental backups, manages replication positions based on binary log info, and waits for the MySQL server to be ready before starting replication.\\n\\nThe code efficiently sets up a self-healing, scalable MySQL environment with automated data cloning and replication, using multiple containers and configuration management tools to implement robust database clustering in Kubernetes.\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: mysql\\nspec:\\n  selector:\\n    matchLabels:\\n      app: mysql\\n      app.kubernetes.io/name: mysql\\n  serviceName: mysql\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: mysql\\n        app.kubernetes.io/name: mysql\\n    spec:\\n      initContainers:\\n      - name: init-mysql\\n        image: mysql:5.7\\n        command:\\n        - bash\\n        - \"-c\"\\n        - |\\n          set -ex\\n          # Generate mysql server-id from pod ordinal index.\\n          [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1\\n          ordinal=${BASH_REMATCH[1]}\\n          echo [mysqld] > /mnt/conf.d/server-id.cnf\\n          # Add an offset to avoid reserved server-id=0 value.\\n          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\\n          # Copy appropriate conf.d files from config-map to emptyDir.\\n          if [[ $ordinal -eq 0 ]]; then\\n            cp /mnt/config-map/primary.cnf /mnt/conf.d/\\n          else\\n            cp /mnt/config-map/replica.cnf /mnt/conf.d/\\n          fi\\n        volumeMounts:\\n        - name: conf\\n          mountPath: /mnt/conf.d\\n        - name: config-map\\n          mountPath: /mnt/config-map\\n      - name: clone-mysql\\n        image: gcr.io/google-samples/xtrabackup:1.0\\n        command:\\n        - bash\\n        - \"-c\"\\n        - |\\n          set -ex\\n          # Skip the clone if data already exists.\\n          [[ -d /var/lib/mysql/mysql ]] && exit 0\\n          # Skip the clone on primary (ordinal index 0).\\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\\n          ordinal=${BASH_REMATCH[1]}\\n          [[ $ordinal -eq 0 ]] && exit 0\\n          # Clone data from previous peer.\\n          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\\n          # Prepare the backup.\\n          xtrabackup --prepare --target-dir=/var/lib/mysql\\n        volumeMounts:\\n        - name: data\\n          mountPath: /var/lib/mysql\\n          subPath: mysql\\n        - name: conf\\n          mountPath: /etc/mysql/conf.d\\n      containers:\\n      - name: mysql\\n        image: mysql:5.7\\n        env:\\n        - name: MYSQL_ALLOW_EMPTY_PASSWORD\\n          value: \"1\"\\n        ports:\\n        - name: mysql\\n          containerPort: 3306\\n        volumeMounts:\\n        - name: data\\n          mountPath: /var/lib/mysql\\n          subPath: mysql\\n        - name: conf\\n          mountPath: /etc/mysql/conf.d\\n        resources:\\n          requests:\\n            cpu: 500m\\n            memory: 1Gi\\n        livenessProbe:\\n          exec:\\n            command: [\"mysqladmin\", \"ping\"]\\n          initialDelaySeconds: 30\\n          periodSeconds: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          exec:\\n            # Check we can execute queries over TCP (skip-networking is off).\\n            command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"]\\n          initialDelaySeconds: 5\\n          periodSeconds: 2\\n          timeoutSeconds: 1\\n      - name: xtrabackup\\n        image: gcr.io/google-samples/xtrabackup:1.0\\n        ports:\\n        - name: xtrabackup\\n          containerPort: 3307\\n        command:\\n        - bash\\n        - \"-c\"\\n        - |\\n          set -ex\\n          cd /var/lib/mysql\\n\\n          # Determine binlog position of cloned data, if any.\\n          if [[ -f xtrabackup_slave_info && \"x$(<xtrabackup_slave_info)\" != \"x\" ]]; then\\n            # XtraBackup already generated a partial \"CHANGE MASTER TO\" query\\n            # because we\\'re cloning from an existing replica. (Need to remove the tailing semicolon!)\\n            cat xtrabackup_slave_info | sed -E \\'s/;$//g\\' > change_master_to.sql.in\\n            # Ignore xtrabackup_binlog_info in this case (it\\'s useless).\\n            rm -f xtrabackup_slave_info xtrabackup_binlog_info\\n          elif [[ -f xtrabackup_binlog_info ]]; then\\n            # We\\'re cloning directly from primary. Parse binlog position.\\n            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\\n            rm -f xtrabackup_binlog_info xtrabackup_slave_info\\n            echo \"CHANGE MASTER TO MASTER_LOG_FILE=\\'${BASH_REMATCH[1]}\\',\\\\\\n                  MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in\\n          fi\\n\\n          # Check if we need to complete a clone by starting replication.\\n          if [[ -f change_master_to.sql.in ]]; then\\n            echo \"Waiting for mysqld to be ready (accepting connections)\"\\n            until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done\\n\\n            echo \"Initializing replication from clone position\"\\n            mysql -h 127.0.0.1 \\\\\\n                  -e \"$(<change_master_to.sql.in), \\\\\\n                          MASTER_HOST=\\'mysql-0.mysql\\', \\\\\\n                          MASTER_USER=\\'root\\', \\\\\\n                          MASTER_PASSWORD=\\'\\', \\\\\\n                          MASTER_CONNECT_RETRY=10; \\\\\\n                        START SLAVE;\" || exit 1\\n            # In case of container restart, attempt this at-most-once.\\n            mv change_master_to.sql.in change_master_to.sql.orig\\n          fi\\n\\n          # Start a server to send backups when requested by peers.\\n          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\\\\\n            \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\"\\n        volumeMounts:\\n        - name: data\\n          mountPath: /var/lib/mysql\\n          subPath: mysql\\n        - name: conf\\n          mountPath: /etc/mysql/conf.d\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 100Mi\\n      volumes:\\n      - name: conf\\n        emptyDir: {}\\n      - name: config-map\\n        configMap:\\n          name: mysql\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: data\\n    spec:\\n      accessModes: [\"ReadWriteOnce\"]\\n      resources:\\n        requests:\\n          storage: 10Gi\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': \"The provided YAML configuration defines a Kubernetes StatefulSet for deploying a MySQL database cluster with three replicas, suited for high availability and replication setup. It leverages init containers and sidecar containers to initialize and clone data appropriately, ensuring each node has a unique server ID and proper replication configuration.\\n\\nThe init containers perform critical setup tasks: one generates a server ID based on the pod's ordinal index, avoiding conflicts, and copies configuration files based on whether the pod is primary or a replica. The second init container, using the `xtrabackup` image, clones data from the preceding replica (except for the primary), streamlining the initialization process. The main MySQL container runs the server with resource requests and probes for liveness and readiness, ensuring the service stays healthy. Additionally, an `xtrabackup` sidecar handles incremental backups, manages replication positions based on binary log info, and waits for the MySQL server to be ready before starting replication.\\n\\nThe code efficiently sets up a self-healing, scalable MySQL environment with automated data cloning and replication, using multiple containers and configuration management tools to implement robust database clustering in Kubernetes.\",\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\mysql\\\\mysql-statefulset.yaml'},\n",
       "  0.7109375),\n",
       " ({'content': '# The provided content is a Kubernetes service configuration written in YAML. It defines a service named \"frontend\" that targets pods with specific labels (app: guestbook, tier: frontend). The service exposes port 80 to handle incoming network traffic. The configuration hints at the possibility of exposing the service externally by uncommenting the \"type: LoadBalancer\" line, which would provision an external load-balanced IP address if supported by the cluster. \\n\\nThis setup is part of a tutorial for deploying a Guestbook application on Google Kubernetes Engine, illustrating how to create a service that routes traffic to frontend pods. The YAML configuration automates the process of load balancing and service discovery within a Kubernetes cluster, ensuring that application components can communicate efficiently and are accessible over the network.\\n# SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: frontend\\n  labels:\\n    app: guestbook\\n    tier: frontend\\nspec:\\n  # if your cluster supports it, uncomment the following to automatically create\\n  # an external load-balanced IP for the frontend service.\\n  # type: LoadBalancer\\n  #type: LoadBalancer\\n  ports:\\n    # the port that this service should serve on\\n  - port: 80\\n  selector:\\n    app: guestbook\\n    tier: frontend',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes service configuration written in YAML. It defines a service named \"frontend\" that targets pods with specific labels (app: guestbook, tier: frontend). The service exposes port 80 to handle incoming network traffic. The configuration hints at the possibility of exposing the service externally by uncommenting the \"type: LoadBalancer\" line, which would provision an external load-balanced IP address if supported by the cluster. \\n\\nThis setup is part of a tutorial for deploying a Guestbook application on Google Kubernetes Engine, illustrating how to create a service that routes traffic to frontend pods. The YAML configuration automates the process of load balancing and service discovery within a Kubernetes cluster, ensuring that application components can communicate efficiently and are accessible over the network.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\guestbook\\\\frontend-service.yaml'},\n",
       "  0.7109375),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes Job that executes a simple task using an Alpine Linux container. The main container, named \"myjob,\" runs a command to write the text \"logging\" into a file located at /opt/logs.txt. It mounts a shared volume at /opt to enable data persistence and sharing with an init container. The init container, \"logshipper,\" also based on Alpine, continuously tails the logs file using the command `tail -F /opt/logs.txt`, effectively monitoring new entries in real-time. Both containers share an ephemeral empty directory volume, which provides temporary storage accessible during the job\\'s execution. The overall design allows for initial log processing or monitoring during the job execution, with the main container performing a logging task and the init container handling real-time log tailing before the main container completes.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: myjob\\nspec:\\n  template:\\n    spec:\\n      containers:\\n        - name: myjob\\n          image: alpine:latest\\n          command: [\\'sh\\', \\'-c\\', \\'echo \"logging\" > /opt/logs.txt\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      initContainers:\\n        - name: logshipper\\n          image: alpine:latest\\n          restartPolicy: Always\\n          command: [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\']\\n          volumeMounts:\\n            - name: data\\n              mountPath: /opt\\n      restartPolicy: Never\\n      volumes:\\n        - name: data\\n          emptyDir: {}',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided YAML configuration defines a Kubernetes Job that executes a simple task using an Alpine Linux container. The main container, named \"myjob,\" runs a command to write the text \"logging\" into a file located at /opt/logs.txt. It mounts a shared volume at /opt to enable data persistence and sharing with an init container. The init container, \"logshipper,\" also based on Alpine, continuously tails the logs file using the command `tail -F /opt/logs.txt`, effectively monitoring new entries in real-time. Both containers share an ephemeral empty directory volume, which provides temporary storage accessible during the job\\'s execution. The overall design allows for initial log processing or monitoring during the job execution, with the main container performing a logging task and the init container handling real-time log tailing before the main container completes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\job-sidecar.yaml'},\n",
       "  0.7109375),\n",
       " ({'content': '# This content defines a Kubernetes Pod configuration in YAML format, focusing on security contexts. It specifies a Pod named \"security-context-demo\" with a global security context that sets user and group IDs (`runAsUser: 1000`, `runAsGroup: 3000`) and additional groups (`supplementalGroups: [4000]`) with a strict policy. The container within the Pod, \"sec-ctx-demo,\" uses the \"agnhost\" image and runs a simple command to sleep for an hour. The container\\'s security context disallows privilege escalation (`allowPrivilegeEscalation: false`). Overall, the configuration demonstrates the setup of security settings at both Pod and container levels to enforce security best practices in a Kubernetes environment, such as defining user permissions and restricting privilege escalation.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: security-context-demo\\nspec:\\n  securityContext:\\n    runAsUser: 1000\\n    runAsGroup: 3000\\n    supplementalGroups: [4000]\\n    supplementalGroupsPolicy: Strict\\n  containers:\\n  - name: sec-ctx-demo\\n    image: registry.k8s.io/e2e-test-images/agnhost:2.45\\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\\n    securityContext:\\n      allowPrivilegeEscalation: false\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes Pod configuration in YAML format, focusing on security contexts. It specifies a Pod named \"security-context-demo\" with a global security context that sets user and group IDs (`runAsUser: 1000`, `runAsGroup: 3000`) and additional groups (`supplementalGroups: [4000]`) with a strict policy. The container within the Pod, \"sec-ctx-demo,\" uses the \"agnhost\" image and runs a simple command to sleep for an hour. The container\\'s security context disallows privilege escalation (`allowPrivilegeEscalation: false`). Overall, the configuration demonstrates the setup of security settings at both Pod and container levels to enforce security best practices in a Kubernetes environment, such as defining user permissions and restricting privilege escalation.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\security-context-6.yaml'},\n",
       "  0.7109375),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Pod with specific security and container settings. It specifies the use of the `v1` API version and creates a Pod named \"default-pod\" with labels and annotations for identification and security enhancements, such as enabling the default seccomp profile for system call filtering. The Pod contains a single container named \"test-container\" that uses the `hashicorp/http-echo:0.2.3` image, which is configured to output a custom message (\"just made some syscalls!\") when run. Additionally, the security context of the container is set to disallow privilege escalation, ensuring better security by preventing the container from gaining higher privileges. Overall, this configuration demonstrates how to deploy a simple, secure Pod with a custom message using Kubernetes, emphasizing security best practices with seccomp profile and privilege restrictions.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: default-pod\\n  labels:\\n    app: default-pod\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/pod: runtime/default\\nspec:\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:0.2.3\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\default-pod.yaml',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Pod with specific security and container settings. It specifies the use of the `v1` API version and creates a Pod named \"default-pod\" with labels and annotations for identification and security enhancements, such as enabling the default seccomp profile for system call filtering. The Pod contains a single container named \"test-container\" that uses the `hashicorp/http-echo:0.2.3` image, which is configured to output a custom message (\"just made some syscalls!\") when run. Additionally, the security context of the container is set to disallow privilege escalation, ensuring better security by preventing the container from gaining higher privileges. Overall, this configuration demonstrates how to deploy a simple, secure Pod with a custom message using Kubernetes, emphasizing security best practices with seccomp profile and privilege restrictions.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.70703125),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Pod named \"qos-demo-2\" in the \"qos-example\" namespace. The Pod includes a single container that uses the \"nginx\" image. The resource specifications for the container set a memory request of 100Mi and a limit of 200Mi. This configuration is designed to demonstrate Kubernetes Quality of Service (QoS) classes based on how resource requests and limits are set: a pod with a request and limit on memory will fall into the \"Burstable\" QoS class, allowing some flexibility in resource management and prioritization during contention.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: qos-demo-2\\n  namespace: qos-example\\nspec:\\n  containers:\\n  - name: qos-demo-2-ctr\\n    image: nginx\\n    resources:\\n      limits:\\n        memory: \"200Mi\"\\n      requests:\\n        memory: \"100Mi\"\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Pod named \"qos-demo-2\" in the \"qos-example\" namespace. The Pod includes a single container that uses the \"nginx\" image. The resource specifications for the container set a memory request of 100Mi and a limit of 200Mi. This configuration is designed to demonstrate Kubernetes Quality of Service (QoS) classes based on how resource requests and limits are set: a pod with a request and limit on memory will fall into the \"Burstable\" QoS class, allowing some flexibility in resource management and prioritization during contention.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\qos\\\\qos-pod-2.yaml'},\n",
       "  0.70703125),\n",
       " ({'content': '# The provided YAML configuration defines a Kubernetes ClusterRole named \"csr-approver\" using RBAC (Role-Based Access Control). This ClusterRole grants specific permissions related to managing certificate signing requests (CSRs). It allows users to perform actions such as retrieving, listing, and watching CSRs, updating CSR approvals, and approving signers associated with a particular signer name, in this case, \"example.com/my-signer-name.\" The role is designed to enable a user or service account to handle CSR approval processes securely and with fine-grained control within the cluster.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: csr-approver\\nrules:\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - certificatesigningrequests/approval\\n  verbs:\\n  - update\\n- apiGroups:\\n  - certificates.k8s.io\\n  resources:\\n  - signers\\n  resourceNames:\\n  - example.com/my-signer-name # example.com/* can be used to authorize for all signers in the \\'example.com\\' domain\\n  verbs:\\n  - approve\\n\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'The provided YAML configuration defines a Kubernetes ClusterRole named \"csr-approver\" using RBAC (Role-Based Access Control). This ClusterRole grants specific permissions related to managing certificate signing requests (CSRs). It allows users to perform actions such as retrieving, listing, and watching CSRs, updating CSR approvals, and approving signers associated with a particular signer name, in this case, \"example.com/my-signer-name.\" The role is designed to enable a user or service account to handle CSR approval processes securely and with fine-grained control within the cluster.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\access\\\\certificate-signing-request\\\\clusterrole-approve.yaml'},\n",
       "  0.703125),\n",
       " ({'content': '# The provided content details the configuration and deployment of a custom Kubernetes scheduler named \"my-scheduler.\" It includes Kubernetes manifest files defining a ServiceAccount for the scheduler, RBAC (Role-Based Access Control) bindings to grant necessary permissions, a ConfigMap containing the scheduler\\'s configuration, and a Deployment to run the scheduler pod.\\n\\nThe ServiceAccount, named \"my-scheduler,\" operates within the \"kube-system\" namespace. Several RBAC RoleBindings associate this ServiceAccount with important cluster roles like \"system:kube-scheduler,\" \"system:volume-scheduler,\" and \"extension-apiserver-authentication-reader,\" thereby granting it appropriate permissions for scheduling, volume management, and API authentication.\\n\\nThe ConfigMap \"my-scheduler-config\" stores the scheduler\\'s configuration in YAML format, specifying API version, scheduler profile, and disabling leader election. The Deployment creates a single replica pod running the custom scheduler container, which uses the image from \"gcr.io/my-gcp-project/my-kube-scheduler:1.0\" and reads its configuration from the mounted ConfigMap. It includes health probes for liveness and readiness checks to ensure the scheduler\\'s proper functioning. The container runs with a specific command to start the kube-scheduler with the provided configuration, and the pod is configured to run in the network namespace without privileged access.\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: my-scheduler\\n  namespace: kube-system\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: my-scheduler-as-kube-scheduler\\nsubjects:\\n- kind: ServiceAccount\\n  name: my-scheduler\\n  namespace: kube-system\\nroleRef:\\n  kind: ClusterRole\\n  name: system:kube-scheduler\\n  apiGroup: rbac.authorization.k8s.io\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: my-scheduler-as-volume-scheduler\\nsubjects:\\n- kind: ServiceAccount\\n  name: my-scheduler\\n  namespace: kube-system\\nroleRef:\\n  kind: ClusterRole\\n  name: system:volume-scheduler\\n  apiGroup: rbac.authorization.k8s.io\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: my-scheduler-extension-apiserver-authentication-reader\\n  namespace: kube-system\\nroleRef:\\n  kind: Role\\n  name: extension-apiserver-authentication-reader\\n  apiGroup: rbac.authorization.k8s.io\\nsubjects:\\n- kind: ServiceAccount\\n  name: my-scheduler\\n  namespace: kube-system\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: my-scheduler-config\\n  namespace: kube-system\\ndata:\\n  my-scheduler-config.yaml: |\\n    apiVersion: kubescheduler.config.k8s.io/v1beta2\\n    kind: KubeSchedulerConfiguration\\n    profiles:\\n      - schedulerName: my-scheduler\\n    leaderElection:\\n      leaderElect: false\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    component: scheduler\\n    tier: control-plane\\n  name: my-scheduler\\n  namespace: kube-system\\nspec:\\n  selector:\\n    matchLabels:\\n      component: scheduler\\n      tier: control-plane\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        component: scheduler\\n        tier: control-plane\\n        version: second\\n    spec:\\n      serviceAccountName: my-scheduler\\n      containers:\\n      - command:\\n        - /usr/local/bin/kube-scheduler\\n        - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml\\n        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0\\n        livenessProbe:\\n          httpGet:\\n            path: /healthz\\n            port: 10259\\n            scheme: HTTPS\\n          initialDelaySeconds: 15\\n        name: kube-second-scheduler\\n        readinessProbe:\\n          httpGet:\\n            path: /healthz\\n            port: 10259\\n            scheme: HTTPS\\n        resources:\\n          requests:\\n            cpu: \\'0.1\\'\\n        securityContext:\\n          privileged: false\\n        volumeMounts:\\n          - name: config-volume\\n            mountPath: /etc/kubernetes/my-scheduler\\n      hostNetwork: false\\n      hostPID: false\\n      volumes:\\n        - name: config-volume\\n          configMap:\\n            name: my-scheduler-config\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\sched\\\\my-scheduler.yaml',\n",
       "   'summary': 'The provided content details the configuration and deployment of a custom Kubernetes scheduler named \"my-scheduler.\" It includes Kubernetes manifest files defining a ServiceAccount for the scheduler, RBAC (Role-Based Access Control) bindings to grant necessary permissions, a ConfigMap containing the scheduler\\'s configuration, and a Deployment to run the scheduler pod.\\n\\nThe ServiceAccount, named \"my-scheduler,\" operates within the \"kube-system\" namespace. Several RBAC RoleBindings associate this ServiceAccount with important cluster roles like \"system:kube-scheduler,\" \"system:volume-scheduler,\" and \"extension-apiserver-authentication-reader,\" thereby granting it appropriate permissions for scheduling, volume management, and API authentication.\\n\\nThe ConfigMap \"my-scheduler-config\" stores the scheduler\\'s configuration in YAML format, specifying API version, scheduler profile, and disabling leader election. The Deployment creates a single replica pod running the custom scheduler container, which uses the image from \"gcr.io/my-gcp-project/my-kube-scheduler:1.0\" and reads its configuration from the mounted ConfigMap. It includes health probes for liveness and readiness checks to ensure the scheduler\\'s proper functioning. The container runs with a specific command to start the kube-scheduler with the provided configuration, and the pod is configured to run in the network namespace without privileged access.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.703125),\n",
       " ({'content': '# This content defines a Kubernetes StatefulSet configuration for deploying a Cassandra database cluster with three replicas. The StatefulSet ensures each Cassandra node is uniquely identifiable and maintains persistent storage, facilitated by volumeClaimTemplates that request 1Gi of SSD storage from a custom StorageClass named \"fast.\" The pods run the Cassandra container with specific resource limits and environment variables configuring seed nodes, cluster name, data center, and rack information. The configuration includes preStop lifecycle hooks to gracefully drain each node before shutdown, as well as readiness probes to verify container health through a custom script. The StorageClass \"fast\" uses the minikube-hostpath provisioner with SSD storage capabilities, ensuring high-performance disk I/O suitable for a database cluster. Overall, this setup enables scalable, resilient deployment of Cassandra within a Kubernetes environment with persistent, high-speed storage.\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: cassandra\\n  labels:\\n    app: cassandra\\nspec:\\n  serviceName: cassandra\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: cassandra\\n  template:\\n    metadata:\\n      labels:\\n        app: cassandra\\n    spec:\\n      terminationGracePeriodSeconds: 500\\n      containers:\\n      - name: cassandra\\n        image: gcr.io/google-samples/cassandra:v13\\n        imagePullPolicy: Always\\n        ports:\\n        - containerPort: 7000\\n          name: intra-node\\n        - containerPort: 7001\\n          name: tls-intra-node\\n        - containerPort: 7199\\n          name: jmx\\n        - containerPort: 9042\\n          name: cql\\n        resources:\\n          limits:\\n            cpu: \"500m\"\\n            memory: 1Gi\\n          requests:\\n            cpu: \"500m\"\\n            memory: 1Gi\\n        securityContext:\\n          capabilities:\\n            add:\\n              - IPC_LOCK\\n        lifecycle:\\n          preStop:\\n            exec:\\n              command: \\n              - /bin/sh\\n              - -c\\n              - nodetool drain\\n        env:\\n          - name: MAX_HEAP_SIZE\\n            value: 512M\\n          - name: HEAP_NEWSIZE\\n            value: 100M\\n          - name: CASSANDRA_SEEDS\\n            value: \"cassandra-0.cassandra.default.svc.cluster.local\"\\n          - name: CASSANDRA_CLUSTER_NAME\\n            value: \"K8Demo\"\\n          - name: CASSANDRA_DC\\n            value: \"DC1-K8Demo\"\\n          - name: CASSANDRA_RACK\\n            value: \"Rack1-K8Demo\"\\n          - name: POD_IP\\n            valueFrom:\\n              fieldRef:\\n                fieldPath: status.podIP\\n        readinessProbe:\\n          exec:\\n            command:\\n            - /bin/bash\\n            - -c\\n            - /ready-probe.sh\\n          initialDelaySeconds: 15\\n          timeoutSeconds: 5\\n        # These volume mounts are persistent. They are like inline claims,\\n        # but not exactly because the names need to match exactly one of\\n        # the stateful pod volumes.\\n        volumeMounts:\\n        - name: cassandra-data\\n          mountPath: /cassandra_data\\n  # These are converted to volume claims by the controller\\n  # and mounted at the paths mentioned above.\\n  # do not use these in production until ssd GCEPersistentDisk or other ssd pd\\n  volumeClaimTemplates:\\n  - metadata:\\n      name: cassandra-data\\n    spec:\\n      accessModes: [ \"ReadWriteOnce\" ]\\n      storageClassName: fast\\n      resources:\\n        requests:\\n          storage: 1Gi\\n---\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: fast\\nprovisioner: k8s.io/minikube-hostpath\\nparameters:\\n  type: pd-ssd\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\cassandra\\\\cassandra-statefulset.yaml',\n",
       "   'summary': 'This content defines a Kubernetes StatefulSet configuration for deploying a Cassandra database cluster with three replicas. The StatefulSet ensures each Cassandra node is uniquely identifiable and maintains persistent storage, facilitated by volumeClaimTemplates that request 1Gi of SSD storage from a custom StorageClass named \"fast.\" The pods run the Cassandra container with specific resource limits and environment variables configuring seed nodes, cluster name, data center, and rack information. The configuration includes preStop lifecycle hooks to gracefully drain each node before shutdown, as well as readiness probes to verify container health through a custom script. The StorageClass \"fast\" uses the minikube-hostpath provisioner with SSD storage capabilities, ensuring high-performance disk I/O suitable for a database cluster. Overall, this setup enables scalable, resilient deployment of Cassandra within a Kubernetes environment with persistent, high-speed storage.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.703125),\n",
       " ({'content': '# This Kubernetes configuration defines a Pod named \"counter\" with three containers that work together to generate and monitor log entries. The first container, \"count,\" runs a Bash script within a Busybox image that continuously increments a counter every second, logs the current count with a timestamp to two log files, and thus demonstrates a simple log-generating process. The other two containers, \"count-log-1\" and \"count-log-2,\" continuously follow (tail -f) these log files to monitor new entries in real-time.\\n\\nThe code\\'s main function is to simulate a logging scenario where one container produces log entries, while the others act as log consumers or viewers. This setup showcases container coordination via shared volumes; all three containers mount the same shared directory, /var/log, provided by an emptyDir volume, ensuring that log files are accessible across containers. Overall, the configuration highlights inter-container communication through shared storage, continuous log generation, and real-time log monitoring, which are fundamental concepts in DevOps and container orchestration.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: counter\\nspec:\\n  containers:\\n  - name: count\\n    image: busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  - name: count-log-1\\n    image: busybox:1.28\\n    args: [/bin/sh, -c, \\'tail -n+1 -F /var/log/1.log\\']\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  - name: count-log-2\\n    image: busybox:1.28\\n    args: [/bin/sh, -c, \\'tail -n+1 -F /var/log/2.log\\']\\n    volumeMounts:\\n    - name: varlog\\n      mountPath: /var/log\\n  volumes:\\n  - name: varlog\\n    emptyDir: {}\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This Kubernetes configuration defines a Pod named \"counter\" with three containers that work together to generate and monitor log entries. The first container, \"count,\" runs a Bash script within a Busybox image that continuously increments a counter every second, logs the current count with a timestamp to two log files, and thus demonstrates a simple log-generating process. The other two containers, \"count-log-1\" and \"count-log-2,\" continuously follow (tail -f) these log files to monitor new entries in real-time.\\n\\nThe code\\'s main function is to simulate a logging scenario where one container produces log entries, while the others act as log consumers or viewers. This setup showcases container coordination via shared volumes; all three containers mount the same shared directory, /var/log, provided by an emptyDir volume, ensuring that log files are accessible across containers. Overall, the configuration highlights inter-container communication through shared storage, continuous log generation, and real-time log monitoring, which are fundamental concepts in DevOps and container orchestration.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\admin\\\\logging\\\\two-files-counter-pod-streaming-sidecar.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.703125),\n",
       " ({'content': '# This content defines a Kubernetes Pod configuration using YAML. The pod is named \"memory-demo\" within the \"mem-example\" namespace and contains a single container based on the \"polinux/stress\" image. The container is configured with resource requests and limits for memory: it requests 100MiB and limits it to 200MiB. The container runs the \"stress\" command with specific arguments to simulate memory pressure, instructing it to create a single worker (`--vm 1`) that consumes 150MiB of RAM (`--vm-bytes 150M`) and includes a hang period (`--vm-hang 1`) to prolong the stress condition. This setup is typically used for testing how Kubernetes handles memory stress scenarios within a container.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: memory-demo\\n  namespace: mem-example\\nspec:\\n  containers:\\n  - name: memory-demo-ctr\\n    image: polinux/stress\\n    resources:\\n      requests:\\n        memory: \"100Mi\"\\n      limits:\\n        memory: \"200Mi\"\\n    command: [\"stress\"]\\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"150M\", \"--vm-hang\", \"1\"]\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes Pod configuration using YAML. The pod is named \"memory-demo\" within the \"mem-example\" namespace and contains a single container based on the \"polinux/stress\" image. The container is configured with resource requests and limits for memory: it requests 100MiB and limits it to 200MiB. The container runs the \"stress\" command with specific arguments to simulate memory pressure, instructing it to create a single worker (`--vm 1`) that consumes 150MiB of RAM (`--vm-bytes 150M`) and includes a hang period (`--vm-hang 1`) to prolong the stress condition. This setup is typically used for testing how Kubernetes handles memory stress scenarios within a container.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.703125),\n",
       " ({'content': '# The provided content is a Kubernetes Pod configuration in YAML format. It defines a Pod named \"memory-demo-2\" within the \"mem-example\" namespace, containing a single container. The container uses the \"polinux/stress\" image, which is designed for stress testing system resources. The container requests 50Mi of memory and has a limit of 100Mi, ensuring resource management within the cluster. The container executes the \"stress\" command with arguments to simulate memory load by creating one virtual memory worker that allocates 250MB of memory and then hangs, which can be useful for testing memory management and performance under pressure in a Kubernetes environment.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: memory-demo-2\\n  namespace: mem-example\\nspec:\\n  containers:\\n  - name: memory-demo-2-ctr\\n    image: polinux/stress\\n    resources:\\n      requests:\\n        memory: \"50Mi\"\\n      limits:\\n        memory: \"100Mi\"\\n    command: [\"stress\"]\\n    args: [\"--vm\", \"1\", \"--vm-bytes\", \"250M\", \"--vm-hang\", \"1\"]\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\resource\\\\memory-request-limit-2.yaml',\n",
       "   'summary': 'The provided content is a Kubernetes Pod configuration in YAML format. It defines a Pod named \"memory-demo-2\" within the \"mem-example\" namespace, containing a single container. The container uses the \"polinux/stress\" image, which is designed for stress testing system resources. The container requests 50Mi of memory and has a limit of 100Mi, ensuring resource management within the cluster. The container executes the \"stress\" command with arguments to simulate memory load by creating one virtual memory worker that allocates 250MB of memory and then hangs, which can be useful for testing memory management and performance under pressure in a Kubernetes environment.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.703125),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Pod named \"violation-pod\" with specific metadata, including labels and annotations. The annotation links to a Seccomp profile (\"localhost/profiles/violation.json\"), which is used to restrict system calls for enhanced security. The Pod contains a single container using the \"hashicorp/http-echo:0.2.3\" image, configured to display the message \"just made some syscalls!\" when executed. The security context of the container explicitly disallows privilege escalation, adding an extra layer of security. Overall, this configuration exemplifies how to deploy a container with security policies like Seccomp profiles and privilege restrictions within Kubernetes.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: violation-pod\\n  labels:\\n    app: violation-pod\\n  annotations:\\n    seccomp.security.alpha.kubernetes.io/pod: localhost/profiles/violation.json\\nspec:\\n  containers:\\n  - name: test-container\\n    image: hashicorp/http-echo:0.2.3\\n    args:\\n    - \"-text=just made some syscalls!\"\\n    securityContext:\\n      allowPrivilegeEscalation: false',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Pod named \"violation-pod\" with specific metadata, including labels and annotations. The annotation links to a Seccomp profile (\"localhost/profiles/violation.json\"), which is used to restrict system calls for enhanced security. The Pod contains a single container using the \"hashicorp/http-echo:0.2.3\" image, configured to display the message \"just made some syscalls!\" when executed. The security context of the container explicitly disallows privilege escalation, adding an extra layer of security. Overall, this configuration exemplifies how to deploy a container with security policies like Seccomp profiles and privilege restrictions within Kubernetes.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\pods\\\\security\\\\seccomp\\\\alpha\\\\violation-pod.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.69921875),\n",
       " ({'content': '# The provided content is a Kubernetes Job configuration in YAML format, designed to run multiple parallel pods with a specific success policy. The job specifies 10 parallel pods (`parallelism: 10`) and aims to complete all 10 tasks (`completions: 10`), with indexing support enabled (`completionMode: Indexed`). The success policy indicates that the job is considered successful if at least one pod with index 0 or within indices 2-3 succeeds.\\n\\nThe pod template runs a single container based on the Python image, executing a Python script. The script checks an environment variable `JOB_COMPLETION_INDEX` to determine its index. If the index is \"2\", the script exits successfully (`sys.exit(0)`), otherwise, it exits with failure (`sys.exit(1)`).\\n\\nThis setup demonstrates how to control job completion criteria based on specific pod indices and their success or failure, allowing for more flexible and targeted job success policies in Kubernetes.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job-success\\nspec:\\n  parallelism: 10\\n  completions: 10\\n  completionMode: Indexed # Required for the success policy\\n  successPolicy:\\n    rules:\\n      - succeededIndexes: 0,2-3\\n        succeededCount: 1\\n  template:\\n    spec:\\n      containers:\\n      - name: main\\n        image: python\\n        command:          # Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded,\\n                          # the overall Job is a success.\\n          - python3\\n          - -c\\n          - |\\n            import os, sys\\n            if os.environ.get(\"JOB_COMPLETION_INDEX\") == \"2\":\\n              sys.exit(0)\\n            else:\\n              sys.exit(1)\\n      restartPolicy: Never\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'The provided content is a Kubernetes Job configuration in YAML format, designed to run multiple parallel pods with a specific success policy. The job specifies 10 parallel pods (`parallelism: 10`) and aims to complete all 10 tasks (`completions: 10`), with indexing support enabled (`completionMode: Indexed`). The success policy indicates that the job is considered successful if at least one pod with index 0 or within indices 2-3 succeeds.\\n\\nThe pod template runs a single container based on the Python image, executing a Python script. The script checks an environment variable `JOB_COMPLETION_INDEX` to determine its index. If the index is \"2\", the script exits successfully (`sys.exit(0)`), otherwise, it exits with failure (`sys.exit(1)`).\\n\\nThis setup demonstrates how to control job completion criteria based on specific pod indices and their success or failure, allowing for more flexible and targeted job success policies in Kubernetes.',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\job-success-policy.yaml',\n",
       "   'subchunk': '1/1'},\n",
       "  0.69921875),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes DaemonSet, which ensures that a copy of a specific pod runs on all suitable nodes in the cluster. The DaemonSet is named \"my-daemonset\" and is labeled with \"app: foo\" for identification. It specifies a selector to match pods with this label and uses a pod template that includes a container running the \"microsoft/windowsservercore:1709\" image. Additionally, the nodeSelector is used to schedule the pods only on Windows nodes, identified by the label \"kubernetes.io/os: windows.\" This setup is useful for deploying system-level services, agents, or monitoring tools uniformly across all Windows-based nodes in a Kubernetes cluster.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: my-daemonset\\n  labels:\\n    app: foo\\nspec:\\n  selector:\\n    matchLabels:\\n      app: foo\\n  template:\\n    metadata:\\n      labels:\\n        app: foo\\n    spec:\\n      containers:\\n      - name: foo\\n        image: microsoft/windowsservercore:1709\\n      nodeSelector:\\n        kubernetes.io/os: windows\\n\\n',\n",
       "   'chunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes DaemonSet, which ensures that a copy of a specific pod runs on all suitable nodes in the cluster. The DaemonSet is named \"my-daemonset\" and is labeled with \"app: foo\" for identification. It specifies a selector to match pods with this label and uses a pod template that includes a container running the \"microsoft/windowsservercore:1709\" image. Additionally, the nodeSelector is used to schedule the pods only on Windows nodes, identified by the label \"kubernetes.io/os: windows.\" This setup is useful for deploying system-level services, agents, or monitoring tools uniformly across all Windows-based nodes in a Kubernetes cluster.',\n",
       "   'subchunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\windows\\\\daemonset.yaml'},\n",
       "  0.69921875),\n",
       " ({'content': '# This content defines a Kubernetes Job configuration in YAML format designed to execute multiple parallel tasks. The job is named \"indexed-job\" and is set to run a total of 5 completions, with a maximum of 3 pods working simultaneously, utilizing an indexed completion mode. This mode allows each pod to process a unique index, which is used to distinguish individual tasks within the job.\\n\\nThe job\\'s pod template runs a container using the \"busybox\" image, executing the command \"rev /input/data.txt\" to reverse the contents of a specific text file. The file \"data.txt\" is made available inside the container through a mounted volume, which uses the downward API to inject the pod\\'s completion index into the file\\'s path, allowing each pod to process a unique data segment based on its assigned index. This setup is useful for parallel batch processing where each task operates on distinct data segments, with the overall process managed efficiently by Kubernetes.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: \\'indexed-job\\'\\nspec:\\n  completions: 5\\n  parallelism: 3\\n  completionMode: Indexed\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      containers:\\n      - name: \\'worker\\'\\n        image: \\'docker.io/library/busybox\\'\\n        command:\\n        - \"rev\"\\n        - \"/input/data.txt\"\\n        volumeMounts:\\n        - mountPath: /input\\n          name: input\\n      volumes:\\n      - name: input\\n        downwardAPI:\\n          items:\\n          - path: \"data.txt\"\\n            fieldRef:\\n              fieldPath: metadata.annotations[\\'batch.kubernetes.io/job-completion-index\\']',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This content defines a Kubernetes Job configuration in YAML format designed to execute multiple parallel tasks. The job is named \"indexed-job\" and is set to run a total of 5 completions, with a maximum of 3 pods working simultaneously, utilizing an indexed completion mode. This mode allows each pod to process a unique index, which is used to distinguish individual tasks within the job.\\n\\nThe job\\'s pod template runs a container using the \"busybox\" image, executing the command \"rev /input/data.txt\" to reverse the contents of a specific text file. The file \"data.txt\" is made available inside the container through a mounted volume, which uses the downward API to inject the pod\\'s completion index into the file\\'s path, allowing each pod to process a unique data segment based on its assigned index. This setup is useful for parallel batch processing where each task operates on distinct data segments, with the overall process managed efficiently by Kubernetes.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job-vol.yaml'},\n",
       "  0.69140625),\n",
       " ({'content': \"# The provided content is a Kubernetes DaemonSet configuration, which ensures that a specific pod runs on all (or selected) nodes in a cluster. This particular DaemonSet deploys Fluentd, a logging agent, configured to forward logs to Elasticsearch. It is set in the `kube-system` namespace and labeled appropriately for management. The update strategy employs rolling updates with a maximum of one unavailable pod during updates, ensuring minimal disruption.\\n\\nThe pod template specifies tolerations allowing it to run on control plane nodes by matching specific taints, which is useful for centralized logging. The container uses a Fluentd image (`quay.io/fluentd_elasticsearch/fluentd:v2.5.2`) with resource limits and requests to optimize resource utilization. It mounts host paths (`/var/log` and `/var/lib/docker/containers`) into the container, enabling Fluentd to collect logs directly from the node's log files and Docker container logs. The configuration thus ensures that logs from all nodes are collected and forwarded for centralized analysis with minimal impact on host resources.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluentd-elasticsearch\\n  namespace: kube-system\\n  labels:\\n    k8s-app: fluentd-logging\\nspec:\\n  selector:\\n    matchLabels:\\n      name: fluentd-elasticsearch\\n  updateStrategy:\\n    type: RollingUpdate\\n    rollingUpdate:\\n      maxUnavailable: 1\\n  template:\\n    metadata:\\n      labels:\\n        name: fluentd-elasticsearch\\n    spec:\\n      tolerations:\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator: Exists\\n        effect: NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      containers:\\n      - name: fluentd-elasticsearch\\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        resources:\\n          limits:\\n            memory: 200Mi\\n          requests:\\n            cpu: 100m\\n            memory: 200Mi\\n        volumeMounts:\\n        - name: varlog\\n          mountPath: /var/log\\n        - name: varlibdockercontainers\\n          mountPath: /var/lib/docker/containers\\n          readOnly: true\\n      terminationGracePeriodSeconds: 30\\n      volumes:\\n      - name: varlog\\n        hostPath:\\n          path: /var/log\\n      - name: varlibdockercontainers\\n        hostPath:\\n          path: /var/lib/docker/containers\\n\",\n",
       "   'subchunk': '1/1',\n",
       "   'summary': \"The provided content is a Kubernetes DaemonSet configuration, which ensures that a specific pod runs on all (or selected) nodes in a cluster. This particular DaemonSet deploys Fluentd, a logging agent, configured to forward logs to Elasticsearch. It is set in the `kube-system` namespace and labeled appropriately for management. The update strategy employs rolling updates with a maximum of one unavailable pod during updates, ensuring minimal disruption.\\n\\nThe pod template specifies tolerations allowing it to run on control plane nodes by matching specific taints, which is useful for centralized logging. The container uses a Fluentd image (`quay.io/fluentd_elasticsearch/fluentd:v2.5.2`) with resource limits and requests to optimize resource utilization. It mounts host paths (`/var/log` and `/var/lib/docker/containers`) into the container, enabling Fluentd to collect logs directly from the node's log files and Docker container logs. The configuration thus ensures that logs from all nodes are collected and forwarded for centralized analysis with minimal impact on host resources.\",\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset-update.yaml'},\n",
       "  0.6875),\n",
       " ({'content': '# This YAML configuration defines a Kubernetes Job that executes multiple parallel tasks with indexed completions. The Job is set to run five total completions, with three pods working simultaneously, and each task is identified by a unique `JOB_COMPLETION_INDEX`. The template specifies an `initContainer` that initializes data based on this index by selecting a specific item from a list and writing it to a shared volume. The main container then reads this data and performs a reverse operation (`rev` command) on the content of `/input/data.txt`. The volume mounted as `emptyDir` facilitates sharing data between the init container and the main worker container within each pod. Overall, this setup ensures each parallel task processes a distinct item from the list based on its index, demonstrating indexed parallelism in Kubernetes Jobs.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: \\'indexed-job\\'\\nspec:\\n  completions: 5\\n  parallelism: 3\\n  completionMode: Indexed\\n  template:\\n    spec:\\n      restartPolicy: Never\\n      initContainers:\\n      - name: \\'input\\'\\n        image: \\'docker.io/library/bash\\'\\n        command:\\n        - \"bash\"\\n        - \"-c\"\\n        - |\\n          items=(foo bar baz qux xyz)\\n          echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt\\n        volumeMounts:\\n        - mountPath: /input\\n          name: input\\n      containers:\\n      - name: \\'worker\\'\\n        image: \\'docker.io/library/busybox\\'\\n        command:\\n        - \"rev\"\\n        - \"/input/data.txt\"\\n        volumeMounts:\\n        - mountPath: /input\\n          name: input\\n      volumes:\\n      - name: input\\n        emptyDir: {}\\n',\n",
       "   'subchunk': '1/1',\n",
       "   'summary': 'This YAML configuration defines a Kubernetes Job that executes multiple parallel tasks with indexed completions. The Job is set to run five total completions, with three pods working simultaneously, and each task is identified by a unique `JOB_COMPLETION_INDEX`. The template specifies an `initContainer` that initializes data based on this index by selecting a specific item from a list and writing it to a shared volume. The main container then reads this data and performs a reverse operation (`rev` command) on the content of `/input/data.txt`. The volume mounted as `emptyDir` facilitates sharing data between the init container and the main worker container within each pod. Overall, this setup ensures each parallel task processes a distinct item from the list based on its index, demonstrating indexed parallelism in Kubernetes Jobs.',\n",
       "   'chunk': '1/1',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\application\\\\job\\\\indexed-job.yaml'},\n",
       "  0.6875),\n",
       " ({'content': '# This content is a Kubernetes configuration defining a DaemonSet named \"fluentd-elasticsearch\" in the \"kube-system\" namespace. The purpose of this DaemonSet is to deploy Fluentd instances on each node to collect and forward log data to Elasticsearch, facilitating centralized logging. It uses a rolling update strategy to ensure minimal downtime during updates and includes tolerations to allow deployment on control plane nodes if necessary. The DaemonSet specifies a container running the Fluentd image, mounted with host paths to access log files stored in \"/var/log\" and Docker container logs in \"/var/lib/docker/containers\", enabling comprehensive log collection from all nodes. The configuration ensures the logging agents are resilient and integrated with the host’s filesystem for efficient log processing across the cluster.\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: fluentd-elasticsearch\\n  namespace: kube-system\\n  labels:\\n    k8s-app: fluentd-logging\\nspec:\\n  selector:\\n    matchLabels:\\n      name: fluentd-elasticsearch\\n  updateStrategy:\\n    type: RollingUpdate\\n    rollingUpdate:\\n      maxUnavailable: 1\\n  template:\\n    metadata:\\n      labels:\\n        name: fluentd-elasticsearch\\n    spec:\\n      tolerations:\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator: Exists\\n        effect: NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator: Exists\\n        effect: NoSchedule\\n      containers:\\n      - name: fluentd-elasticsearch\\n        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        volumeMounts:\\n        - name: varlog\\n          mountPath: /var/log\\n        - name: varlibdockercontainers\\n          mountPath: /var/lib/docker/containers\\n          readOnly: true\\n      terminationGracePeriodSeconds: 30\\n      volumes:\\n      - name: varlog\\n        hostPath:\\n          path: /var/log\\n      - name: varlibdockercontainers\\n        hostPath:\\n          path: /var/lib/docker/containers\\n',\n",
       "   'filename': 'knowledge\\\\kubernetes\\\\website-main\\\\content\\\\en\\\\examples\\\\controllers\\\\fluentd-daemonset.yaml',\n",
       "   'summary': 'This content is a Kubernetes configuration defining a DaemonSet named \"fluentd-elasticsearch\" in the \"kube-system\" namespace. The purpose of this DaemonSet is to deploy Fluentd instances on each node to collect and forward log data to Elasticsearch, facilitating centralized logging. It uses a rolling update strategy to ensure minimal downtime during updates and includes tolerations to allow deployment on control plane nodes if necessary. The DaemonSet specifies a container running the Fluentd image, mounted with host paths to access log files stored in \"/var/log\" and Docker container logs in \"/var/lib/docker/containers\", enabling comprehensive log collection from all nodes. The configuration ensures the logging agents are resilient and integrated with the host’s filesystem for efficient log processing across the cluster.',\n",
       "   'subchunk': '1/1',\n",
       "   'chunk': '1/1'},\n",
       "  0.67578125)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_results = late_chunking_helper.re_rank(query=query, docs=rag_results)\n",
    "ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8080/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "d:\\Python\\MasterIA\\TFM\\test_folder\\.venv\\lib\\site-packages\\weaviate\\warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Object(uuid=_WeaviateUUIDInt('5dd3f31c-5e73-4bdb-b099-e7fcf1eb1acb'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '## {{% heading \"whatsnext\" %}}  \\n* Read the [kubectl overview](/docs/reference/kubectl/) and learn about [JsonPath](/docs/reference/kubectl/jsonpath).  \\n* See [kubectl](/docs/reference/kubectl/kubectl/) options.  \\n* Also read [kubectl Usage Conventions](/docs/reference/kubectl/conventions/) to understand how to use kubectl in reusable scripts.  \\n* See more community [kubectl cheatsheets](https://github.com/dennyzhang/cheatsheet-kubernetes-A4).', 'filename': 'knowledge/kubernetes/website-main/content/en/docs/reference/kubectl/quick-reference.md', 'chunk': '23/23', 'subchunk': '1/1'}, references=None, vector={}, collection='Test'),\n",
       " Object(uuid=_WeaviateUUIDInt('dc2a97aa-b075-49f4-a7ca-1efaeb886585'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '## Kubectl autocomplete  \\n### BASH  \\n```bash\\nsource <(kubectl completion bash) # set up autocomplete in bash into the current shell, bash-completion package should be installed first.\\necho \"source <(kubectl completion bash)\" >> ~/.bashrc # add autocomplete permanently to your bash shell.\\n```  \\nYou can also use a shorthand alias for `kubectl` that also works with completion:  \\n```bash\\nalias k=kubectl\\ncomplete -o default -F __start_kubectl k\\n```  \\n### ZSH  \\n```bash\\nsource <(kubectl completion zsh)  # set up autocomplete in zsh into the current shell\\necho \\'[[ $commands[kubectl] ]] && source <(kubectl completion zsh)\\' >> ~/.zshrc # add autocomplete permanently to your zsh shell\\n```  \\n### FISH  \\n{{< note >}}\\nRequires kubectl version 1.23 or above.\\n{{< /note >}}  \\n```bash\\necho \\'kubectl completion fish | source\\' > ~/.config/fish/completions/kubectl.fish && source ~/.config/fish/completions/kubectl.fish\\n```  \\n### A note on `--all-namespaces`  \\nAppending `--all-namespaces` happens frequently enough that you should be aware of the shorthand for `--all-namespaces`:  \\n```kubectl -A```', 'filename': 'knowledge/kubernetes/website-main/content/en/docs/reference/kubectl/quick-reference.md', 'chunk': '2/23', 'subchunk': '1/1'}, references=None, vector={}, collection='Test'),\n",
       " Object(uuid=_WeaviateUUIDInt('7b17d2ce-0c4b-41a3-988d-66ce28ba9389'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'content': '# List Names of Pods that belong to Particular RC\\n# \"jq\" command useful for transformations that are too complex for jsonpath, it can be found at https://jqlang.github.io/jq/\\nsel=${$(kubectl get rc my-rc --output=json | jq -j \\'.spec.selector | to_entries | .[] | \"\\\\(.key)=\\\\(.value),\"\\')%?}\\necho $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name})\\n\\n# Show labels for all pods (or any other Kubernetes object that supports labelling)\\nkubectl get pods --show-labels\\n\\n# Check which nodes are ready\\nJSONPATH=\\'{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}\\' \\\\\\n&& kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\"\\n\\n# Check which nodes are ready with custom-columns\\nkubectl get node -o custom-columns=\\'NODE_NAME:.metadata.name,STATUS:.status.conditions[?(@.type==\"Ready\")].status\\'\\n\\n# Output decoded secrets without external tools\\nkubectl get secret my-secret -o go-template=\\'{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\\\n\"}}{{$v|base64decode}}{{\"\\\\n\\\\n\"}}{{end}}\\'\\n\\n# List all Secrets currently in use by a pod\\nkubectl get pods -o json | jq \\'.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name\\' | grep -v null | sort | uniq\\n\\n# List all containerIDs of initContainer of all pods\\n# Helpful when cleaning up stopped containers, while avoiding removal of initContainers.\\nkubectl get pods --all-namespaces -o jsonpath=\\'{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\\\n\"}{end}\\' | cut -d/ -f3\\n\\n# List Events sorted by timestamp\\nkubectl get events --sort-by=.metadata.creationTimestamp\\n\\n# List all warning events\\nkubectl events --types=Warning\\n\\n# Compares the current state of the cluster against the state that the cluster would be in if the manifest was applied.\\nkubectl diff -f ./my-manifest.yaml\\n\\n# Produce a period-delimited tree of all keys returned for nodes\\n# Helpful when locating a key within a complex nested JSON structure\\nkubectl get nodes -o json | jq -c \\'paths|join(\".\")\\'', 'chunk': '8/23', 'subchunk': '2/3', 'filename': 'knowledge/kubernetes/website-main/content/en/docs/reference/kubectl/quick-reference.md'}, references=None, vector={}, collection='Test')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "with weaviate_helper.connect() as client:\n",
    "    collection = client.collections.get(name=\"test\")\n",
    "    response = collection.query.fetch_objects(\n",
    "    filters=Filter.by_property(\"filename\").like(\"*quick-reference.md*\"),\n",
    "    limit=3\n",
    ")\n",
    "response.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
