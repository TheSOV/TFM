popeye_scan:
  description: >
    **Goal**  
    Run the **Popeye** diagnostic tool against the current **test** cluster and
    produce a human-readable plain text digest of all findings.

    **Execution Workflow (async-safe)**

    1. Invoke the Popeye action with default lint level, targeting the active
       kube-context for the *test* cluster.  
    2. Capture Popeye's report (scores, lints, warnings, errors).  
    3. Boil the output down to a concise plain text summary:  
       - Include for each problem all the relevant information that Popeye provides.
    4. Do **not** propose fixes; just report the errors with the 
    most useful information to the devops engineer.

    <Important>
    - Ignore the pods owned by Kubernetes core system.
    - Focus on extracting relevant information, about all the possible issues found in the cluster,
    only taking in count the information obtained from the tools.
    </Important>

    <blackboard>
    {blackboard}
    </blackboard>

    <feedback>
    {feedback}
    </feedback>
  expected_output: >
    A detailed plain text summary of the cluster issues, describing in detail the problem
    and analyzing deeply the cause of each issue.
  max_retries: 2
  async_execution: true
  agent: devops_tester   

kubectl_status_check:
  description: >
    **Goal**  
    Use raw **kubectl** commands to inspect cluster health and produce practical
    guidance on reading pod and node status, events, and logs.

    **Execution Workflow**

    You can run commands like the following, at your choice, until you are convinced 
    you have enough information to provide a concise health snapshot:


    • kubectl get <resource> -A -o wide → check status, labels, node placement, IPs, etc.
    Valid <resource> examples:

        pod → phase, restarts, node

        svc → type, clusterIP, external IP, ports

        deploy → replicas, up-to-date, available

        daemonset, statefulset, job, cronjob → scheduling and rollout info

        ingress → hosts, TLS, load balancer address

        pvc, pv → status (Bound, Pending), capacity, access modes

        node → version, readiness, taints, internal IPs

    • kubectl describe <resource> <name> → detailed state, events, recent failures
    Examples:

        describe pod → probe failures, termination reasons

        describe node → pressure, allocatable resources, conditions

        describe svc → selector, port mapping, endpoints

        describe pvc → claim status, mount errors

        describe deploy → rollout history, selector, conditions

    • kubectl explain <resource>[.<field>] → understand object structure and valid fields
    Examples:

        explain pod.spec.containers

        explain deployment.spec.strategy.rollingUpdate

        explain node.status.conditions

    • kubectl get events --sort-by='.lastTimestamp' → cluster-level recent warnings and errors

    Execute commands for ALL the available resources in the cluster, check the blackboard for 
    more information.

    Do **not** propose fixes; just report the errors with the 
    most useful information to the devops engineer.

    Then generate a report with the information collected in the logs that represents issues for the
    cluster. Ensure to add all the details shown in the logs not only the last line. All the information
    that is useful to debug and solve the issue should be included.

    <Important>
    Ignore the pods owned by Kubernetes core system.
    </Important>

    <blackboard>
    {blackboard}
    </blackboard>

    <feedback>
    {feedback}
    </feedback>

  expected_output: >
    A detailed plain text summary of the cluster issues, describing in detail the problem
    and analyzing deeply the cause of each issue.  Focus on extracting relevant information, about all the possible issues found in the cluster,
    only taking in count the information obtained from the tools.
  async_execution: true
  max_retries: 3
  agent: devops_tester   

aggregate_cluster_issues:
  description: >
    **Goal**  
    In the context of the task, you will have the Popeye and kubectl findings, 
    consult the researcher if needed, and draft a **comprehensive issue list**. 
    In this step you dont need to execute any tool, just use the data received 
    as context from the previous steps.

    In this step set all issues as high severity. We will classify them in a later step.

    **Execution Workflow**

    1. Parse outputs of *popeye_scan* and *kubectl_status_check*.  
    2. **Analyse the presented issues, and merge duplicate or related items into one comprehensive,
    well documented, issue.**  
    3. If missing context, query **devops_researcher** once.  
    4. Read the involved manifests and try to understand the context of the issue.
    5. Produce a clean plain text checklist of issues.

    <Important>
    - Ignore the pods owned by Kubernetes core system.
    - Focus on extracting relevant information, about all the possible issues found in the cluster,
    only taking in count the information obtained from the tools.
    - Make deep analysis to merge duplicate or related items into one comprehensive,
    well documented, issue.
    </Important>

    <blackboard>
    {blackboard}
    </blackboard>

    <feedback>
    {feedback}
    </feedback>

  expected_output: >
    A list of JSON objects, each object containing:
     - issue: a brief and ilustrative title of the issue
     - problem_description: a complete description of the issue, with all the details and context. 
     Focus on extracting relevant information, about all the possible issues found in the cluster,
     only taking in count the information obtained from the tools. Do not propose solutions here.
     - severity: the severity of the issue (must be one of: LOW, MEDIUM, HIGH)
    
  
  max_retries: 3
  context:
    - popeye_scan
    - kubectl_status_check
  agent: devops_tester

classify_cluster_issues:
  description: >
    **Goal**
    Classify the issues into High, Medium and Low severity
    following the severity rules.


    **Severity Rules**

    • **HIGH** - Blocks core functionality (apply failures, pods never *Ready*,
      critical micro-service outage) **Only if the issue prevents the cluster from working
      such as CrashLoopBackOff, ImagePullBackOff, or any other error that prevents the 
      pod from running and get to the state Ready**.
      No other issues should be classified as high.  
    • **MEDIUM** - Security or configuration risk that could hurt prod but does
      not stop pods from becoming *Ready* but may represent a risk to the cluster.  
    • **LOW** - Warnings, best-practice gaps, cosmetic lints.

    ** End of Severity Rules**

  expected_output: >
    The exact same issues list, with all the same data, and properly classified.
    
  max_retries: 3
  context:
    - aggregate_cluster_issues
  agent: devops_tester

log_cluster_issues:
  description: >
    **Goal — create a JSON record**

    Analyze the final list of cluster issues from the previous step and create a JSON record summarizing the findings.
    The JSON object must conform to the specified schema.

    **Execution Workflow**
    1. Review the output from the `classify_cluster_issues` task.
    2. Count the number of issues for each severity level (High, Medium, Low).
    3. Identify the single most critical issue.
    4. Construct a `task_description` string that summarizes these findings in no more than 50 words.
    5. Generate a final JSON object with the following fields:
       - `agent`: "devops_tester"
       - `task_name`: "Log Cluster Issues Summary"
       - `task_description`: The summary you created.

  expected_output: >
    A JSON object with the following structure:
    {{
      "agent": "devops_tester",
      "task_name": "Log Cluster Issues Summary",
      "task_description": "A summary of the cluster issues found, including the number of issues by 
      severity and the most critical blocker.'"
    }}
  max_retries: 3
  context:
    - classify_cluster_issues
  agent: devops_tester
