popeye_scan:
  description: >
    **Goal**  
    Run the **Popeye** diagnostic tool against the current **test** cluster and
    produce a human-readable Markdown digest of all findings.

    **Execution Workflow (async-safe)**

    1. Invoke the Popeye action with default lint level, targeting the active
       kube-context for the *test* cluster.  
    2. Capture Popeye's report (scores, lints, warnings, errors).  
    3. Boil the output down to a concise Markdown summary:  
       • group findings by *Kind* → *Namespace* → *Resource*  
       • include Popeye's severity (fatal / error / warn / info)  
       • add a one-line human explanation of each item.  
    4. Do **not** propose fixes; just report.

    Classify the issues using the following severity rules:

    • **HIGH** - Blocks core functionality (apply failures, pods never *Ready*,
      critical micro-service outage).  
    • **MEDIUM** - Security or configuration risk that could hurt prod but does
      not stop pods from becoming *Ready*.  
    • **LOW** - Warnings, best-practice gaps, cosmetic lints.


    <blackboard>
    {blackboard}
    </blackboard>

    <feedback>
    {feedback}
    </feedback>
  expected_output: >
    Well-structured summary of Popeye issues in Markdown listing every Popeye issue with:  
    • Kind / Namespace / Name • Popeye severity • One-sentence explanation.
  max_retries: 2
  async_execution: true
  agent: devops_tester   

kubectl_status_check:
  description: >
    **Goal**  
    Use raw **kubectl** commands to inspect cluster health and produce practical
    guidance on reading pod and node status, events, and logs.

    **Execution Workflow (async-safe)**

    You can run commands like the following, at your choice, until you are convinced 
    you have enough information to provide a concise health snapshot:


    • kubectl get <resource> -A -o wide → check status, labels, node placement, IPs, etc.
    Valid <resource> examples:

        pod → phase, restarts, node

        svc → type, clusterIP, external IP, ports

        deploy → replicas, up-to-date, available

        daemonset, statefulset, job, cronjob → scheduling and rollout info

        ingress → hosts, TLS, load balancer address

        pvc, pv → status (Bound, Pending), capacity, access modes

        node → version, readiness, taints, internal IPs

    • kubectl describe <resource> <name> → detailed state, events, recent failures
    Examples:

        describe pod → probe failures, termination reasons

        describe node → pressure, allocatable resources, conditions

        describe svc → selector, port mapping, endpoints

        describe pvc → claim status, mount errors

        describe deploy → rollout history, selector, conditions

    • kubectl explain <resource>[.<field>] → understand object structure and valid fields
    Examples:

        explain pod.spec.containers

        explain deployment.spec.strategy.rollingUpdate

        explain node.status.conditions

    • kubectl get events --sort-by='.lastTimestamp' → cluster-level recent warnings and errors

    Summarise key indicators that signal trouble:
    • CrashLoopBackOff, ImagePullBackOff, OOMKilled, CreateContainerConfigError
    • NodeNotReady, MemoryPressure, DiskPressure, NetworkUnavailable
    • PVC Pending, FailedMount, ResizePending
    • ENDPOINTS <none> in service
    • Ingress stuck without IP or returns 404/default backend

    Provide actionable tips, not full solutions—e.g.:
    • ImagePullBackOff → check image name/tag, registry credentials, DNS resolution
    • PVC Pending → verify storage class, capacity, node affinity
    • 0/3 READY in a deploy → look at pod events (probe failures, crash loops)
    • Ingress has no IP → check cloud controller, annotations, and LB quotas
    • DiskPressure → clean up disk or reschedule pods to healthy nodes
    • Service endpoints missing → check pod labels match service selector

    Classify the issues using the following severity rules:

    • **HIGH** - Blocks core functionality (apply failures, pods never *Ready*,
      critical micro-service outage).  
    • **MEDIUM** - Security or configuration risk that could hurt prod but does
      not stop pods from becoming *Ready*.  
    • **LOW** - Warnings, best-practice gaps, cosmetic lints.

    Execute commands for ALL the available resources in the cluster.

    Consolidate everything into Markdown.

    <blackboard>
    {blackboard}
    </blackboard>

    <feedback>
    {feedback}
    </feedback>

  expected_output: >
    Markdown containing:  
    • A health snapshot (status for all the resources).  
    • A summary of the causes of non-ready nodes and non-ready pods or other issues.
    • Brief comments
    • No need to include commands, or other generic information, only health snapshot and issues summary.

    Include relevant information to help the devops engineer to understand the issues and take action.
  max_retries: 3
  async_execution: true
  agent: devops_tester   

aggregate_cluster_issues:
  description: >
    **Goal**  
    Correlate the Popeye and kubectl findings, consult the researcher if needed,
    and draft a **comprehensive issue list** with severity ratings.

    **Severity Rules**

    • **HIGH** - Blocks core functionality (apply failures, pods never *Ready*,
      critical micro-service outage).  
    • **MEDIUM** - Security or configuration risk that could hurt prod but does
      not stop pods from becoming *Ready*.  
    • **LOW** - Warnings, best-practice gaps, cosmetic lints.

    **Deliverable**

    For *each* issue provide:  
    • **issue** - short technical label.  
    • **problem_description** - why it matters (1-2 sentences).  
    • **possible_manifest_file_path** - if traceable (`deployment-api.yaml`, etc.).  
    • **severity** - High | Medium | Low.
    • **observations** - Any additional observations or comments that might result useful to solve the issue.

    **Execution Workflow**

    1. Parse outputs of *popeye_scan* and *kubectl_status_check*.  
    2. Merge duplicate or related items.  
    3. If missing context, query **devops_researcher** once.  
    4. Read the involved manifests and try to understand the context of the issue.
    5. Produce a clean Markdown checklist of issues.

    Focus on the most critical issues first.

    <feedback>
    {feedback}
    </feedback>

  expected_output: >
    Comprehensive Markdown list of issues with: issue, problem_description, 
    possible_manifest_file_path, severity, observations, and any other relevant information.

    Include relevant information to help the devops engineer to understand the issues and take action.
  max_retries: 3
  context:
    - popeye_scan
    - kubectl_status_check
  agent: devops_tester

issues_to_json:
  description: >
    **Goal**  
    Transform the Markdown issue list into a strict JSON array for automated
    consumption.

    **Execution Workflow**

    1. Parse each Markdown bullet from *aggregate_cluster_issues*.  
    2. For every entry emit:  
       `{ "issue": "...", "problem_description": "...", "possible_manifest_file_path": "...", "severity": "High" | "Medium" | "Low", "observations": "..." }`  
    3. Preserve original order.  
    4. Return **only** the JSON—no extra prose.

    Focus on the most critical issues first.

  expected_output: >
    A JSON array of issue objects with keys: issue, 
    problem_description, possible_manifest_file_path, severity, observations.

    Include relevant information to help the devops engineer to understand the issues and take action.
  max_retries: 3
  context:
    - aggregate_cluster_issues
  agent: devops_tester


log_cluster_issues:
  description: >
    **Goal — create a record**

    Summarise, in no more that 50 words, Produce a concise technical log capturing problem summary
  expected_output: >
    In no more that 50 words:  
    • Number of issues found and severity mix.  
    • Most critical blocker.  
    • High-level advice for resolution path.  
  max_retries: 2
  context:
    - issues_to_json
  agent: devops_tester
